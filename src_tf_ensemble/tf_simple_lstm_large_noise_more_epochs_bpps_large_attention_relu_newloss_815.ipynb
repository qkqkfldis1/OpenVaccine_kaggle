{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-09-12T05:47:08.823964Z",
     "iopub.status.busy": "2020-09-12T05:47:08.823205Z",
     "iopub.status.idle": "2020-09-12T05:47:15.758339Z",
     "shell.execute_reply": "2020-09-12T05:47:15.757303Z"
    },
    "papermill": {
     "duration": 6.954489,
     "end_time": "2020-09-12T05:47:15.758481",
     "exception": false,
     "start_time": "2020-09-12T05:47:08.803992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow as tf\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01294,
     "end_time": "2020-09-12T05:47:15.784990",
     "exception": false,
     "start_time": "2020-09-12T05:47:15.772050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define helper functions and useful vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-12T05:47:15.817688Z",
     "iopub.status.busy": "2020-09-12T05:47:15.815907Z",
     "iopub.status.idle": "2020-09-12T05:47:15.818497Z",
     "shell.execute_reply": "2020-09-12T05:47:15.818980Z"
    },
    "papermill": {
     "duration": 0.020522,
     "end_time": "2020-09-12T05:47:15.819094",
     "exception": false,
     "start_time": "2020-09-12T05:47:15.798572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will tell us the columns we are predicting\n",
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Classifier((None, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_layer(hidden_dim, dropout):\n",
    "    return tf.keras.layers.Bidirectional(\n",
    "                                tf.keras.layers.GRU(hidden_dim,\n",
    "                                dropout=dropout,\n",
    "                                return_sequences=True,\n",
    "                                kernel_initializer = 'orthogonal'))\n",
    "\n",
    "def lstm_layer(hidden_dim, dropout):\n",
    "    return tf.keras.layers.Bidirectional(\n",
    "                                tf.keras.layers.LSTM(hidden_dim,\n",
    "                                dropout=dropout,\n",
    "                                return_sequences=True,\n",
    "                                kernel_initializer = 'orthogonal'))\n",
    "\n",
    "def build_model(gru=False,seq_len=107, pred_len=68, dropout=0.25,\n",
    "                embed_dim=128, hidden_dim=384):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(seq_len, 3))\n",
    "    inputs_bpps = tf.keras.layers.Input(shape=(seq_len, 1))\n",
    "    \n",
    "    bpps = tf.keras.layers.Dense(embed_dim, activation='relu')(inputs_bpps)\n",
    "    \n",
    "    \n",
    "    token_embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)\n",
    "    query_embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)\n",
    "    \n",
    "    t_embed = token_embed(inputs)\n",
    "    q_embed = query_embed(inputs)\n",
    "    \n",
    "    t_reshaped = tf.reshape(\n",
    "        t_embed, shape=(-1, t_embed.shape[1],  t_embed.shape[2] * t_embed.shape[3]))\n",
    "    \n",
    "    t_reshaped = tf.keras.layers.SpatialDropout1D(.2)(t_reshaped)\n",
    "    \n",
    "    t_reshaped = tf.concat([t_reshaped, bpps], axis=2)\n",
    "    \n",
    "    t_hidden = gru_layer(hidden_dim, dropout)(t_reshaped)\n",
    "    t_hidden = gru_layer(hidden_dim, dropout)(t_hidden)\n",
    "    t_hidden = gru_layer(hidden_dim, dropout)(t_hidden)\n",
    "    \n",
    "    q_reshaped = tf.reshape(\n",
    "        q_embed, shape=(-1, q_embed.shape[1],  q_embed.shape[2] * q_embed.shape[3]))\n",
    "    \n",
    "    q_reshaped = tf.keras.layers.SpatialDropout1D(.2)(q_reshaped)\n",
    "    \n",
    "    q_reshaped = tf.concat([q_reshaped, bpps], axis=2)\n",
    "    \n",
    "    q_hidden = gru_layer(hidden_dim, dropout)(q_reshaped)\n",
    "    q_hidden = gru_layer(hidden_dim, dropout)(q_hidden)\n",
    "    q_hidden = gru_layer(hidden_dim, dropout)(q_hidden)\n",
    "    \n",
    "    query_value_attention_seq = tf.keras.layers.Attention()(\n",
    "    [t_hidden, q_hidden])\n",
    "    \n",
    "    #query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    #    q_hidden)\n",
    "    #query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    #    query_value_attention_seq)\n",
    "\n",
    "    hidden = tf.keras.layers.Concatenate()(\n",
    "        [q_hidden, query_value_attention_seq])\n",
    "    \n",
    "    #only making predictions on the first part of each sequence\n",
    "    truncated = hidden[:, :pred_len]\n",
    "    \n",
    "    out = tf.keras.layers.Dense(5)(truncated)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs, inputs_bpps], outputs=out)\n",
    "\n",
    "    #some optimizers\n",
    "    adam = tf.optimizers.Adam()\n",
    "    def MCRMSE(y_true, y_pred):\n",
    "        colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n",
    "        return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n",
    "    model.compile(optimizer = adam, loss=MCRMSE)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-12T05:47:15.894673Z",
     "iopub.status.busy": "2020-09-12T05:47:15.893039Z",
     "iopub.status.idle": "2020-09-12T05:47:15.895372Z",
     "shell.execute_reply": "2020-09-12T05:47:15.895926Z"
    },
    "papermill": {
     "duration": 0.023046,
     "end_time": "2020-09-12T05:47:15.896053",
     "exception": false,
     "start_time": "2020-09-12T05:47:15.873007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n",
    "\n",
    "def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n",
    "    return np.transpose(\n",
    "        np.array(\n",
    "            df[cols]\n",
    "            .applymap(lambda seq: [token2int[x] for x in seq])\n",
    "            .values\n",
    "            .tolist()\n",
    "        ),\n",
    "        (0, 2, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013134,
     "end_time": "2020-09-12T05:47:15.922732",
     "exception": false,
     "start_time": "2020-09-12T05:47:15.909598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-12T05:47:15.957048Z",
     "iopub.status.busy": "2020-09-12T05:47:15.956359Z",
     "iopub.status.idle": "2020-09-12T05:47:16.951033Z",
     "shell.execute_reply": "2020-09-12T05:47:16.950138Z"
    },
    "papermill": {
     "duration": 1.012628,
     "end_time": "2020-09-12T05:47:16.951150",
     "exception": false,
     "start_time": "2020-09-12T05:47:15.938522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_json('../input//train.json', lines=True)\n",
    "test = pd.read_json('../input//test.json', lines=True)\n",
    "sample_df = pd.read_csv('../input//sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id_001f94081'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.load('../input/bpps/id_001f94081.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80145771, 0.8162878 , 0.9399976 , 0.98687779, 0.98872014,\n",
       "       0.8726042 , 0.64923491, 0.45909952, 0.38734203, 0.37291085,\n",
       "       0.48373307, 0.89664275, 0.90951632, 0.90861451, 0.98940993,\n",
       "       0.98145491, 0.81965108, 0.79170963, 0.38557835, 0.52880297,\n",
       "       0.5372444 , 0.52122251, 0.90823082, 0.40080513, 0.43650824,\n",
       "       0.54643881, 0.38305727, 0.53568993, 0.67795352, 0.8628183 ,\n",
       "       0.78497647, 0.83994244, 0.78492865, 0.70795306, 0.73116841,\n",
       "       0.82452095, 0.86113227, 0.52072924, 0.37224691, 0.20266188,\n",
       "       0.26856484, 0.39608598, 0.14207184, 0.65970159, 0.65338135,\n",
       "       0.80168215, 0.97622094, 0.40194761, 0.28440304, 0.09579655,\n",
       "       0.23852457, 0.80521037, 0.70261301, 0.81546441, 0.94987859,\n",
       "       0.92402902, 0.76050376, 0.63090152, 0.77919177, 0.73839201,\n",
       "       0.61416194, 0.70487221, 0.35752507, 0.40452985, 0.65327547,\n",
       "       0.58192038, 0.92482731, 0.95905864, 0.13151534, 0.09435281,\n",
       "       0.0744179 , 0.05975572, 0.04247293, 0.05214685, 0.0915126 ,\n",
       "       0.81173428, 0.93450864, 0.96903919, 0.88908934, 0.17730243,\n",
       "       0.11017743, 0.09443205, 0.13040592, 0.17304841, 0.19466995,\n",
       "       0.21206308, 1.        , 0.99890886, 0.99869359, 0.9428686 ,\n",
       "       0.86038865, 0.93654079, 0.96160082, 0.96708391, 0.94263689,\n",
       "       0.99683127, 0.98872616, 0.93275765, 1.        , 0.99814014,\n",
       "       0.95080026, 1.        , 1.        , 0.9606901 , 1.        ,\n",
       "       1.        , 0.95382831])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - foo.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target columns\n",
    "target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-12T05:47:16.996064Z",
     "iopub.status.busy": "2020-09-12T05:47:16.993995Z",
     "iopub.status.idle": "2020-09-12T05:47:17.326887Z",
     "shell.execute_reply": "2020-09-12T05:47:17.326212Z"
    },
    "papermill": {
     "duration": 0.361794,
     "end_time": "2020-09-12T05:47:17.327041",
     "exception": false,
     "start_time": "2020-09-12T05:47:16.965247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs = preprocess_inputs(train[train.signal_to_noise > 1])\n",
    "train_labels = np.array(train[train.signal_to_noise > 1][target_cols].values.tolist()).transpose((0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bpps = np.stack([1 - np.load(f'../input/bpps/{ele}.npy').sum(1) for ele in train['id']])\n",
    "train_bpps = train_bpps[train.signal_to_noise > 1][:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2096, 68, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2096, 68, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like(train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 107, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 107, 3, 128)  1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 107, 3, 128)  1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 107, 384)]   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 107, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 107, 384)]   0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 107, 384)     0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 107, 128)     256         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 107, 384)     0           tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 107, 512)]   0           spatial_dropout1d_1[0][0]        \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 107, 512)]   0           spatial_dropout1d[0][0]          \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 107, 768)     2068992     tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 107, 768)     2068992     tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 107, 768)     2658816     bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 107, 768)     2658816     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 107, 768)     2658816     bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 107, 768)     2658816     bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 107, 768)     0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 107, 1536)    0           bidirectional_5[0][0]            \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 68, 1536)]   0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 68, 5)        7685        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 14,784,773\n",
      "Trainable params: 14,784,773\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n",
      "53/53 - 9s - loss: 0.4212 - val_loss: 0.3631\n",
      "Epoch 2/120\n",
      "53/53 - 7s - loss: 0.3490 - val_loss: 0.3316\n",
      "Epoch 3/120\n",
      "53/53 - 7s - loss: 0.3267 - val_loss: 0.3228\n",
      "Epoch 4/120\n",
      "53/53 - 7s - loss: 0.3153 - val_loss: 0.3018\n",
      "Epoch 5/120\n",
      "53/53 - 7s - loss: 0.2996 - val_loss: 0.2920\n",
      "Epoch 6/120\n",
      "53/53 - 8s - loss: 0.2882 - val_loss: 0.2748\n",
      "Epoch 7/120\n",
      "53/53 - 8s - loss: 0.2762 - val_loss: 0.2647\n",
      "Epoch 8/120\n",
      "53/53 - 8s - loss: 0.2665 - val_loss: 0.2597\n",
      "Epoch 9/120\n",
      "53/53 - 8s - loss: 0.2582 - val_loss: 0.2541\n",
      "Epoch 10/120\n",
      "53/53 - 8s - loss: 0.2519 - val_loss: 0.2560\n",
      "Epoch 11/120\n",
      "53/53 - 8s - loss: 0.2501 - val_loss: 0.2445\n",
      "Epoch 12/120\n",
      "53/53 - 7s - loss: 0.2416 - val_loss: 0.2415\n",
      "Epoch 13/120\n",
      "53/53 - 8s - loss: 0.2343 - val_loss: 0.2374\n",
      "Epoch 14/120\n",
      "53/53 - 8s - loss: 0.2327 - val_loss: 0.2377\n",
      "Epoch 15/120\n",
      "53/53 - 7s - loss: 0.2261 - val_loss: 0.2354\n",
      "Epoch 16/120\n",
      "53/53 - 7s - loss: 0.2200 - val_loss: 0.2330\n",
      "Epoch 17/120\n",
      "53/53 - 7s - loss: 0.2163 - val_loss: 0.2310\n",
      "Epoch 18/120\n",
      "53/53 - 8s - loss: 0.2104 - val_loss: 0.2297\n",
      "Epoch 19/120\n",
      "53/53 - 8s - loss: 0.2074 - val_loss: 0.2288\n",
      "Epoch 20/120\n",
      "53/53 - 8s - loss: 0.2043 - val_loss: 0.2278\n",
      "Epoch 21/120\n",
      "53/53 - 8s - loss: 0.2005 - val_loss: 0.2262\n",
      "Epoch 22/120\n",
      "53/53 - 8s - loss: 0.1958 - val_loss: 0.2245\n",
      "Epoch 23/120\n",
      "53/53 - 8s - loss: 0.1933 - val_loss: 0.2298\n",
      "Epoch 24/120\n",
      "53/53 - 8s - loss: 0.1915 - val_loss: 0.2241\n",
      "Epoch 25/120\n",
      "53/53 - 8s - loss: 0.1877 - val_loss: 0.2238\n",
      "Epoch 26/120\n",
      "53/53 - 8s - loss: 0.1848 - val_loss: 0.2224\n",
      "Epoch 27/120\n",
      "53/53 - 8s - loss: 0.1837 - val_loss: 0.2235\n",
      "Epoch 28/120\n",
      "53/53 - 8s - loss: 0.1812 - val_loss: 0.2216\n",
      "Epoch 29/120\n",
      "53/53 - 8s - loss: 0.1776 - val_loss: 0.2232\n",
      "Epoch 30/120\n",
      "53/53 - 8s - loss: 0.1762 - val_loss: 0.2238\n",
      "Epoch 31/120\n",
      "53/53 - 8s - loss: 0.1741 - val_loss: 0.2223\n",
      "Epoch 32/120\n",
      "53/53 - 8s - loss: 0.1728 - val_loss: 0.2218\n",
      "Epoch 33/120\n",
      "53/53 - 8s - loss: 0.1698 - val_loss: 0.2205\n",
      "Epoch 34/120\n",
      "53/53 - 8s - loss: 0.1668 - val_loss: 0.2201\n",
      "Epoch 35/120\n",
      "53/53 - 8s - loss: 0.1654 - val_loss: 0.2186\n",
      "Epoch 36/120\n",
      "53/53 - 8s - loss: 0.1633 - val_loss: 0.2191\n",
      "Epoch 37/120\n",
      "53/53 - 8s - loss: 0.1610 - val_loss: 0.2191\n",
      "Epoch 38/120\n",
      "53/53 - 7s - loss: 0.1607 - val_loss: 0.2189\n",
      "Epoch 39/120\n",
      "53/53 - 8s - loss: 0.1596 - val_loss: 0.2189\n",
      "Epoch 40/120\n",
      "53/53 - 8s - loss: 0.1558 - val_loss: 0.2181\n",
      "Epoch 41/120\n",
      "53/53 - 8s - loss: 0.1551 - val_loss: 0.2198\n",
      "Epoch 42/120\n",
      "53/53 - 8s - loss: 0.1530 - val_loss: 0.2195\n",
      "Epoch 43/120\n",
      "53/53 - 8s - loss: 0.1511 - val_loss: 0.2191\n",
      "Epoch 44/120\n",
      "53/53 - 7s - loss: 0.1505 - val_loss: 0.2182\n",
      "Epoch 45/120\n",
      "53/53 - 8s - loss: 0.1487 - val_loss: 0.2174\n",
      "Epoch 46/120\n",
      "53/53 - 8s - loss: 0.1477 - val_loss: 0.2176\n",
      "Epoch 47/120\n",
      "53/53 - 8s - loss: 0.1469 - val_loss: 0.2179\n",
      "Epoch 48/120\n",
      "53/53 - 8s - loss: 0.1443 - val_loss: 0.2184\n",
      "Epoch 49/120\n",
      "53/53 - 8s - loss: 0.1447 - val_loss: 0.2195\n",
      "Epoch 50/120\n",
      "53/53 - 8s - loss: 0.1436 - val_loss: 0.2183\n",
      "Epoch 51/120\n",
      "53/53 - 8s - loss: 0.1422 - val_loss: 0.2195\n",
      "Epoch 52/120\n",
      "53/53 - 8s - loss: 0.1410 - val_loss: 0.2173\n",
      "Epoch 53/120\n",
      "53/53 - 8s - loss: 0.1398 - val_loss: 0.2180\n",
      "Epoch 54/120\n",
      "53/53 - 8s - loss: 0.1378 - val_loss: 0.2188\n",
      "Epoch 55/120\n",
      "53/53 - 8s - loss: 0.1381 - val_loss: 0.2194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/120\n",
      "53/53 - 8s - loss: 0.1300 - val_loss: 0.2137\n",
      "Epoch 57/120\n",
      "53/53 - 8s - loss: 0.1251 - val_loss: 0.2138\n",
      "Epoch 58/120\n",
      "53/53 - 8s - loss: 0.1236 - val_loss: 0.2136\n",
      "Epoch 59/120\n",
      "53/53 - 7s - loss: 0.1222 - val_loss: 0.2135\n",
      "Epoch 60/120\n",
      "53/53 - 7s - loss: 0.1211 - val_loss: 0.2135\n",
      "Epoch 61/120\n",
      "53/53 - 7s - loss: 0.1204 - val_loss: 0.2136\n",
      "Epoch 62/120\n",
      "53/53 - 7s - loss: 0.1197 - val_loss: 0.2140\n",
      "Epoch 63/120\n",
      "53/53 - 7s - loss: 0.1190 - val_loss: 0.2136\n",
      "Epoch 64/120\n",
      "53/53 - 8s - loss: 0.1186 - val_loss: 0.2136\n",
      "Epoch 65/120\n",
      "53/53 - 7s - loss: 0.1182 - val_loss: 0.2137\n",
      "Epoch 66/120\n",
      "53/53 - 8s - loss: 0.1176 - val_loss: 0.2137\n",
      "Epoch 67/120\n",
      "53/53 - 7s - loss: 0.1172 - val_loss: 0.2136\n",
      "Epoch 68/120\n",
      "53/53 - 7s - loss: 0.1170 - val_loss: 0.2139\n",
      "Epoch 69/120\n",
      "53/53 - 7s - loss: 0.1160 - val_loss: 0.2136\n",
      "Epoch 70/120\n",
      "53/53 - 7s - loss: 0.1159 - val_loss: 0.2137\n",
      "Epoch 71/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2137\n",
      "Epoch 72/120\n",
      "53/53 - 7s - loss: 0.1157 - val_loss: 0.2137\n",
      "Epoch 73/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2136\n",
      "Epoch 74/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2137\n",
      "Epoch 75/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2137\n",
      "Epoch 76/120\n",
      "53/53 - 7s - loss: 0.1152 - val_loss: 0.2138\n",
      "Epoch 77/120\n",
      "53/53 - 7s - loss: 0.1153 - val_loss: 0.2137\n",
      "Epoch 78/120\n",
      "53/53 - 7s - loss: 0.1152 - val_loss: 0.2136\n",
      "Epoch 79/120\n",
      "53/53 - 7s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 80/120\n",
      "53/53 - 7s - loss: 0.1153 - val_loss: 0.2137\n",
      "Epoch 81/120\n",
      "53/53 - 7s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 82/120\n",
      "53/53 - 7s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 83/120\n",
      "53/53 - 8s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 84/120\n",
      "53/53 - 8s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 85/120\n",
      "53/53 - 8s - loss: 0.1153 - val_loss: 0.2137\n",
      "Epoch 86/120\n",
      "53/53 - 8s - loss: 0.1150 - val_loss: 0.2137\n",
      "Epoch 87/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 88/120\n",
      "53/53 - 7s - loss: 0.1150 - val_loss: 0.2137\n",
      "Epoch 89/120\n",
      "53/53 - 8s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 90/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 91/120\n",
      "53/53 - 7s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 92/120\n",
      "53/53 - 8s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 93/120\n",
      "53/53 - 8s - loss: 0.1150 - val_loss: 0.2137\n",
      "Epoch 94/120\n",
      "53/53 - 8s - loss: 0.1150 - val_loss: 0.2137\n",
      "Epoch 95/120\n",
      "53/53 - 8s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 96/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 97/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 98/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 99/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 100/120\n",
      "53/53 - 8s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 101/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 102/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 103/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 104/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 105/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 106/120\n",
      "53/53 - 8s - loss: 0.1150 - val_loss: 0.2137\n",
      "Epoch 107/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 108/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 109/120\n",
      "53/53 - 8s - loss: 0.1150 - val_loss: 0.2137\n",
      "Epoch 110/120\n",
      "53/53 - 7s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 111/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 112/120\n",
      "53/53 - 8s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 113/120\n",
      "53/53 - 8s - loss: 0.1150 - val_loss: 0.2137\n",
      "Epoch 114/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 115/120\n",
      "53/53 - 8s - loss: 0.1153 - val_loss: 0.2137\n",
      "Epoch 116/120\n",
      "53/53 - 7s - loss: 0.1150 - val_loss: 0.2137\n",
      "Epoch 117/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 118/120\n",
      "53/53 - 7s - loss: 0.1152 - val_loss: 0.2137\n",
      "Epoch 119/120\n",
      "53/53 - 8s - loss: 0.1151 - val_loss: 0.2137\n",
      "Epoch 120/120\n",
      "53/53 - 8s - loss: 0.1152 - val_loss: 0.2137\n",
      "#################### 0.371128592462302\n",
      "(629, 107, 5) (3005, 130, 5)\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 107, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 107, 3, 128)  1792        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 107, 3, 128)  1792        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 107, 384)]   0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 107, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 107, 384)]   0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, 107, 384)     0           tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 107, 128)     256         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_6 (SpatialDro (None, 107, 384)     0           tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_7 (TensorFlo [(None, 107, 512)]   0           spatial_dropout1d_7[0][0]        \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_6 (TensorFlo [(None, 107, 512)]   0           spatial_dropout1d_6[0][0]        \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 107, 768)     2068992     tf_op_layer_concat_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 107, 768)     2068992     tf_op_layer_concat_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 107, 768)     2658816     bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 107, 768)     2658816     bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 107, 768)     2658816     bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 107, 768)     2658816     bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 107, 768)     0           bidirectional_20[0][0]           \n",
      "                                                                 bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 107, 1536)    0           bidirectional_23[0][0]           \n",
      "                                                                 attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(None, 68, 1536)]   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 68, 5)        7685        tf_op_layer_strided_slice_3[0][0]\n",
      "==================================================================================================\n",
      "Total params: 14,784,773\n",
      "Trainable params: 14,784,773\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 - 10s - loss: 0.4126 - val_loss: 0.3602\n",
      "Epoch 2/120\n",
      "53/53 - 8s - loss: 0.3540 - val_loss: 0.3339\n",
      "Epoch 3/120\n",
      "53/53 - 8s - loss: 0.3290 - val_loss: 0.3163\n",
      "Epoch 4/120\n",
      "53/53 - 8s - loss: 0.3194 - val_loss: 0.3042\n",
      "Epoch 5/120\n",
      "53/53 - 8s - loss: 0.3058 - val_loss: 0.2893\n",
      "Epoch 6/120\n",
      "53/53 - 7s - loss: 0.2938 - val_loss: 0.2813\n",
      "Epoch 7/120\n",
      "53/53 - 7s - loss: 0.2802 - val_loss: 0.2667\n",
      "Epoch 8/120\n",
      "53/53 - 7s - loss: 0.2701 - val_loss: 0.2594\n",
      "Epoch 9/120\n",
      "53/53 - 7s - loss: 0.2633 - val_loss: 0.2551\n",
      "Epoch 10/120\n",
      "53/53 - 7s - loss: 0.2531 - val_loss: 0.2502\n",
      "Epoch 11/120\n",
      "53/53 - 7s - loss: 0.2468 - val_loss: 0.2401\n",
      "Epoch 12/120\n",
      "53/53 - 7s - loss: 0.2435 - val_loss: 0.2378\n",
      "Epoch 13/120\n",
      "53/53 - 8s - loss: 0.2364 - val_loss: 0.2356\n",
      "Epoch 14/120\n",
      "53/53 - 8s - loss: 0.2304 - val_loss: 0.2312\n",
      "Epoch 15/120\n",
      "53/53 - 7s - loss: 0.2253 - val_loss: 0.2290\n",
      "Epoch 16/120\n",
      "53/53 - 7s - loss: 0.2219 - val_loss: 0.2320\n",
      "Epoch 17/120\n",
      "53/53 - 7s - loss: 0.2176 - val_loss: 0.2264\n",
      "Epoch 18/120\n",
      "53/53 - 7s - loss: 0.2127 - val_loss: 0.2253\n",
      "Epoch 19/120\n",
      "53/53 - 7s - loss: 0.2079 - val_loss: 0.2265\n",
      "Epoch 20/120\n",
      "53/53 - 7s - loss: 0.2042 - val_loss: 0.2243\n",
      "Epoch 21/120\n",
      "53/53 - 7s - loss: 0.1998 - val_loss: 0.2219\n",
      "Epoch 22/120\n",
      "53/53 - 7s - loss: 0.1939 - val_loss: 0.2198\n",
      "Epoch 23/120\n",
      "53/53 - 8s - loss: 0.1914 - val_loss: 0.2228\n",
      "Epoch 24/120\n",
      "53/53 - 8s - loss: 0.1880 - val_loss: 0.2182\n",
      "Epoch 25/120\n",
      "53/53 - 8s - loss: 0.1853 - val_loss: 0.2187\n",
      "Epoch 26/120\n",
      "53/53 - 8s - loss: 0.1809 - val_loss: 0.2182\n",
      "Epoch 27/120\n",
      "53/53 - 8s - loss: 0.1775 - val_loss: 0.2176\n",
      "Epoch 28/120\n",
      "53/53 - 8s - loss: 0.1744 - val_loss: 0.2169\n",
      "Epoch 29/120\n",
      "53/53 - 8s - loss: 0.1709 - val_loss: 0.2164\n",
      "Epoch 30/120\n",
      "53/53 - 8s - loss: 0.1686 - val_loss: 0.2170\n",
      "Epoch 31/120\n",
      "53/53 - 8s - loss: 0.1662 - val_loss: 0.2164\n",
      "Epoch 32/120\n",
      "53/53 - 8s - loss: 0.1638 - val_loss: 0.2181\n",
      "Epoch 33/120\n",
      "53/53 - 8s - loss: 0.1614 - val_loss: 0.2165\n",
      "Epoch 34/120\n",
      "53/53 - 8s - loss: 0.1590 - val_loss: 0.2162\n",
      "Epoch 35/120\n",
      "53/53 - 8s - loss: 0.1561 - val_loss: 0.2185\n",
      "Epoch 36/120\n",
      "53/53 - 8s - loss: 0.1550 - val_loss: 0.2155\n",
      "Epoch 37/120\n",
      "53/53 - 8s - loss: 0.1538 - val_loss: 0.2178\n",
      "Epoch 38/120\n",
      "53/53 - 8s - loss: 0.1511 - val_loss: 0.2142\n",
      "Epoch 39/120\n",
      "53/53 - 8s - loss: 0.1497 - val_loss: 0.2141\n",
      "Epoch 40/120\n",
      "53/53 - 8s - loss: 0.1472 - val_loss: 0.2152\n",
      "Epoch 41/120\n",
      "53/53 - 8s - loss: 0.1456 - val_loss: 0.2174\n",
      "Epoch 42/120\n",
      "53/53 - 8s - loss: 0.1445 - val_loss: 0.2160\n",
      "Epoch 43/120\n",
      "53/53 - 8s - loss: 0.1428 - val_loss: 0.2147\n",
      "Epoch 44/120\n",
      "53/53 - 8s - loss: 0.1406 - val_loss: 0.2152\n",
      "Epoch 45/120\n",
      "53/53 - 8s - loss: 0.1393 - val_loss: 0.2166\n",
      "Epoch 46/120\n",
      "53/53 - 7s - loss: 0.1380 - val_loss: 0.2167\n",
      "Epoch 47/120\n",
      "53/53 - 7s - loss: 0.1373 - val_loss: 0.2150\n",
      "Epoch 48/120\n",
      "53/53 - 7s - loss: 0.1362 - val_loss: 0.2142\n",
      "Epoch 49/120\n",
      "53/53 - 7s - loss: 0.1346 - val_loss: 0.2169\n",
      "Epoch 50/120\n",
      "53/53 - 7s - loss: 0.1264 - val_loss: 0.2109\n",
      "Epoch 51/120\n",
      "53/53 - 7s - loss: 0.1216 - val_loss: 0.2105\n",
      "Epoch 52/120\n",
      "53/53 - 7s - loss: 0.1196 - val_loss: 0.2102\n",
      "Epoch 53/120\n",
      "53/53 - 8s - loss: 0.1183 - val_loss: 0.2103\n",
      "Epoch 54/120\n",
      "53/53 - 8s - loss: 0.1175 - val_loss: 0.2104\n",
      "Epoch 55/120\n",
      "53/53 - 8s - loss: 0.1167 - val_loss: 0.2104\n",
      "Epoch 56/120\n",
      "53/53 - 8s - loss: 0.1162 - val_loss: 0.2102\n",
      "Epoch 57/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2106\n",
      "Epoch 58/120\n",
      "53/53 - 8s - loss: 0.1149 - val_loss: 0.2107\n",
      "Epoch 59/120\n",
      "53/53 - 8s - loss: 0.1148 - val_loss: 0.2104\n",
      "Epoch 60/120\n",
      "53/53 - 8s - loss: 0.1142 - val_loss: 0.2105\n",
      "Epoch 61/120\n",
      "53/53 - 8s - loss: 0.1138 - val_loss: 0.2107\n",
      "Epoch 62/120\n",
      "53/53 - 8s - loss: 0.1135 - val_loss: 0.2105\n",
      "Epoch 63/120\n",
      "53/53 - 8s - loss: 0.1128 - val_loss: 0.2105\n",
      "Epoch 64/120\n",
      "53/53 - 8s - loss: 0.1125 - val_loss: 0.2105\n",
      "Epoch 65/120\n",
      "53/53 - 8s - loss: 0.1125 - val_loss: 0.2105\n",
      "Epoch 66/120\n",
      "53/53 - 8s - loss: 0.1125 - val_loss: 0.2105\n",
      "Epoch 67/120\n",
      "53/53 - 7s - loss: 0.1123 - val_loss: 0.2105\n",
      "Epoch 68/120\n",
      "53/53 - 8s - loss: 0.1123 - val_loss: 0.2105\n",
      "Epoch 69/120\n",
      "53/53 - 8s - loss: 0.1123 - val_loss: 0.2105\n",
      "Epoch 70/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 71/120\n",
      "53/53 - 8s - loss: 0.1120 - val_loss: 0.2105\n",
      "Epoch 72/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 73/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 74/120\n",
      "53/53 - 8s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 75/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 76/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 77/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 78/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 79/120\n",
      "53/53 - 9s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 80/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 81/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 82/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 83/120\n",
      "53/53 - 7s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 84/120\n",
      "53/53 - 7s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 85/120\n",
      "53/53 - 7s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 86/120\n",
      "53/53 - 7s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 87/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 88/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 89/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 90/120\n",
      "53/53 - 7s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 91/120\n",
      "53/53 - 8s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 92/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 93/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 94/120\n",
      "53/53 - 7s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 95/120\n",
      "53/53 - 7s - loss: 0.1116 - val_loss: 0.2105\n",
      "Epoch 96/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 97/120\n",
      "53/53 - 7s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 98/120\n",
      "53/53 - 7s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 99/120\n",
      "53/53 - 7s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 100/120\n",
      "53/53 - 7s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 101/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 102/120\n",
      "53/53 - 8s - loss: 0.1116 - val_loss: 0.2105\n",
      "Epoch 103/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 104/120\n",
      "53/53 - 8s - loss: 0.1120 - val_loss: 0.2105\n",
      "Epoch 105/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 106/120\n",
      "53/53 - 7s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 107/120\n",
      "53/53 - 8s - loss: 0.1116 - val_loss: 0.2105\n",
      "Epoch 108/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 109/120\n",
      "53/53 - 8s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 110/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 111/120\n",
      "53/53 - 8s - loss: 0.1119 - val_loss: 0.2105\n",
      "Epoch 112/120\n",
      "53/53 - 7s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 113/120\n",
      "53/53 - 8s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 114/120\n",
      "53/53 - 8s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 115/120\n",
      "53/53 - 8s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 116/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 117/120\n",
      "53/53 - 9s - loss: 0.1117 - val_loss: 0.2105\n",
      "Epoch 118/120\n",
      "53/53 - 9s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 119/120\n",
      "53/53 - 8s - loss: 0.1118 - val_loss: 0.2105\n",
      "Epoch 120/120\n",
      "53/53 - 8s - loss: 0.1117 - val_loss: 0.2105\n",
      "#################### 0.36098429863555886\n",
      "(629, 107, 5) (3005, 130, 5)\n",
      "Model: \"functional_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 107, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 107, 3, 128)  1792        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 107, 3, 128)  1792        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_13 (TensorF [(None, 107, 384)]   0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 107, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_12 (TensorF [(None, 107, 384)]   0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_13 (SpatialDr (None, 107, 384)     0           tf_op_layer_Reshape_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 107, 128)     256         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_12 (SpatialDr (None, 107, 384)     0           tf_op_layer_Reshape_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_13 (TensorFl [(None, 107, 512)]   0           spatial_dropout1d_13[0][0]       \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_12 (TensorFl [(None, 107, 512)]   0           spatial_dropout1d_12[0][0]       \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_39 (Bidirectional (None, 107, 768)     2068992     tf_op_layer_concat_13[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_36 (Bidirectional (None, 107, 768)     2068992     tf_op_layer_concat_12[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_40 (Bidirectional (None, 107, 768)     2658816     bidirectional_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_37 (Bidirectional (None, 107, 768)     2658816     bidirectional_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_41 (Bidirectional (None, 107, 768)     2658816     bidirectional_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_38 (Bidirectional (None, 107, 768)     2658816     bidirectional_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 107, 768)     0           bidirectional_38[0][0]           \n",
      "                                                                 bidirectional_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 107, 1536)    0           bidirectional_41[0][0]           \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 68, 1536)]   0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 68, 5)        7685        tf_op_layer_strided_slice_6[0][0]\n",
      "==================================================================================================\n",
      "Total params: 14,784,773\n",
      "Trainable params: 14,784,773\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 - 12s - loss: 0.4079 - val_loss: 0.3622\n",
      "Epoch 2/120\n",
      "53/53 - 8s - loss: 0.3457 - val_loss: 0.3352\n",
      "Epoch 3/120\n",
      "53/53 - 8s - loss: 0.3256 - val_loss: 0.3171\n",
      "Epoch 4/120\n",
      "53/53 - 8s - loss: 0.3129 - val_loss: 0.3045\n",
      "Epoch 5/120\n",
      "53/53 - 8s - loss: 0.3022 - val_loss: 0.2906\n",
      "Epoch 6/120\n",
      "53/53 - 7s - loss: 0.2902 - val_loss: 0.2805\n",
      "Epoch 7/120\n",
      "53/53 - 8s - loss: 0.2760 - val_loss: 0.2698\n",
      "Epoch 8/120\n",
      "53/53 - 7s - loss: 0.2653 - val_loss: 0.2654\n",
      "Epoch 9/120\n",
      "53/53 - 7s - loss: 0.2569 - val_loss: 0.2525\n",
      "Epoch 10/120\n",
      "53/53 - 7s - loss: 0.2476 - val_loss: 0.2484\n",
      "Epoch 11/120\n",
      "53/53 - 8s - loss: 0.2421 - val_loss: 0.2415\n",
      "Epoch 12/120\n",
      "53/53 - 8s - loss: 0.2366 - val_loss: 0.2377\n",
      "Epoch 13/120\n",
      "53/53 - 7s - loss: 0.2304 - val_loss: 0.2365\n",
      "Epoch 14/120\n",
      "53/53 - 8s - loss: 0.2273 - val_loss: 0.2383\n",
      "Epoch 15/120\n",
      "53/53 - 8s - loss: 0.2218 - val_loss: 0.2314\n",
      "Epoch 16/120\n",
      "53/53 - 7s - loss: 0.2143 - val_loss: 0.2311\n",
      "Epoch 17/120\n",
      "53/53 - 7s - loss: 0.2109 - val_loss: 0.2293\n",
      "Epoch 18/120\n",
      "53/53 - 7s - loss: 0.2059 - val_loss: 0.2294\n",
      "Epoch 19/120\n",
      "53/53 - 7s - loss: 0.2015 - val_loss: 0.2286\n",
      "Epoch 20/120\n",
      "53/53 - 7s - loss: 0.1969 - val_loss: 0.2281\n",
      "Epoch 21/120\n",
      "53/53 - 8s - loss: 0.1933 - val_loss: 0.2257\n",
      "Epoch 22/120\n",
      "53/53 - 8s - loss: 0.1895 - val_loss: 0.2279\n",
      "Epoch 23/120\n",
      "53/53 - 8s - loss: 0.1850 - val_loss: 0.2276\n",
      "Epoch 24/120\n",
      "53/53 - 8s - loss: 0.1827 - val_loss: 0.2252\n",
      "Epoch 25/120\n",
      "53/53 - 8s - loss: 0.1782 - val_loss: 0.2254\n",
      "Epoch 26/120\n",
      "53/53 - 8s - loss: 0.1750 - val_loss: 0.2243\n",
      "Epoch 27/120\n",
      "53/53 - 7s - loss: 0.1714 - val_loss: 0.2244\n",
      "Epoch 28/120\n",
      "53/53 - 8s - loss: 0.1681 - val_loss: 0.2239\n",
      "Epoch 29/120\n",
      "53/53 - 7s - loss: 0.1669 - val_loss: 0.2241\n",
      "Epoch 30/120\n",
      "53/53 - 7s - loss: 0.1638 - val_loss: 0.2231\n",
      "Epoch 31/120\n",
      "53/53 - 7s - loss: 0.1598 - val_loss: 0.2225\n",
      "Epoch 32/120\n",
      "53/53 - 7s - loss: 0.1571 - val_loss: 0.2208\n",
      "Epoch 33/120\n",
      "53/53 - 7s - loss: 0.1549 - val_loss: 0.2226\n",
      "Epoch 34/120\n",
      "53/53 - 8s - loss: 0.1533 - val_loss: 0.2228\n",
      "Epoch 35/120\n",
      "53/53 - 8s - loss: 0.1525 - val_loss: 0.2220\n",
      "Epoch 36/120\n",
      "53/53 - 8s - loss: 0.1495 - val_loss: 0.2212\n",
      "Epoch 37/120\n",
      "53/53 - 8s - loss: 0.1473 - val_loss: 0.2239\n",
      "Epoch 38/120\n",
      "53/53 - 8s - loss: 0.1458 - val_loss: 0.2238\n",
      "Epoch 39/120\n",
      "53/53 - 7s - loss: 0.1439 - val_loss: 0.2231\n",
      "Epoch 40/120\n",
      "53/53 - 8s - loss: 0.1416 - val_loss: 0.2227\n",
      "Epoch 41/120\n",
      "53/53 - 8s - loss: 0.1400 - val_loss: 0.2230\n",
      "Epoch 42/120\n",
      "53/53 - 8s - loss: 0.1395 - val_loss: 0.2251\n",
      "Epoch 43/120\n",
      "53/53 - 7s - loss: 0.1308 - val_loss: 0.2184\n",
      "Epoch 44/120\n",
      "53/53 - 7s - loss: 0.1255 - val_loss: 0.2179\n",
      "Epoch 45/120\n",
      "53/53 - 8s - loss: 0.1238 - val_loss: 0.2180\n",
      "Epoch 46/120\n",
      "53/53 - 7s - loss: 0.1227 - val_loss: 0.2177\n",
      "Epoch 47/120\n",
      "53/53 - 7s - loss: 0.1215 - val_loss: 0.2177\n",
      "Epoch 48/120\n",
      "53/53 - 8s - loss: 0.1210 - val_loss: 0.2177\n",
      "Epoch 49/120\n",
      "53/53 - 8s - loss: 0.1204 - val_loss: 0.2179\n",
      "Epoch 50/120\n",
      "53/53 - 8s - loss: 0.1198 - val_loss: 0.2179\n",
      "Epoch 51/120\n",
      "53/53 - 8s - loss: 0.1192 - val_loss: 0.2178\n",
      "Epoch 52/120\n",
      "53/53 - 8s - loss: 0.1188 - val_loss: 0.2179\n",
      "Epoch 53/120\n",
      "53/53 - 8s - loss: 0.1184 - val_loss: 0.2179\n",
      "Epoch 54/120\n",
      "53/53 - 8s - loss: 0.1180 - val_loss: 0.2182\n",
      "Epoch 55/120\n",
      "53/53 - 8s - loss: 0.1175 - val_loss: 0.2181\n",
      "Epoch 56/120\n",
      "53/53 - 8s - loss: 0.1175 - val_loss: 0.2179\n",
      "Epoch 57/120\n",
      "53/53 - 7s - loss: 0.1164 - val_loss: 0.2179\n",
      "Epoch 58/120\n",
      "53/53 - 8s - loss: 0.1162 - val_loss: 0.2178\n",
      "Epoch 59/120\n",
      "53/53 - 8s - loss: 0.1162 - val_loss: 0.2179\n",
      "Epoch 60/120\n",
      "53/53 - 8s - loss: 0.1161 - val_loss: 0.2179\n",
      "Epoch 61/120\n",
      "53/53 - 8s - loss: 0.1162 - val_loss: 0.2178\n",
      "Epoch 62/120\n",
      "53/53 - 8s - loss: 0.1158 - val_loss: 0.2179\n",
      "Epoch 63/120\n",
      "53/53 - 8s - loss: 0.1160 - val_loss: 0.2179\n",
      "Epoch 64/120\n",
      "53/53 - 7s - loss: 0.1157 - val_loss: 0.2179\n",
      "Epoch 65/120\n",
      "53/53 - 8s - loss: 0.1158 - val_loss: 0.2179\n",
      "Epoch 66/120\n",
      "53/53 - 8s - loss: 0.1159 - val_loss: 0.2179\n",
      "Epoch 67/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2179\n",
      "Epoch 68/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 69/120\n",
      "53/53 - 8s - loss: 0.1155 - val_loss: 0.2179\n",
      "Epoch 70/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 71/120\n",
      "53/53 - 8s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 72/120\n",
      "53/53 - 7s - loss: 0.1157 - val_loss: 0.2180\n",
      "Epoch 73/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 74/120\n",
      "53/53 - 7s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 75/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 76/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2179\n",
      "Epoch 77/120\n",
      "53/53 - 8s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 78/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 79/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 80/120\n",
      "53/53 - 8s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 81/120\n",
      "53/53 - 8s - loss: 0.1154 - val_loss: 0.2179\n",
      "Epoch 82/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2179\n",
      "Epoch 83/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 84/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 85/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 86/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 87/120\n",
      "53/53 - 8s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 88/120\n",
      "53/53 - 7s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 89/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 90/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 91/120\n",
      "53/53 - 8s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 92/120\n",
      "53/53 - 8s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 93/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 94/120\n",
      "53/53 - 8s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 95/120\n",
      "53/53 - 8s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 96/120\n",
      "53/53 - 8s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 97/120\n",
      "53/53 - 8s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 98/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 99/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 100/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 101/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 102/120\n",
      "53/53 - 8s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 103/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 104/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 105/120\n",
      "53/53 - 8s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 106/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 107/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 108/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 109/120\n",
      "53/53 - 7s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 110/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 111/120\n",
      "53/53 - 7s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 112/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 113/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 114/120\n",
      "53/53 - 6s - loss: 0.1153 - val_loss: 0.2180\n",
      "Epoch 115/120\n",
      "53/53 - 7s - loss: 0.1156 - val_loss: 0.2180\n",
      "Epoch 116/120\n",
      "53/53 - 6s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 117/120\n",
      "53/53 - 6s - loss: 0.1155 - val_loss: 0.2180\n",
      "Epoch 118/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 119/120\n",
      "53/53 - 7s - loss: 0.1154 - val_loss: 0.2180\n",
      "Epoch 120/120\n",
      "53/53 - 7s - loss: 0.1155 - val_loss: 0.2180\n",
      "#################### 0.37114711324464633\n",
      "(629, 107, 5) (3005, 130, 5)\n",
      "Model: \"functional_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 107, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 107, 3, 128)  1792        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 107, 3, 128)  1792        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_19 (TensorF [(None, 107, 384)]   0           embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 107, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_18 (TensorF [(None, 107, 384)]   0           embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_19 (SpatialDr (None, 107, 384)     0           tf_op_layer_Reshape_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 107, 128)     256         input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_18 (SpatialDr (None, 107, 384)     0           tf_op_layer_Reshape_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_19 (TensorFl [(None, 107, 512)]   0           spatial_dropout1d_19[0][0]       \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_18 (TensorFl [(None, 107, 512)]   0           spatial_dropout1d_18[0][0]       \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_57 (Bidirectional (None, 107, 768)     2068992     tf_op_layer_concat_19[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_54 (Bidirectional (None, 107, 768)     2068992     tf_op_layer_concat_18[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_58 (Bidirectional (None, 107, 768)     2658816     bidirectional_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_55 (Bidirectional (None, 107, 768)     2658816     bidirectional_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_59 (Bidirectional (None, 107, 768)     2658816     bidirectional_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_56 (Bidirectional (None, 107, 768)     2658816     bidirectional_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 107, 768)     0           bidirectional_56[0][0]           \n",
      "                                                                 bidirectional_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 107, 1536)    0           bidirectional_59[0][0]           \n",
      "                                                                 attention_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [(None, 68, 1536)]   0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 68, 5)        7685        tf_op_layer_strided_slice_9[0][0]\n",
      "==================================================================================================\n",
      "Total params: 14,784,773\n",
      "Trainable params: 14,784,773\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 - 10s - loss: 0.4101 - val_loss: 0.3704\n",
      "Epoch 2/120\n",
      "53/53 - 6s - loss: 0.3467 - val_loss: 0.3321\n",
      "Epoch 3/120\n",
      "53/53 - 6s - loss: 0.3257 - val_loss: 0.3229\n",
      "Epoch 4/120\n",
      "53/53 - 6s - loss: 0.3140 - val_loss: 0.3047\n",
      "Epoch 5/120\n",
      "53/53 - 6s - loss: 0.2979 - val_loss: 0.2913\n",
      "Epoch 6/120\n",
      "53/53 - 6s - loss: 0.2846 - val_loss: 0.2829\n",
      "Epoch 7/120\n",
      "53/53 - 6s - loss: 0.2741 - val_loss: 0.2707\n",
      "Epoch 8/120\n",
      "53/53 - 6s - loss: 0.2644 - val_loss: 0.2633\n",
      "Epoch 9/120\n",
      "53/53 - 6s - loss: 0.2537 - val_loss: 0.2627\n",
      "Epoch 10/120\n",
      "53/53 - 7s - loss: 0.2478 - val_loss: 0.2505\n",
      "Epoch 11/120\n",
      "53/53 - 6s - loss: 0.2418 - val_loss: 0.2439\n",
      "Epoch 12/120\n",
      "53/53 - 6s - loss: 0.2356 - val_loss: 0.2455\n",
      "Epoch 13/120\n",
      "53/53 - 6s - loss: 0.2300 - val_loss: 0.2361\n",
      "Epoch 14/120\n",
      "53/53 - 6s - loss: 0.2254 - val_loss: 0.2373\n",
      "Epoch 15/120\n",
      "53/53 - 6s - loss: 0.2224 - val_loss: 0.2374\n",
      "Epoch 16/120\n",
      "53/53 - 6s - loss: 0.2156 - val_loss: 0.2316\n",
      "Epoch 17/120\n",
      "53/53 - 6s - loss: 0.2103 - val_loss: 0.2295\n",
      "Epoch 18/120\n",
      "53/53 - 6s - loss: 0.2050 - val_loss: 0.2289\n",
      "Epoch 19/120\n",
      "53/53 - 6s - loss: 0.2022 - val_loss: 0.2303\n",
      "Epoch 20/120\n",
      "53/53 - 6s - loss: 0.1979 - val_loss: 0.2245\n",
      "Epoch 21/120\n",
      "53/53 - 6s - loss: 0.1929 - val_loss: 0.2258\n",
      "Epoch 22/120\n",
      "53/53 - 6s - loss: 0.1884 - val_loss: 0.2232\n",
      "Epoch 23/120\n",
      "53/53 - 6s - loss: 0.1855 - val_loss: 0.2219\n",
      "Epoch 24/120\n",
      "53/53 - 6s - loss: 0.1812 - val_loss: 0.2222\n",
      "Epoch 25/120\n",
      "53/53 - 7s - loss: 0.1781 - val_loss: 0.2214\n",
      "Epoch 26/120\n",
      "53/53 - 6s - loss: 0.1761 - val_loss: 0.2217\n",
      "Epoch 27/120\n",
      "53/53 - 6s - loss: 0.1718 - val_loss: 0.2227\n",
      "Epoch 28/120\n",
      "53/53 - 7s - loss: 0.1693 - val_loss: 0.2206\n",
      "Epoch 29/120\n",
      "53/53 - 6s - loss: 0.1665 - val_loss: 0.2212\n",
      "Epoch 30/120\n",
      "53/53 - 6s - loss: 0.1633 - val_loss: 0.2218\n",
      "Epoch 31/120\n",
      "53/53 - 6s - loss: 0.1614 - val_loss: 0.2204\n",
      "Epoch 32/120\n",
      "53/53 - 6s - loss: 0.1582 - val_loss: 0.2203\n",
      "Epoch 33/120\n",
      "53/53 - 7s - loss: 0.1554 - val_loss: 0.2207\n",
      "Epoch 34/120\n",
      "53/53 - 6s - loss: 0.1531 - val_loss: 0.2204\n",
      "Epoch 35/120\n",
      "53/53 - 7s - loss: 0.1519 - val_loss: 0.2227\n",
      "Epoch 36/120\n",
      "53/53 - 6s - loss: 0.1504 - val_loss: 0.2197\n",
      "Epoch 37/120\n",
      "53/53 - 6s - loss: 0.1484 - val_loss: 0.2220\n",
      "Epoch 38/120\n",
      "53/53 - 6s - loss: 0.1470 - val_loss: 0.2204\n",
      "Epoch 39/120\n",
      "53/53 - 6s - loss: 0.1449 - val_loss: 0.2200\n",
      "Epoch 40/120\n",
      "53/53 - 6s - loss: 0.1423 - val_loss: 0.2184\n",
      "Epoch 41/120\n",
      "53/53 - 6s - loss: 0.1408 - val_loss: 0.2211\n",
      "Epoch 42/120\n",
      "53/53 - 6s - loss: 0.1385 - val_loss: 0.2221\n",
      "Epoch 43/120\n",
      "53/53 - 6s - loss: 0.1379 - val_loss: 0.2198\n",
      "Epoch 44/120\n",
      "53/53 - 6s - loss: 0.1371 - val_loss: 0.2199\n",
      "Epoch 45/120\n",
      "53/53 - 7s - loss: 0.1346 - val_loss: 0.2211\n",
      "Epoch 46/120\n",
      "53/53 - 6s - loss: 0.1332 - val_loss: 0.2212\n",
      "Epoch 47/120\n",
      "53/53 - 6s - loss: 0.1328 - val_loss: 0.2235\n",
      "Epoch 48/120\n",
      "53/53 - 6s - loss: 0.1324 - val_loss: 0.2200\n",
      "Epoch 49/120\n",
      "53/53 - 6s - loss: 0.1300 - val_loss: 0.2196\n",
      "Epoch 50/120\n",
      "53/53 - 6s - loss: 0.1283 - val_loss: 0.2207\n",
      "Epoch 51/120\n",
      "53/53 - 6s - loss: 0.1203 - val_loss: 0.2166\n",
      "Epoch 52/120\n",
      "53/53 - 6s - loss: 0.1159 - val_loss: 0.2165\n",
      "Epoch 53/120\n",
      "53/53 - 7s - loss: 0.1141 - val_loss: 0.2165\n",
      "Epoch 54/120\n",
      "53/53 - 6s - loss: 0.1130 - val_loss: 0.2164\n",
      "Epoch 55/120\n",
      "53/53 - 6s - loss: 0.1125 - val_loss: 0.2162\n",
      "Epoch 56/120\n",
      "53/53 - 6s - loss: 0.1116 - val_loss: 0.2164\n",
      "Epoch 57/120\n",
      "53/53 - 6s - loss: 0.1112 - val_loss: 0.2165\n",
      "Epoch 58/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2164\n",
      "Epoch 59/120\n",
      "53/53 - 6s - loss: 0.1102 - val_loss: 0.2163\n",
      "Epoch 60/120\n",
      "53/53 - 6s - loss: 0.1098 - val_loss: 0.2168\n",
      "Epoch 61/120\n",
      "53/53 - 6s - loss: 0.1094 - val_loss: 0.2165\n",
      "Epoch 62/120\n",
      "53/53 - 7s - loss: 0.1091 - val_loss: 0.2168\n",
      "Epoch 63/120\n",
      "53/53 - 7s - loss: 0.1088 - val_loss: 0.2170\n",
      "Epoch 64/120\n",
      "53/53 - 7s - loss: 0.1085 - val_loss: 0.2167\n",
      "Epoch 65/120\n",
      "53/53 - 7s - loss: 0.1082 - val_loss: 0.2164\n",
      "Epoch 66/120\n",
      "53/53 - 7s - loss: 0.1073 - val_loss: 0.2167\n",
      "Epoch 67/120\n",
      "53/53 - 7s - loss: 0.1073 - val_loss: 0.2166\n",
      "Epoch 68/120\n",
      "53/53 - 7s - loss: 0.1071 - val_loss: 0.2166\n",
      "Epoch 69/120\n",
      "53/53 - 7s - loss: 0.1071 - val_loss: 0.2166\n",
      "Epoch 70/120\n",
      "53/53 - 7s - loss: 0.1070 - val_loss: 0.2166\n",
      "Epoch 71/120\n",
      "53/53 - 7s - loss: 0.1070 - val_loss: 0.2166\n",
      "Epoch 72/120\n",
      "53/53 - 7s - loss: 0.1068 - val_loss: 0.2166\n",
      "Epoch 73/120\n",
      "53/53 - 7s - loss: 0.1067 - val_loss: 0.2166\n",
      "Epoch 74/120\n",
      "53/53 - 7s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 75/120\n",
      "53/53 - 6s - loss: 0.1067 - val_loss: 0.2166\n",
      "Epoch 76/120\n",
      "53/53 - 7s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 77/120\n",
      "53/53 - 7s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 78/120\n",
      "53/53 - 7s - loss: 0.1067 - val_loss: 0.2166\n",
      "Epoch 79/120\n",
      "53/53 - 7s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 80/120\n",
      "53/53 - 7s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 81/120\n",
      "53/53 - 7s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 82/120\n",
      "53/53 - 7s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 83/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 84/120\n",
      "53/53 - 6s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 85/120\n",
      "53/53 - 7s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 86/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 87/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 88/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 89/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 90/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 91/120\n",
      "53/53 - 7s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 92/120\n",
      "53/53 - 7s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 93/120\n",
      "53/53 - 7s - loss: 0.1063 - val_loss: 0.2166\n",
      "Epoch 94/120\n",
      "53/53 - 7s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 95/120\n",
      "53/53 - 7s - loss: 0.1067 - val_loss: 0.2166\n",
      "Epoch 96/120\n",
      "53/53 - 7s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 97/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 98/120\n",
      "53/53 - 6s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 99/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 100/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 101/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 102/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 103/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 104/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 105/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 106/120\n",
      "53/53 - 6s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 107/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 108/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 109/120\n",
      "53/53 - 6s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 110/120\n",
      "53/53 - 6s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 111/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 112/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 113/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 114/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 115/120\n",
      "53/53 - 6s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 116/120\n",
      "53/53 - 6s - loss: 0.1066 - val_loss: 0.2166\n",
      "Epoch 117/120\n",
      "53/53 - 6s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 118/120\n",
      "53/53 - 6s - loss: 0.1064 - val_loss: 0.2166\n",
      "Epoch 119/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "Epoch 120/120\n",
      "53/53 - 6s - loss: 0.1065 - val_loss: 0.2166\n",
      "#################### 0.36210384000717916\n",
      "(629, 107, 5) (3005, 130, 5)\n",
      "Model: \"functional_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 107, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 107, 3, 128)  1792        input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, 107, 3, 128)  1792        input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_25 (TensorF [(None, 107, 384)]   0           embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 107, 1)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_24 (TensorF [(None, 107, 384)]   0           embedding_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_25 (SpatialDr (None, 107, 384)     0           tf_op_layer_Reshape_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 107, 128)     256         input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_24 (SpatialDr (None, 107, 384)     0           tf_op_layer_Reshape_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_25 (TensorFl [(None, 107, 512)]   0           spatial_dropout1d_25[0][0]       \n",
      "                                                                 dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_24 (TensorFl [(None, 107, 512)]   0           spatial_dropout1d_24[0][0]       \n",
      "                                                                 dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_75 (Bidirectional (None, 107, 768)     2068992     tf_op_layer_concat_25[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_72 (Bidirectional (None, 107, 768)     2068992     tf_op_layer_concat_24[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_76 (Bidirectional (None, 107, 768)     2658816     bidirectional_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_73 (Bidirectional (None, 107, 768)     2658816     bidirectional_72[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_77 (Bidirectional (None, 107, 768)     2658816     bidirectional_76[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_74 (Bidirectional (None, 107, 768)     2658816     bidirectional_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 107, 768)     0           bidirectional_74[0][0]           \n",
      "                                                                 bidirectional_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 107, 1536)    0           bidirectional_77[0][0]           \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 68, 1536)]   0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 68, 5)        7685        tf_op_layer_strided_slice_12[0][0\n",
      "==================================================================================================\n",
      "Total params: 14,784,773\n",
      "Trainable params: 14,784,773\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 - 9s - loss: 0.4295 - val_loss: 0.3763\n",
      "Epoch 2/120\n",
      "53/53 - 6s - loss: 0.3525 - val_loss: 0.3340\n",
      "Epoch 3/120\n",
      "53/53 - 6s - loss: 0.3279 - val_loss: 0.3217\n",
      "Epoch 4/120\n",
      "53/53 - 6s - loss: 0.3177 - val_loss: 0.3144\n",
      "Epoch 5/120\n",
      "53/53 - 6s - loss: 0.3051 - val_loss: 0.3004\n",
      "Epoch 6/120\n",
      "53/53 - 6s - loss: 0.2922 - val_loss: 0.2908\n",
      "Epoch 7/120\n",
      "53/53 - 6s - loss: 0.2824 - val_loss: 0.2790\n",
      "Epoch 8/120\n",
      "53/53 - 6s - loss: 0.2748 - val_loss: 0.2651\n",
      "Epoch 9/120\n",
      "53/53 - 6s - loss: 0.2640 - val_loss: 0.2582\n",
      "Epoch 10/120\n",
      "53/53 - 6s - loss: 0.2554 - val_loss: 0.2558\n",
      "Epoch 11/120\n",
      "53/53 - 6s - loss: 0.2518 - val_loss: 0.2511\n",
      "Epoch 12/120\n",
      "53/53 - 6s - loss: 0.2436 - val_loss: 0.2482\n",
      "Epoch 13/120\n",
      "53/53 - 6s - loss: 0.2384 - val_loss: 0.2407\n",
      "Epoch 14/120\n",
      "53/53 - 6s - loss: 0.2327 - val_loss: 0.2435\n",
      "Epoch 15/120\n",
      "53/53 - 6s - loss: 0.2267 - val_loss: 0.2398\n",
      "Epoch 16/120\n",
      "53/53 - 6s - loss: 0.2230 - val_loss: 0.2365\n",
      "Epoch 17/120\n",
      "53/53 - 6s - loss: 0.2195 - val_loss: 0.2339\n",
      "Epoch 18/120\n",
      "53/53 - 6s - loss: 0.2131 - val_loss: 0.2325\n",
      "Epoch 19/120\n",
      "53/53 - 6s - loss: 0.2087 - val_loss: 0.2308\n",
      "Epoch 20/120\n",
      "53/53 - 6s - loss: 0.2047 - val_loss: 0.2334\n",
      "Epoch 21/120\n",
      "53/53 - 6s - loss: 0.2007 - val_loss: 0.2313\n",
      "Epoch 22/120\n",
      "53/53 - 6s - loss: 0.1990 - val_loss: 0.2291\n",
      "Epoch 23/120\n",
      "53/53 - 6s - loss: 0.1946 - val_loss: 0.2309\n",
      "Epoch 24/120\n",
      "53/53 - 6s - loss: 0.1913 - val_loss: 0.2270\n",
      "Epoch 25/120\n",
      "53/53 - 6s - loss: 0.1873 - val_loss: 0.2284\n",
      "Epoch 26/120\n",
      "53/53 - 7s - loss: 0.1855 - val_loss: 0.2274\n",
      "Epoch 27/120\n",
      "53/53 - 6s - loss: 0.1844 - val_loss: 0.2267\n",
      "Epoch 28/120\n",
      "53/53 - 6s - loss: 0.1808 - val_loss: 0.2240\n",
      "Epoch 29/120\n",
      "53/53 - 6s - loss: 0.1791 - val_loss: 0.2255\n",
      "Epoch 30/120\n",
      "53/53 - 6s - loss: 0.1767 - val_loss: 0.2261\n",
      "Epoch 31/120\n",
      "53/53 - 6s - loss: 0.1739 - val_loss: 0.2226\n",
      "Epoch 32/120\n",
      "53/53 - 6s - loss: 0.1730 - val_loss: 0.2238\n",
      "Epoch 33/120\n",
      "53/53 - 6s - loss: 0.1702 - val_loss: 0.2237\n",
      "Epoch 34/120\n",
      "53/53 - 6s - loss: 0.1675 - val_loss: 0.2234\n",
      "Epoch 35/120\n",
      "53/53 - 6s - loss: 0.1648 - val_loss: 0.2217\n",
      "Epoch 36/120\n",
      "53/53 - 6s - loss: 0.1635 - val_loss: 0.2229\n",
      "Epoch 37/120\n",
      "53/53 - 6s - loss: 0.1634 - val_loss: 0.2221\n",
      "Epoch 38/120\n",
      "53/53 - 6s - loss: 0.1604 - val_loss: 0.2229\n",
      "Epoch 39/120\n",
      "53/53 - 7s - loss: 0.1590 - val_loss: 0.2228\n",
      "Epoch 40/120\n",
      "53/53 - 7s - loss: 0.1584 - val_loss: 0.2216\n",
      "Epoch 41/120\n",
      "53/53 - 7s - loss: 0.1560 - val_loss: 0.2231\n",
      "Epoch 42/120\n",
      "53/53 - 7s - loss: 0.1546 - val_loss: 0.2221\n",
      "Epoch 43/120\n",
      "53/53 - 6s - loss: 0.1525 - val_loss: 0.2226\n",
      "Epoch 44/120\n",
      "53/53 - 6s - loss: 0.1515 - val_loss: 0.2207\n",
      "Epoch 45/120\n",
      "53/53 - 6s - loss: 0.1500 - val_loss: 0.2229\n",
      "Epoch 46/120\n",
      "53/53 - 6s - loss: 0.1497 - val_loss: 0.2209\n",
      "Epoch 47/120\n",
      "53/53 - 6s - loss: 0.1480 - val_loss: 0.2212\n",
      "Epoch 48/120\n",
      "53/53 - 6s - loss: 0.1452 - val_loss: 0.2207\n",
      "Epoch 49/120\n",
      "53/53 - 6s - loss: 0.1448 - val_loss: 0.2218\n",
      "Epoch 50/120\n",
      "53/53 - 6s - loss: 0.1438 - val_loss: 0.2200\n",
      "Epoch 51/120\n",
      "53/53 - 6s - loss: 0.1418 - val_loss: 0.2218\n",
      "Epoch 52/120\n",
      "53/53 - 6s - loss: 0.1410 - val_loss: 0.2225\n",
      "Epoch 53/120\n",
      "53/53 - 6s - loss: 0.1402 - val_loss: 0.2212\n",
      "Epoch 54/120\n",
      "53/53 - 6s - loss: 0.1391 - val_loss: 0.2245\n",
      "Epoch 55/120\n",
      "53/53 - 6s - loss: 0.1387 - val_loss: 0.2215\n",
      "Epoch 56/120\n",
      "53/53 - 6s - loss: 0.1371 - val_loss: 0.2213\n",
      "Epoch 57/120\n",
      "53/53 - 6s - loss: 0.1373 - val_loss: 0.2227\n",
      "Epoch 58/120\n",
      "53/53 - 6s - loss: 0.1358 - val_loss: 0.2217\n",
      "Epoch 59/120\n",
      "53/53 - 6s - loss: 0.1334 - val_loss: 0.2215\n",
      "Epoch 60/120\n",
      "53/53 - 6s - loss: 0.1317 - val_loss: 0.2212\n",
      "Epoch 61/120\n",
      "53/53 - 6s - loss: 0.1248 - val_loss: 0.2181\n",
      "Epoch 62/120\n",
      "53/53 - 6s - loss: 0.1206 - val_loss: 0.2174\n",
      "Epoch 63/120\n",
      "53/53 - 6s - loss: 0.1188 - val_loss: 0.2175\n",
      "Epoch 64/120\n",
      "53/53 - 6s - loss: 0.1176 - val_loss: 0.2172\n",
      "Epoch 65/120\n",
      "53/53 - 6s - loss: 0.1169 - val_loss: 0.2176\n",
      "Epoch 66/120\n",
      "53/53 - 6s - loss: 0.1161 - val_loss: 0.2171\n",
      "Epoch 67/120\n",
      "53/53 - 6s - loss: 0.1156 - val_loss: 0.2174\n",
      "Epoch 68/120\n",
      "53/53 - 6s - loss: 0.1151 - val_loss: 0.2174\n",
      "Epoch 69/120\n",
      "53/53 - 6s - loss: 0.1146 - val_loss: 0.2175\n",
      "Epoch 70/120\n",
      "53/53 - 6s - loss: 0.1142 - val_loss: 0.2174\n",
      "Epoch 71/120\n",
      "53/53 - 6s - loss: 0.1138 - val_loss: 0.2178\n",
      "Epoch 72/120\n",
      "53/53 - 6s - loss: 0.1136 - val_loss: 0.2175\n",
      "Epoch 73/120\n",
      "53/53 - 6s - loss: 0.1130 - val_loss: 0.2176\n",
      "Epoch 74/120\n",
      "53/53 - 6s - loss: 0.1127 - val_loss: 0.2173\n",
      "Epoch 75/120\n",
      "53/53 - 6s - loss: 0.1123 - val_loss: 0.2178\n",
      "Epoch 76/120\n",
      "53/53 - 6s - loss: 0.1120 - val_loss: 0.2175\n",
      "Epoch 77/120\n",
      "53/53 - 6s - loss: 0.1113 - val_loss: 0.2175\n",
      "Epoch 78/120\n",
      "53/53 - 6s - loss: 0.1112 - val_loss: 0.2176\n",
      "Epoch 79/120\n",
      "53/53 - 6s - loss: 0.1111 - val_loss: 0.2176\n",
      "Epoch 80/120\n",
      "53/53 - 6s - loss: 0.1109 - val_loss: 0.2175\n",
      "Epoch 81/120\n",
      "53/53 - 6s - loss: 0.1110 - val_loss: 0.2176\n",
      "Epoch 82/120\n",
      "53/53 - 6s - loss: 0.1108 - val_loss: 0.2175\n",
      "Epoch 83/120\n",
      "53/53 - 7s - loss: 0.1109 - val_loss: 0.2175\n",
      "Epoch 84/120\n",
      "53/53 - 7s - loss: 0.1109 - val_loss: 0.2175\n",
      "Epoch 85/120\n",
      "53/53 - 7s - loss: 0.1107 - val_loss: 0.2175\n",
      "Epoch 86/120\n",
      "53/53 - 7s - loss: 0.1106 - val_loss: 0.2174\n",
      "Epoch 87/120\n",
      "53/53 - 7s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 88/120\n",
      "53/53 - 7s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 89/120\n",
      "53/53 - 7s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 90/120\n",
      "53/53 - 6s - loss: 0.1104 - val_loss: 0.2175\n",
      "Epoch 91/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 92/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 93/120\n",
      "53/53 - 6s - loss: 0.1104 - val_loss: 0.2175\n",
      "Epoch 94/120\n",
      "53/53 - 6s - loss: 0.1104 - val_loss: 0.2175\n",
      "Epoch 95/120\n",
      "53/53 - 6s - loss: 0.1104 - val_loss: 0.2175\n",
      "Epoch 96/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 97/120\n",
      "53/53 - 7s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 98/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 99/120\n",
      "53/53 - 6s - loss: 0.1107 - val_loss: 0.2175\n",
      "Epoch 100/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 101/120\n",
      "53/53 - 6s - loss: 0.1104 - val_loss: 0.2175\n",
      "Epoch 102/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 103/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 104/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 105/120\n",
      "53/53 - 6s - loss: 0.1107 - val_loss: 0.2175\n",
      "Epoch 106/120\n",
      "53/53 - 6s - loss: 0.1107 - val_loss: 0.2175\n",
      "Epoch 107/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 108/120\n",
      "53/53 - 6s - loss: 0.1104 - val_loss: 0.2175\n",
      "Epoch 109/120\n",
      "53/53 - 6s - loss: 0.1107 - val_loss: 0.2175\n",
      "Epoch 110/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 111/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 112/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 113/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 114/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 115/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 116/120\n",
      "53/53 - 6s - loss: 0.1106 - val_loss: 0.2175\n",
      "Epoch 117/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 118/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 119/120\n",
      "53/53 - 6s - loss: 0.1105 - val_loss: 0.2175\n",
      "Epoch 120/120\n",
      "53/53 - 6s - loss: 0.1104 - val_loss: 0.2175\n",
      "#################### 0.3748778881001081\n",
      "(629, 107, 5) (3005, 130, 5)\n"
     ]
    }
   ],
   "source": [
    "FOLDS = KFold(n_splits=5, random_state=815, shuffle=True)\n",
    "\n",
    "oofs_pred = np.zeros_like(train_labels)\n",
    "public_preds_array = []\n",
    "public_preds_array = []\n",
    "\n",
    "for i, (trn_idx, vld_idx) in enumerate(FOLDS.split(train_inputs)):\n",
    "    trn_inputs = train_inputs[trn_idx]\n",
    "    vld_inputs = train_inputs[vld_idx]\n",
    "    \n",
    "    trn_inputs_bpps = train_bpps[trn_idx]\n",
    "    vld_inputs_bpps = train_bpps[vld_idx]\n",
    "\n",
    "    trn_labels = train_labels[trn_idx]\n",
    "    vld_labels = train_labels[vld_idx]\n",
    "\n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        [trn_inputs, trn_inputs_bpps], trn_labels, \n",
    "        validation_data=([vld_inputs, vld_inputs_bpps], vld_labels),\n",
    "        batch_size=32,\n",
    "        epochs=120,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(),\n",
    "            tf.keras.callbacks.ModelCheckpoint('tf_simple_lstm_large_noise_more_epochs_bpps_large_attention_relu_newloss_815.h5')\n",
    "        ],\n",
    "        verbose=2,\n",
    "    )\n",
    "    model.load_weights('./tf_simple_lstm_large_noise_more_epochs_bpps_large_attention_relu_newloss_815.h5')\n",
    "    outputs = model.predict([vld_inputs, vld_inputs_bpps])\n",
    "    oofs_pred[vld_idx] = outputs\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    errors = []\n",
    "    for idx in range(5):\n",
    "         errors.append(np.sqrt(mean_squared_error(vld_labels[:, idx], outputs[:, idx])))\n",
    "    final_error = np.mean(errors)\n",
    "    print('#'*20, final_error)\n",
    "\n",
    "    public_df = test.query(\"seq_length == 107\").copy()\n",
    "    private_df = test.query(\"seq_length == 130\").copy()\n",
    "\n",
    "    public_inputs = preprocess_inputs(public_df)\n",
    "    private_inputs = preprocess_inputs(private_df)\n",
    "    \n",
    "    public_bpps = np.stack([1 - np.load(f'../input/bpps/{ele}.npy').sum(1) for ele in public_df['id']])\n",
    "    public_bpps = public_bpps[:, :, np.newaxis]\n",
    "    \n",
    "    private_bpps = np.stack([1 - np.load(f'../input/bpps/{ele}.npy').sum(1) for ele in private_df['id']])\n",
    "    private_bpps = private_bpps[:, :, np.newaxis] \n",
    "\n",
    "    # Caveat: The prediction format requires the output to be the same length as the input,\n",
    "    # although it's not the case for the training data.\n",
    "    model_short = build_model(seq_len=107, pred_len=107)\n",
    "    model_long = build_model(seq_len=130, pred_len=130)\n",
    "\n",
    "    model_short.load_weights('tf_simple_lstm_large_noise_more_epochs_bpps_large_attention_relu_newloss_815.h5')\n",
    "    model_long.load_weights('tf_simple_lstm_large_noise_more_epochs_bpps_large_attention_relu_newloss_815.h5')\n",
    "\n",
    "    public_preds = model_short.predict([public_inputs, public_bpps])\n",
    "    private_preds = model_long.predict([private_inputs,private_bpps])\n",
    "    \n",
    "    public_preds_array.append(public_preds)\n",
    "    public_preds_array.append(private_preds)\n",
    "\n",
    "    print(public_preds.shape, private_preds.shape)\n",
    "\n",
    "    preds_ls = []\n",
    "\n",
    "    for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n",
    "        for idx, uid in enumerate(df.id):\n",
    "            single_pred = preds[idx]\n",
    "\n",
    "            single_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
    "            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "\n",
    "            preds_ls.append(single_df)\n",
    "\n",
    "    preds_df = pd.concat(preds_ls)\n",
    "\n",
    "    submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\n",
    "    submission.to_csv(f'submission_tf_simple_lstm_large_noise_more_epochs_bpps_large_attention_relu_newloss_815_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, uid in enumerate(train.id):\n",
    "#     single_pred = oofs_pred[i]\n",
    "\n",
    "#     oof_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
    "#     oof_df['id_seqpos'] = [f'{uid}_{x}' for x in range(oof_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'roi_heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4d9071f78b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'roi_heads'"
     ]
    }
   ],
   "source": [
    "model.roi_heads.box_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyh",
   "language": "python",
   "name": "lyh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "papermill": {
   "duration": 161.997015,
   "end_time": "2020-09-12T05:49:46.470488",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-12T05:47:04.473473",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

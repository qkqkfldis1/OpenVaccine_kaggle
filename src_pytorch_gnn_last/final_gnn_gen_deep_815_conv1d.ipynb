{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!! seed=815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [07:41:12] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../input//train.json\n",
      "Training\n",
      "Fold 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddafdc9a4daf48d7b3ee2d853865cd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.38782  |  V Loss: 0.33223  |  V MCRMSE: 0.33311 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.33069  |  V Loss: 0.41338  |  V MCRMSE: 0.44518 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30821  |  V Loss: 0.28978  |  V MCRMSE: 0.29088 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.28928  |  V Loss: 0.26698  |  V MCRMSE: 0.26812 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.27470  |  V Loss: 0.26209  |  V MCRMSE: 0.26332 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.26414  |  V Loss: 0.24974  |  V MCRMSE: 0.25093 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.25518  |  V Loss: 0.24667  |  V MCRMSE: 0.24792 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.24824  |  V Loss: 0.25496  |  V MCRMSE: 0.25632 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.24250  |  V Loss: 0.24040  |  V MCRMSE: 0.24163 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.23871  |  V Loss: 0.23826  |  V MCRMSE: 0.23963 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.23400  |  V Loss: 0.24836  |  V MCRMSE: 0.24994 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.22880  |  V Loss: 0.23406  |  V MCRMSE: 0.23528 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22555  |  V Loss: 0.23032  |  V MCRMSE: 0.23167 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.22078  |  V Loss: 0.22660  |  V MCRMSE: 0.22796 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.21715  |  V Loss: 0.22691  |  V MCRMSE: 0.22818 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.21437  |  V Loss: 0.22474  |  V MCRMSE: 0.22603 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.21064  |  V Loss: 0.22680  |  V MCRMSE: 0.22793 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.20815  |  V Loss: 0.22242  |  V MCRMSE: 0.22379 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.20516  |  V Loss: 0.22393  |  V MCRMSE: 0.22506 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.20152  |  V Loss: 0.22289  |  V MCRMSE: 0.22416 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.20052  |  V Loss: 0.22181  |  V MCRMSE: 0.22302 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.19716  |  V Loss: 0.22427  |  V MCRMSE: 0.22558 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.19433  |  V Loss: 0.21993  |  V MCRMSE: 0.22116 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.19168  |  V Loss: 0.21828  |  V MCRMSE: 0.21970 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.19040  |  V Loss: 0.21813  |  V MCRMSE: 0.21942 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.18863  |  V Loss: 0.22038  |  V MCRMSE: 0.22159 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.18507  |  V Loss: 0.21924  |  V MCRMSE: 0.22053 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.18262  |  V Loss: 0.21410  |  V MCRMSE: 0.21543 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.18147  |  V Loss: 0.21858  |  V MCRMSE: 0.22006 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.18089  |  V Loss: 0.21498  |  V MCRMSE: 0.21628 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.17946  |  V Loss: 0.21421  |  V MCRMSE: 0.21561 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.17786  |  V Loss: 0.21617  |  V MCRMSE: 0.21746 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17537  |  V Loss: 0.21606  |  V MCRMSE: 0.21751 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17400  |  V Loss: 0.21208  |  V MCRMSE: 0.21349 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.17325  |  V Loss: 0.21693  |  V MCRMSE: 0.21836 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.17194  |  V Loss: 0.21098  |  V MCRMSE: 0.21226 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.17193  |  V Loss: 0.21268  |  V MCRMSE: 0.21389 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.16938  |  V Loss: 0.21393  |  V MCRMSE: 0.21519 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16746  |  V Loss: 0.21492  |  V MCRMSE: 0.21616 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16787  |  V Loss: 0.21611  |  V MCRMSE: 0.21752 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16492  |  V Loss: 0.21283  |  V MCRMSE: 0.21413 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16485  |  V Loss: 0.20971  |  V MCRMSE: 0.21115 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.16358  |  V Loss: 0.21318  |  V MCRMSE: 0.21467 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.16376  |  V Loss: 0.21144  |  V MCRMSE: 0.21271 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.16227  |  V Loss: 0.21157  |  V MCRMSE: 0.21299 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.16176  |  V Loss: 0.21018  |  V MCRMSE: 0.21156 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.15992  |  V Loss: 0.20950  |  V MCRMSE: 0.21096 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.15906  |  V Loss: 0.20970  |  V MCRMSE: 0.21105 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.15775  |  V Loss: 0.21034  |  V MCRMSE: 0.21167 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.15773  |  V Loss: 0.20983  |  V MCRMSE: 0.21102 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15734  |  V Loss: 0.21279  |  V MCRMSE: 0.21419 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15613  |  V Loss: 0.20871  |  V MCRMSE: 0.21006 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15474  |  V Loss: 0.20908  |  V MCRMSE: 0.21039 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15563  |  V Loss: 0.21098  |  V MCRMSE: 0.21234 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15379  |  V Loss: 0.21211  |  V MCRMSE: 0.21345 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15383  |  V Loss: 0.20969  |  V MCRMSE: 0.21123 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.15249  |  V Loss: 0.20851  |  V MCRMSE: 0.20993 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.15134  |  V Loss: 0.20932  |  V MCRMSE: 0.21064 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.15194  |  V Loss: 0.20713  |  V MCRMSE: 0.20845 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.15068  |  V Loss: 0.20778  |  V MCRMSE: 0.20919 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.14953  |  V Loss: 0.20918  |  V MCRMSE: 0.21060 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.14975  |  V Loss: 0.21177  |  V MCRMSE: 0.21315 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.14914  |  V Loss: 0.20857  |  V MCRMSE: 0.20995 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.14864  |  V Loss: 0.20662  |  V MCRMSE: 0.20809 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.14885  |  V Loss: 0.20940  |  V MCRMSE: 0.21083 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.14749  |  V Loss: 0.20896  |  V MCRMSE: 0.21038 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.14703  |  V Loss: 0.21114  |  V MCRMSE: 0.21241 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.14695  |  V Loss: 0.21015  |  V MCRMSE: 0.21152 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.14662  |  V Loss: 0.20938  |  V MCRMSE: 0.21069 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.14511  |  V Loss: 0.20665  |  V MCRMSE: 0.20811 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.14476  |  V Loss: 0.20976  |  V MCRMSE: 0.21118 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.14346  |  V Loss: 0.21073  |  V MCRMSE: 0.21226 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.14337  |  V Loss: 0.20800  |  V MCRMSE: 0.20944 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.14288  |  V Loss: 0.20773  |  V MCRMSE: 0.20903 | min \n",
      "\n",
      "Epoch    75: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  74 |  T Loss: 0.14296  |  V Loss: 0.20775  |  V MCRMSE: 0.20920 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.13792  |  V Loss: 0.20468  |  V MCRMSE: 0.20609 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.13592  |  V Loss: 0.20417  |  V MCRMSE: 0.20558 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.13474  |  V Loss: 0.20398  |  V MCRMSE: 0.20544 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.13451  |  V Loss: 0.20502  |  V MCRMSE: 0.20649 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.13335  |  V Loss: 0.20474  |  V MCRMSE: 0.20618 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.13301  |  V Loss: 0.20467  |  V MCRMSE: 0.20608 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.13264  |  V Loss: 0.20450  |  V MCRMSE: 0.20598 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.13258  |  V Loss: 0.20433  |  V MCRMSE: 0.20571 | min \n",
      "\n",
      "Epoch  83 |  T Loss: 0.13212  |  V Loss: 0.20504  |  V MCRMSE: 0.20650 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.13171  |  V Loss: 0.20483  |  V MCRMSE: 0.20622 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.13148  |  V Loss: 0.20539  |  V MCRMSE: 0.20684 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.13155  |  V Loss: 0.20535  |  V MCRMSE: 0.20677 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.13100  |  V Loss: 0.20388  |  V MCRMSE: 0.20532 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.13077  |  V Loss: 0.20467  |  V MCRMSE: 0.20612 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.13060  |  V Loss: 0.20452  |  V MCRMSE: 0.20598 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.13020  |  V Loss: 0.20410  |  V MCRMSE: 0.20544 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.13054  |  V Loss: 0.20575  |  V MCRMSE: 0.20715 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.12990  |  V Loss: 0.20395  |  V MCRMSE: 0.20534 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.12933  |  V Loss: 0.20530  |  V MCRMSE: 0.20668 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.12933  |  V Loss: 0.20359  |  V MCRMSE: 0.20500 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.12925  |  V Loss: 0.20388  |  V MCRMSE: 0.20532 | min \n",
      "\n",
      "Epoch  96 |  T Loss: 0.12888  |  V Loss: 0.20590  |  V MCRMSE: 0.20735 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12815  |  V Loss: 0.20519  |  V MCRMSE: 0.20663 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12852  |  V Loss: 0.20327  |  V MCRMSE: 0.20469 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.12811  |  V Loss: 0.20456  |  V MCRMSE: 0.20592 | min \n",
      "\n",
      "\n",
      "#################### 0.20469200611114502\n",
      "Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24b7b525bd148688a0762c73b1d666a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.38495  |  V Loss: 0.34405  |  V MCRMSE: 0.34473 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.32884  |  V Loss: 0.31779  |  V MCRMSE: 0.31864 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30485  |  V Loss: 0.29776  |  V MCRMSE: 0.29844 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.28805  |  V Loss: 0.27267  |  V MCRMSE: 0.27361 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.27095  |  V Loss: 0.26154  |  V MCRMSE: 0.26242 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.26203  |  V Loss: 0.25810  |  V MCRMSE: 0.25909 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.28845  |  V Loss: 0.27509  |  V MCRMSE: 0.27610 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.26912  |  V Loss: 0.25833  |  V MCRMSE: 0.25930 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.25783  |  V Loss: 0.24905  |  V MCRMSE: 0.25002 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.25004  |  V Loss: 0.24513  |  V MCRMSE: 0.24616 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.24340  |  V Loss: 0.24814  |  V MCRMSE: 0.24915 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.24303  |  V Loss: 0.24927  |  V MCRMSE: 0.25028 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.23814  |  V Loss: 0.24352  |  V MCRMSE: 0.24445 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.23392  |  V Loss: 0.23782  |  V MCRMSE: 0.23890 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.23055  |  V Loss: 0.23612  |  V MCRMSE: 0.23719 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.22717  |  V Loss: 0.23305  |  V MCRMSE: 0.23414 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.22333  |  V Loss: 0.22939  |  V MCRMSE: 0.23041 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.22105  |  V Loss: 0.23277  |  V MCRMSE: 0.23380 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.21895  |  V Loss: 0.22768  |  V MCRMSE: 0.22867 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.21695  |  V Loss: 0.22829  |  V MCRMSE: 0.22938 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.21304  |  V Loss: 0.22458  |  V MCRMSE: 0.22555 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.21074  |  V Loss: 0.22484  |  V MCRMSE: 0.22596 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.20779  |  V Loss: 0.23066  |  V MCRMSE: 0.23169 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.20577  |  V Loss: 0.22976  |  V MCRMSE: 0.23072 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.20333  |  V Loss: 0.22279  |  V MCRMSE: 0.22382 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.20011  |  V Loss: 0.22433  |  V MCRMSE: 0.22541 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.19794  |  V Loss: 0.22274  |  V MCRMSE: 0.22382 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.19792  |  V Loss: 0.22274  |  V MCRMSE: 0.22371 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.19415  |  V Loss: 0.22048  |  V MCRMSE: 0.22147 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.19373  |  V Loss: 0.21945  |  V MCRMSE: 0.22051 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.19037  |  V Loss: 0.22609  |  V MCRMSE: 0.22714 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.18992  |  V Loss: 0.22089  |  V MCRMSE: 0.22184 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.18750  |  V Loss: 0.21738  |  V MCRMSE: 0.21838 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.18523  |  V Loss: 0.22090  |  V MCRMSE: 0.22207 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.18196  |  V Loss: 0.21803  |  V MCRMSE: 0.21904 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.18141  |  V Loss: 0.22416  |  V MCRMSE: 0.22527 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.18114  |  V Loss: 0.21700  |  V MCRMSE: 0.21799 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.17906  |  V Loss: 0.21595  |  V MCRMSE: 0.21700 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.17786  |  V Loss: 0.21579  |  V MCRMSE: 0.21688 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.17549  |  V Loss: 0.22054  |  V MCRMSE: 0.22161 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.17565  |  V Loss: 0.21747  |  V MCRMSE: 0.21855 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.17322  |  V Loss: 0.21530  |  V MCRMSE: 0.21641 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.17181  |  V Loss: 0.21535  |  V MCRMSE: 0.21655 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.17142  |  V Loss: 0.21534  |  V MCRMSE: 0.21650 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.17023  |  V Loss: 0.21448  |  V MCRMSE: 0.21555 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.16806  |  V Loss: 0.21662  |  V MCRMSE: 0.21777 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.16730  |  V Loss: 0.21668  |  V MCRMSE: 0.21776 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.16548  |  V Loss: 0.21274  |  V MCRMSE: 0.21390 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.16517  |  V Loss: 0.21283  |  V MCRMSE: 0.21400 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.16395  |  V Loss: 0.21468  |  V MCRMSE: 0.21585 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.16464  |  V Loss: 0.21419  |  V MCRMSE: 0.21545 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.16155  |  V Loss: 0.21228  |  V MCRMSE: 0.21338 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.16017  |  V Loss: 0.21266  |  V MCRMSE: 0.21379 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.16093  |  V Loss: 0.21224  |  V MCRMSE: 0.21344 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15992  |  V Loss: 0.21409  |  V MCRMSE: 0.21529 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15885  |  V Loss: 0.21912  |  V MCRMSE: 0.22028 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.15871  |  V Loss: 0.21302  |  V MCRMSE: 0.21419 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.15730  |  V Loss: 0.21421  |  V MCRMSE: 0.21535 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.15661  |  V Loss: 0.21265  |  V MCRMSE: 0.21378 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.15555  |  V Loss: 0.21476  |  V MCRMSE: 0.21590 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.15494  |  V Loss: 0.21150  |  V MCRMSE: 0.21272 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.15495  |  V Loss: 0.21271  |  V MCRMSE: 0.21386 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.15368  |  V Loss: 0.21316  |  V MCRMSE: 0.21420 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.15386  |  V Loss: 0.21416  |  V MCRMSE: 0.21531 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.15464  |  V Loss: 0.21463  |  V MCRMSE: 0.21587 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.15330  |  V Loss: 0.21400  |  V MCRMSE: 0.21518 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.15107  |  V Loss: 0.21039  |  V MCRMSE: 0.21165 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.15044  |  V Loss: 0.21735  |  V MCRMSE: 0.21856 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.15056  |  V Loss: 0.21445  |  V MCRMSE: 0.21557 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.14933  |  V Loss: 0.21293  |  V MCRMSE: 0.21416 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.14935  |  V Loss: 0.21325  |  V MCRMSE: 0.21444 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.14794  |  V Loss: 0.21222  |  V MCRMSE: 0.21338 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.14747  |  V Loss: 0.21439  |  V MCRMSE: 0.21567 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.14712  |  V Loss: 0.21250  |  V MCRMSE: 0.21378 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.14646  |  V Loss: 0.21100  |  V MCRMSE: 0.21219 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.14567  |  V Loss: 0.21096  |  V MCRMSE: 0.21221 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.14559  |  V Loss: 0.21022  |  V MCRMSE: 0.21136 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.14418  |  V Loss: 0.21310  |  V MCRMSE: 0.21434 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.14461  |  V Loss: 0.21132  |  V MCRMSE: 0.21255 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.14282  |  V Loss: 0.21347  |  V MCRMSE: 0.21468 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.14301  |  V Loss: 0.20944  |  V MCRMSE: 0.21075 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.14278  |  V Loss: 0.21324  |  V MCRMSE: 0.21455 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.14317  |  V Loss: 0.21371  |  V MCRMSE: 0.21490 | min \n",
      "\n",
      "Epoch  83 |  T Loss: 0.14260  |  V Loss: 0.21125  |  V MCRMSE: 0.21258 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.14262  |  V Loss: 0.21205  |  V MCRMSE: 0.21327 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.14228  |  V Loss: 0.21143  |  V MCRMSE: 0.21274 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.14089  |  V Loss: 0.21367  |  V MCRMSE: 0.21492 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.14070  |  V Loss: 0.21110  |  V MCRMSE: 0.21241 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.13956  |  V Loss: 0.21158  |  V MCRMSE: 0.21284 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.13927  |  V Loss: 0.21084  |  V MCRMSE: 0.21222 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.13942  |  V Loss: 0.20962  |  V MCRMSE: 0.21091 | min \n",
      "\n",
      "Epoch    92: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  91 |  T Loss: 0.13791  |  V Loss: 0.21002  |  V MCRMSE: 0.21126 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.13440  |  V Loss: 0.20850  |  V MCRMSE: 0.20980 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.13208  |  V Loss: 0.20880  |  V MCRMSE: 0.21012 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.13137  |  V Loss: 0.20933  |  V MCRMSE: 0.21065 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.13084  |  V Loss: 0.20913  |  V MCRMSE: 0.21041 | min \n",
      "\n",
      "Epoch  96 |  T Loss: 0.12996  |  V Loss: 0.20971  |  V MCRMSE: 0.21103 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12990  |  V Loss: 0.21080  |  V MCRMSE: 0.21208 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12944  |  V Loss: 0.20894  |  V MCRMSE: 0.21023 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.12894  |  V Loss: 0.20930  |  V MCRMSE: 0.21066 | min \n",
      "\n",
      "\n",
      "#################### 0.2098028063774109\n",
      "Fold 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e363fc0a47d4f0fb742ed922ae7e8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.37683  |  V Loss: 0.34125  |  V MCRMSE: 0.34237 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.32674  |  V Loss: 0.32235  |  V MCRMSE: 0.32362 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30094  |  V Loss: 0.29371  |  V MCRMSE: 0.29505 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.28195  |  V Loss: 0.27896  |  V MCRMSE: 0.28060 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.26703  |  V Loss: 0.26517  |  V MCRMSE: 0.26689 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.25975  |  V Loss: 0.26056  |  V MCRMSE: 0.26223 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.25074  |  V Loss: 0.25469  |  V MCRMSE: 0.25657 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.24490  |  V Loss: 0.24846  |  V MCRMSE: 0.25034 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.23861  |  V Loss: 0.25072  |  V MCRMSE: 0.25273 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.23306  |  V Loss: 0.24490  |  V MCRMSE: 0.24683 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.22988  |  V Loss: 0.24439  |  V MCRMSE: 0.24627 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.22613  |  V Loss: 0.23933  |  V MCRMSE: 0.24139 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22065  |  V Loss: 0.24064  |  V MCRMSE: 0.24231 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.21667  |  V Loss: 0.23425  |  V MCRMSE: 0.23634 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.21204  |  V Loss: 0.23389  |  V MCRMSE: 0.23580 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.20772  |  V Loss: 0.23476  |  V MCRMSE: 0.23679 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.20643  |  V Loss: 0.23445  |  V MCRMSE: 0.23631 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.20363  |  V Loss: 0.23182  |  V MCRMSE: 0.23395 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.19975  |  V Loss: 0.22870  |  V MCRMSE: 0.23073 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.19674  |  V Loss: 0.22714  |  V MCRMSE: 0.22912 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.19333  |  V Loss: 0.22757  |  V MCRMSE: 0.22965 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.19043  |  V Loss: 0.22508  |  V MCRMSE: 0.22710 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.18783  |  V Loss: 0.22769  |  V MCRMSE: 0.22963 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.18704  |  V Loss: 0.22307  |  V MCRMSE: 0.22522 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.18529  |  V Loss: 0.22015  |  V MCRMSE: 0.22221 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.18120  |  V Loss: 0.22360  |  V MCRMSE: 0.22568 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.17996  |  V Loss: 0.22275  |  V MCRMSE: 0.22471 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.17866  |  V Loss: 0.22355  |  V MCRMSE: 0.22548 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.17661  |  V Loss: 0.22333  |  V MCRMSE: 0.22531 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.17491  |  V Loss: 0.21980  |  V MCRMSE: 0.22200 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.17228  |  V Loss: 0.22001  |  V MCRMSE: 0.22211 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.17140  |  V Loss: 0.21979  |  V MCRMSE: 0.22191 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17059  |  V Loss: 0.22195  |  V MCRMSE: 0.22396 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17024  |  V Loss: 0.21831  |  V MCRMSE: 0.22039 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.16832  |  V Loss: 0.21970  |  V MCRMSE: 0.22186 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.16665  |  V Loss: 0.22959  |  V MCRMSE: 0.23148 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.16664  |  V Loss: 0.22025  |  V MCRMSE: 0.22234 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.16465  |  V Loss: 0.22398  |  V MCRMSE: 0.22598 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16323  |  V Loss: 0.22112  |  V MCRMSE: 0.22322 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16312  |  V Loss: 0.21843  |  V MCRMSE: 0.22057 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16026  |  V Loss: 0.21795  |  V MCRMSE: 0.22004 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16011  |  V Loss: 0.21798  |  V MCRMSE: 0.22021 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.15790  |  V Loss: 0.21719  |  V MCRMSE: 0.21932 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.15834  |  V Loss: 0.21604  |  V MCRMSE: 0.21814 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.15682  |  V Loss: 0.22129  |  V MCRMSE: 0.22335 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.15558  |  V Loss: 0.21928  |  V MCRMSE: 0.22148 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.15710  |  V Loss: 0.22008  |  V MCRMSE: 0.22213 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.15533  |  V Loss: 0.21987  |  V MCRMSE: 0.22194 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.15373  |  V Loss: 0.21742  |  V MCRMSE: 0.21975 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.15307  |  V Loss: 0.21745  |  V MCRMSE: 0.21948 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15215  |  V Loss: 0.21590  |  V MCRMSE: 0.21798 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15091  |  V Loss: 0.21785  |  V MCRMSE: 0.21998 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15152  |  V Loss: 0.21589  |  V MCRMSE: 0.21801 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15012  |  V Loss: 0.21572  |  V MCRMSE: 0.21785 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15070  |  V Loss: 0.21735  |  V MCRMSE: 0.21945 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.14849  |  V Loss: 0.21603  |  V MCRMSE: 0.21805 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.14713  |  V Loss: 0.21574  |  V MCRMSE: 0.21778 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.14644  |  V Loss: 0.21842  |  V MCRMSE: 0.22049 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.14743  |  V Loss: 0.21726  |  V MCRMSE: 0.21932 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.14731  |  V Loss: 0.21637  |  V MCRMSE: 0.21848 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.14596  |  V Loss: 0.21757  |  V MCRMSE: 0.21961 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.14573  |  V Loss: 0.21667  |  V MCRMSE: 0.21882 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.14521  |  V Loss: 0.21789  |  V MCRMSE: 0.22001 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.14467  |  V Loss: 0.21605  |  V MCRMSE: 0.21814 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.14372  |  V Loss: 0.21672  |  V MCRMSE: 0.21883 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.14340  |  V Loss: 0.21652  |  V MCRMSE: 0.21855 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.14210  |  V Loss: 0.21648  |  V MCRMSE: 0.21854 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.14256  |  V Loss: 0.21529  |  V MCRMSE: 0.21734 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.14213  |  V Loss: 0.21508  |  V MCRMSE: 0.21724 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.14139  |  V Loss: 0.21589  |  V MCRMSE: 0.21798 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.14026  |  V Loss: 0.21545  |  V MCRMSE: 0.21759 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.14154  |  V Loss: 0.21475  |  V MCRMSE: 0.21698 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.13978  |  V Loss: 0.21474  |  V MCRMSE: 0.21686 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.13996  |  V Loss: 0.21522  |  V MCRMSE: 0.21737 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.13994  |  V Loss: 0.21334  |  V MCRMSE: 0.21556 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.13918  |  V Loss: 0.21703  |  V MCRMSE: 0.21927 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.13798  |  V Loss: 0.21534  |  V MCRMSE: 0.21753 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.13681  |  V Loss: 0.21615  |  V MCRMSE: 0.21838 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.13725  |  V Loss: 0.21663  |  V MCRMSE: 0.21879 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.13648  |  V Loss: 0.21511  |  V MCRMSE: 0.21719 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.13643  |  V Loss: 0.21540  |  V MCRMSE: 0.21762 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.13618  |  V Loss: 0.21538  |  V MCRMSE: 0.21747 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.13545  |  V Loss: 0.21464  |  V MCRMSE: 0.21679 | min \n",
      "\n",
      "Epoch  83 |  T Loss: 0.13451  |  V Loss: 0.21422  |  V MCRMSE: 0.21643 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.13539  |  V Loss: 0.21396  |  V MCRMSE: 0.21608 | min \n",
      "\n",
      "Epoch    86: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  85 |  T Loss: 0.13477  |  V Loss: 0.21560  |  V MCRMSE: 0.21761 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.13061  |  V Loss: 0.21290  |  V MCRMSE: 0.21503 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.12843  |  V Loss: 0.21320  |  V MCRMSE: 0.21538 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.12751  |  V Loss: 0.21213  |  V MCRMSE: 0.21432 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.12679  |  V Loss: 0.21184  |  V MCRMSE: 0.21403 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.12639  |  V Loss: 0.21271  |  V MCRMSE: 0.21493 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.12615  |  V Loss: 0.21280  |  V MCRMSE: 0.21496 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.12550  |  V Loss: 0.21322  |  V MCRMSE: 0.21541 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.12521  |  V Loss: 0.21282  |  V MCRMSE: 0.21502 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.12521  |  V Loss: 0.21376  |  V MCRMSE: 0.21586 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.12505  |  V Loss: 0.21327  |  V MCRMSE: 0.21543 | min \n",
      "\n",
      "Epoch  96 |  T Loss: 0.12416  |  V Loss: 0.21301  |  V MCRMSE: 0.21518 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12386  |  V Loss: 0.21247  |  V MCRMSE: 0.21468 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12410  |  V Loss: 0.21147  |  V MCRMSE: 0.21369 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.12368  |  V Loss: 0.21332  |  V MCRMSE: 0.21543 | min \n",
      "\n",
      "\n",
      "#################### 0.21369092166423798\n",
      "Fold 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31db4fbe06b84f9aa66228cdc9b1e125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.38461  |  V Loss: 0.34592  |  V MCRMSE: 0.34652 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.33278  |  V Loss: 0.32646  |  V MCRMSE: 0.32733 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30990  |  V Loss: 0.30639  |  V MCRMSE: 0.30722 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.28918  |  V Loss: 0.28790  |  V MCRMSE: 0.28866 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.27274  |  V Loss: 0.27153  |  V MCRMSE: 0.27238 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.25994  |  V Loss: 0.26802  |  V MCRMSE: 0.26878 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.25260  |  V Loss: 0.25864  |  V MCRMSE: 0.25963 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.24451  |  V Loss: 0.25873  |  V MCRMSE: 0.25981 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.24019  |  V Loss: 0.25165  |  V MCRMSE: 0.25264 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.23386  |  V Loss: 0.25232  |  V MCRMSE: 0.25323 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.22897  |  V Loss: 0.25196  |  V MCRMSE: 0.25284 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.22477  |  V Loss: 0.24863  |  V MCRMSE: 0.24971 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22115  |  V Loss: 0.24386  |  V MCRMSE: 0.24502 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.21773  |  V Loss: 0.24337  |  V MCRMSE: 0.24452 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.21239  |  V Loss: 0.24014  |  V MCRMSE: 0.24122 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.20899  |  V Loss: 0.23747  |  V MCRMSE: 0.23861 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.20637  |  V Loss: 0.24039  |  V MCRMSE: 0.24151 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.20320  |  V Loss: 0.24095  |  V MCRMSE: 0.24224 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.20469  |  V Loss: 0.23477  |  V MCRMSE: 0.23594 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.19963  |  V Loss: 0.23456  |  V MCRMSE: 0.23589 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.19619  |  V Loss: 0.23115  |  V MCRMSE: 0.23236 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.19432  |  V Loss: 0.23284  |  V MCRMSE: 0.23418 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.19059  |  V Loss: 0.23935  |  V MCRMSE: 0.24066 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.18939  |  V Loss: 0.23306  |  V MCRMSE: 0.23448 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.18635  |  V Loss: 0.23141  |  V MCRMSE: 0.23271 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.18422  |  V Loss: 0.22858  |  V MCRMSE: 0.22993 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.18142  |  V Loss: 0.22913  |  V MCRMSE: 0.23049 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.18016  |  V Loss: 0.23178  |  V MCRMSE: 0.23313 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.17797  |  V Loss: 0.23059  |  V MCRMSE: 0.23202 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.17706  |  V Loss: 0.22664  |  V MCRMSE: 0.22807 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.17489  |  V Loss: 0.22934  |  V MCRMSE: 0.23079 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.17442  |  V Loss: 0.22751  |  V MCRMSE: 0.22901 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17179  |  V Loss: 0.22848  |  V MCRMSE: 0.22983 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17105  |  V Loss: 0.23023  |  V MCRMSE: 0.23156 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.16900  |  V Loss: 0.22743  |  V MCRMSE: 0.22887 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.16755  |  V Loss: 0.22760  |  V MCRMSE: 0.22901 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.16790  |  V Loss: 0.22800  |  V MCRMSE: 0.22958 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.16533  |  V Loss: 0.22669  |  V MCRMSE: 0.22809 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16384  |  V Loss: 0.22448  |  V MCRMSE: 0.22601 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16294  |  V Loss: 0.22702  |  V MCRMSE: 0.22853 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16174  |  V Loss: 0.22461  |  V MCRMSE: 0.22602 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16037  |  V Loss: 0.22693  |  V MCRMSE: 0.22845 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.15987  |  V Loss: 0.22554  |  V MCRMSE: 0.22704 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.15945  |  V Loss: 0.22503  |  V MCRMSE: 0.22645 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.15852  |  V Loss: 0.22503  |  V MCRMSE: 0.22638 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.15710  |  V Loss: 0.22374  |  V MCRMSE: 0.22517 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.15633  |  V Loss: 0.22381  |  V MCRMSE: 0.22530 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.15652  |  V Loss: 0.22720  |  V MCRMSE: 0.22870 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.15581  |  V Loss: 0.22486  |  V MCRMSE: 0.22629 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.15460  |  V Loss: 0.22289  |  V MCRMSE: 0.22440 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15252  |  V Loss: 0.22385  |  V MCRMSE: 0.22529 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15166  |  V Loss: 0.22581  |  V MCRMSE: 0.22726 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15150  |  V Loss: 0.22674  |  V MCRMSE: 0.22822 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15127  |  V Loss: 0.22424  |  V MCRMSE: 0.22570 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15099  |  V Loss: 0.22487  |  V MCRMSE: 0.22640 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15030  |  V Loss: 0.22639  |  V MCRMSE: 0.22793 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.14969  |  V Loss: 0.22449  |  V MCRMSE: 0.22607 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.14854  |  V Loss: 0.22187  |  V MCRMSE: 0.22343 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.14841  |  V Loss: 0.22501  |  V MCRMSE: 0.22652 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.14782  |  V Loss: 0.22293  |  V MCRMSE: 0.22447 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.14628  |  V Loss: 0.22214  |  V MCRMSE: 0.22371 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.14581  |  V Loss: 0.22624  |  V MCRMSE: 0.22771 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.14524  |  V Loss: 0.22229  |  V MCRMSE: 0.22390 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.14585  |  V Loss: 0.22430  |  V MCRMSE: 0.22587 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.14465  |  V Loss: 0.22516  |  V MCRMSE: 0.22655 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.14368  |  V Loss: 0.22341  |  V MCRMSE: 0.22479 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.14232  |  V Loss: 0.22289  |  V MCRMSE: 0.22435 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.14288  |  V Loss: 0.22473  |  V MCRMSE: 0.22625 | min \n",
      "\n",
      "Epoch    69: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  68 |  T Loss: 0.14273  |  V Loss: 0.22388  |  V MCRMSE: 0.22530 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.13815  |  V Loss: 0.22148  |  V MCRMSE: 0.22302 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.13505  |  V Loss: 0.22180  |  V MCRMSE: 0.22331 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.13392  |  V Loss: 0.22114  |  V MCRMSE: 0.22260 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.13284  |  V Loss: 0.22020  |  V MCRMSE: 0.22171 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.13254  |  V Loss: 0.22197  |  V MCRMSE: 0.22345 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.13178  |  V Loss: 0.22119  |  V MCRMSE: 0.22272 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.13169  |  V Loss: 0.22043  |  V MCRMSE: 0.22197 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.13135  |  V Loss: 0.21952  |  V MCRMSE: 0.22108 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.13072  |  V Loss: 0.22087  |  V MCRMSE: 0.22242 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.13037  |  V Loss: 0.22102  |  V MCRMSE: 0.22262 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.13023  |  V Loss: 0.22162  |  V MCRMSE: 0.22312 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.13036  |  V Loss: 0.22052  |  V MCRMSE: 0.22208 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.12919  |  V Loss: 0.22079  |  V MCRMSE: 0.22229 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.13000  |  V Loss: 0.22074  |  V MCRMSE: 0.22232 | min \n",
      "\n",
      "Epoch  83 |  T Loss: 0.12938  |  V Loss: 0.22151  |  V MCRMSE: 0.22305 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.12927  |  V Loss: 0.22041  |  V MCRMSE: 0.22197 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.12872  |  V Loss: 0.22127  |  V MCRMSE: 0.22282 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.12846  |  V Loss: 0.22001  |  V MCRMSE: 0.22158 | min \n",
      "\n",
      "Epoch    88: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch  87 |  T Loss: 0.12812  |  V Loss: 0.22033  |  V MCRMSE: 0.22186 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.12589  |  V Loss: 0.22078  |  V MCRMSE: 0.22228 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.12449  |  V Loss: 0.21933  |  V MCRMSE: 0.22093 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.12463  |  V Loss: 0.21925  |  V MCRMSE: 0.22083 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.12403  |  V Loss: 0.21954  |  V MCRMSE: 0.22113 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.12399  |  V Loss: 0.21933  |  V MCRMSE: 0.22090 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.12345  |  V Loss: 0.22078  |  V MCRMSE: 0.22235 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.12323  |  V Loss: 0.22053  |  V MCRMSE: 0.22206 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.12303  |  V Loss: 0.21959  |  V MCRMSE: 0.22116 | min \n",
      "\n",
      "Epoch  96 |  T Loss: 0.12288  |  V Loss: 0.21896  |  V MCRMSE: 0.22051 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12269  |  V Loss: 0.22087  |  V MCRMSE: 0.22238 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12221  |  V Loss: 0.21947  |  V MCRMSE: 0.22104 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.12204  |  V Loss: 0.21962  |  V MCRMSE: 0.22116 | min \n",
      "\n",
      "\n",
      "#################### 0.22050988674163818\n",
      "Fold 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c234621be54bd7ae83403569d73b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.38455  |  V Loss: 0.34358  |  V MCRMSE: 0.34445 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.33323  |  V Loss: 0.32739  |  V MCRMSE: 0.32828 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.31494  |  V Loss: 0.31055  |  V MCRMSE: 0.31143 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.29340  |  V Loss: 0.28704  |  V MCRMSE: 0.28805 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.27927  |  V Loss: 0.27319  |  V MCRMSE: 0.27420 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.26492  |  V Loss: 0.27236  |  V MCRMSE: 0.27333 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.25748  |  V Loss: 0.26806  |  V MCRMSE: 0.26911 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.25055  |  V Loss: 0.25834  |  V MCRMSE: 0.25936 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.24491  |  V Loss: 0.26166  |  V MCRMSE: 0.26268 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.24116  |  V Loss: 0.25136  |  V MCRMSE: 0.25249 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.23665  |  V Loss: 0.24848  |  V MCRMSE: 0.24956 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.23093  |  V Loss: 0.24723  |  V MCRMSE: 0.24834 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22451  |  V Loss: 0.24416  |  V MCRMSE: 0.24529 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.22396  |  V Loss: 0.23952  |  V MCRMSE: 0.24068 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.22010  |  V Loss: 0.24662  |  V MCRMSE: 0.24769 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.21756  |  V Loss: 0.24927  |  V MCRMSE: 0.25026 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.21678  |  V Loss: 0.24213  |  V MCRMSE: 0.24330 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.21089  |  V Loss: 0.23996  |  V MCRMSE: 0.24106 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.20580  |  V Loss: 0.23529  |  V MCRMSE: 0.23656 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.20528  |  V Loss: 0.23541  |  V MCRMSE: 0.23655 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.20233  |  V Loss: 0.23269  |  V MCRMSE: 0.23389 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.19912  |  V Loss: 0.23099  |  V MCRMSE: 0.23217 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.19832  |  V Loss: 0.23098  |  V MCRMSE: 0.23225 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.19368  |  V Loss: 0.23617  |  V MCRMSE: 0.23751 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.19483  |  V Loss: 0.22912  |  V MCRMSE: 0.23055 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.18887  |  V Loss: 0.22759  |  V MCRMSE: 0.22870 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.18761  |  V Loss: 0.22859  |  V MCRMSE: 0.22998 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.18511  |  V Loss: 0.22642  |  V MCRMSE: 0.22776 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.18215  |  V Loss: 0.22568  |  V MCRMSE: 0.22712 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.18070  |  V Loss: 0.22644  |  V MCRMSE: 0.22787 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.17916  |  V Loss: 0.22710  |  V MCRMSE: 0.22866 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.17756  |  V Loss: 0.22639  |  V MCRMSE: 0.22786 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17678  |  V Loss: 0.22827  |  V MCRMSE: 0.22974 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17512  |  V Loss: 0.22468  |  V MCRMSE: 0.22618 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.17307  |  V Loss: 0.22253  |  V MCRMSE: 0.22403 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.17098  |  V Loss: 0.22270  |  V MCRMSE: 0.22411 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.17124  |  V Loss: 0.22375  |  V MCRMSE: 0.22527 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.17068  |  V Loss: 0.22576  |  V MCRMSE: 0.22710 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16844  |  V Loss: 0.22603  |  V MCRMSE: 0.22760 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16605  |  V Loss: 0.22438  |  V MCRMSE: 0.22587 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16562  |  V Loss: 0.22292  |  V MCRMSE: 0.22430 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16451  |  V Loss: 0.22700  |  V MCRMSE: 0.22840 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.17005  |  V Loss: 0.22418  |  V MCRMSE: 0.22552 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.16269  |  V Loss: 0.22177  |  V MCRMSE: 0.22321 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.16185  |  V Loss: 0.22180  |  V MCRMSE: 0.22322 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.16011  |  V Loss: 0.21998  |  V MCRMSE: 0.22147 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.15977  |  V Loss: 0.22224  |  V MCRMSE: 0.22369 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.15873  |  V Loss: 0.22198  |  V MCRMSE: 0.22339 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.15764  |  V Loss: 0.22406  |  V MCRMSE: 0.22544 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.15765  |  V Loss: 0.22093  |  V MCRMSE: 0.22239 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15669  |  V Loss: 0.22218  |  V MCRMSE: 0.22351 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15571  |  V Loss: 0.22293  |  V MCRMSE: 0.22437 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15496  |  V Loss: 0.21932  |  V MCRMSE: 0.22079 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15381  |  V Loss: 0.22050  |  V MCRMSE: 0.22200 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15344  |  V Loss: 0.21887  |  V MCRMSE: 0.22029 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15251  |  V Loss: 0.21951  |  V MCRMSE: 0.22098 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.15144  |  V Loss: 0.22125  |  V MCRMSE: 0.22277 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.15071  |  V Loss: 0.22202  |  V MCRMSE: 0.22346 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.15097  |  V Loss: 0.21991  |  V MCRMSE: 0.22146 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.15056  |  V Loss: 0.21973  |  V MCRMSE: 0.22113 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.15016  |  V Loss: 0.22027  |  V MCRMSE: 0.22174 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.14851  |  V Loss: 0.21965  |  V MCRMSE: 0.22118 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.14874  |  V Loss: 0.22013  |  V MCRMSE: 0.22169 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.14850  |  V Loss: 0.22111  |  V MCRMSE: 0.22270 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.14570  |  V Loss: 0.22124  |  V MCRMSE: 0.22292 | min \n",
      "\n",
      "Epoch    66: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  65 |  T Loss: 0.14626  |  V Loss: 0.21894  |  V MCRMSE: 0.22049 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.14059  |  V Loss: 0.21729  |  V MCRMSE: 0.21894 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.13833  |  V Loss: 0.21766  |  V MCRMSE: 0.21927 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.13769  |  V Loss: 0.21697  |  V MCRMSE: 0.21859 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.13666  |  V Loss: 0.21711  |  V MCRMSE: 0.21875 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.13606  |  V Loss: 0.21718  |  V MCRMSE: 0.21882 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.13644  |  V Loss: 0.21632  |  V MCRMSE: 0.21793 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.13553  |  V Loss: 0.21628  |  V MCRMSE: 0.21783 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.13498  |  V Loss: 0.21543  |  V MCRMSE: 0.21704 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.13492  |  V Loss: 0.21631  |  V MCRMSE: 0.21793 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.13406  |  V Loss: 0.21654  |  V MCRMSE: 0.21816 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.13424  |  V Loss: 0.21608  |  V MCRMSE: 0.21769 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.13358  |  V Loss: 0.21612  |  V MCRMSE: 0.21780 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.13312  |  V Loss: 0.21536  |  V MCRMSE: 0.21707 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.13291  |  V Loss: 0.21643  |  V MCRMSE: 0.21812 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.13318  |  V Loss: 0.21583  |  V MCRMSE: 0.21752 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.13262  |  V Loss: 0.21727  |  V MCRMSE: 0.21893 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.13163  |  V Loss: 0.21654  |  V MCRMSE: 0.21825 | min \n",
      "\n",
      "Epoch  83 |  T Loss: 0.13219  |  V Loss: 0.21676  |  V MCRMSE: 0.21848 | min \n",
      "\n",
      "Epoch    85: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch  84 |  T Loss: 0.13221  |  V Loss: 0.21782  |  V MCRMSE: 0.21944 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.12924  |  V Loss: 0.21498  |  V MCRMSE: 0.21670 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.12816  |  V Loss: 0.21541  |  V MCRMSE: 0.21704 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.12770  |  V Loss: 0.21572  |  V MCRMSE: 0.21736 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.12726  |  V Loss: 0.21503  |  V MCRMSE: 0.21674 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.12673  |  V Loss: 0.21547  |  V MCRMSE: 0.21713 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.12635  |  V Loss: 0.21577  |  V MCRMSE: 0.21746 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.12635  |  V Loss: 0.21511  |  V MCRMSE: 0.21677 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.12606  |  V Loss: 0.21524  |  V MCRMSE: 0.21688 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.12635  |  V Loss: 0.21565  |  V MCRMSE: 0.21728 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.12602  |  V Loss: 0.21555  |  V MCRMSE: 0.21723 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.12565  |  V Loss: 0.21550  |  V MCRMSE: 0.21710 | min \n",
      "\n",
      "Epoch    97: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch  96 |  T Loss: 0.12572  |  V Loss: 0.21558  |  V MCRMSE: 0.21721 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12405  |  V Loss: 0.21505  |  V MCRMSE: 0.21666 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12388  |  V Loss: 0.21420  |  V MCRMSE: 0.21586 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.12333  |  V Loss: 0.21530  |  V MCRMSE: 0.21693 | min \n",
      "\n",
      "\n",
      "#################### 0.2158595323562622\n",
      "CV MCRMSE  0.2129877507686615\n",
      "Predicting test data\n",
      "Reading ../input//test.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission.csv\n",
      "Predicting test data\n",
      "Reading ../input//test.json\n",
      "Writing submission.csv\n",
      "[[1.        0.7920414]\n",
      " [0.7920414 1.       ]]\n",
      "[[1.         0.82817033]\n",
      " [0.82817033 1.        ]]\n",
      "[[1.         0.81859137]\n",
      " [0.81859137 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import math\n",
    "import torch.nn as nn\n",
    "# fix seed ----------------------------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed = 815\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "def seed_py(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    return seed\n",
    "\n",
    "seed_torch(seed)\n",
    "seed_py(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "NAME = 'final_gnn_feast_deep_815_conv1d'\n",
    "\n",
    "print('!!!!! seed=%d'%seed)\n",
    "#---------------------------------------------------------------------------------\n",
    "## https://www.kaggle.com/symyksr/openvaccine-deepergcn #########################\n",
    "\n",
    "\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
    "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, ARMAConv, ClusterGCNConv, FeaStConv, GENConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "data_dir = '../input/'\n",
    "train_file = data_dir+'/train.json'\n",
    "test_file  = data_dir+'/test.json'\n",
    "bpps_top   = data_dir+'/bpps'\n",
    "\n",
    "\n",
    "# settings\n",
    "\n",
    "train_with_noisy_data     = True\n",
    "add_edge_for_paired_nodes = True\n",
    "add_codon_nodes           = True\n",
    "\n",
    "bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n",
    "bpps_nb_std  = 0.08914   # std of bpps_nb across all training data\n",
    "error_mean_limit = 0.5\n",
    "\n",
    "nb_fold    = 5\n",
    "device     = 'cuda'\n",
    "batch_size = 16\n",
    "epochs     = 100\n",
    "lr         = 0.001\n",
    "T = 5\n",
    "node_hidden_channels = 64\n",
    "edge_hidden_channels = 64\n",
    "hidden_channels3 = 64\n",
    "num_layers = 4\n",
    "dropout1 = 0.1\n",
    "dropout2 = 0.1\n",
    "dropout3 = 0.1\n",
    "\n",
    "##################### all data preparation ####################################\n",
    "\n",
    "def match_pair(structure):\n",
    "    pair = [-1] * len(structure)\n",
    "    pair_no = -1\n",
    "\n",
    "    pair_no_stack = []\n",
    "    for i, c in enumerate(structure):\n",
    "        if c == '(':\n",
    "            pair_no += 1\n",
    "            pair[i] = pair_no\n",
    "            pair_no_stack.append(pair_no)\n",
    "        elif c == ')':\n",
    "            pair[i] = pair_no_stack.pop()\n",
    "    return pair\n",
    "\n",
    "class MyData(Data):\n",
    "    def __init__(self, x=None, edge_index=None, edge_attr=None, y=None,\n",
    "                 pos=None, norm=None, face=None, weight=None, **kwargs):\n",
    "        super(MyData, self).__init__(x=x, edge_index=edge_index,\n",
    "                                     edge_attr=edge_attr, y=y, pos=pos,\n",
    "                                     norm=norm, face=face, **kwargs)\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "def calc_error_mean(row):\n",
    "    reactivity_error = row['reactivity_error']\n",
    "    deg_error_Mg_pH10 = row['deg_error_Mg_pH10']\n",
    "    deg_error_Mg_50C = row['deg_error_Mg_50C']\n",
    "\n",
    "    return np.mean(np.abs(reactivity_error) +\n",
    "                   np.abs(deg_error_Mg_pH10) + \\\n",
    "                   np.abs(deg_error_Mg_50C)) / 3\n",
    "\n",
    "\n",
    "def calc_sample_weight(row):\n",
    "    if sample_is_clean(row):\n",
    "        return 1.\n",
    "    else:\n",
    "        error_mean = calc_error_mean(row)\n",
    "        if error_mean >= error_mean_limit:\n",
    "            return 0.\n",
    "\n",
    "        return 1. - error_mean / error_mean_limit\n",
    "\n",
    "\n",
    "# add directed edge for node1 -> node2 and for node2 -> node1\n",
    "def add_edges(edge_index, edge_features, node1, node2, feature1, feature2):\n",
    "    edge_index.append([node1, node2])\n",
    "    edge_features.append(feature1)\n",
    "    edge_index.append([node2, node1])\n",
    "    edge_features.append(feature2)\n",
    "\n",
    "\n",
    "def add_edges_between_base_nodes(edge_index, edge_features, node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        1, # forward edge: 1, backward edge: -1\n",
    "        1, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        -1, # forward edge: 1, backward edge: -1\n",
    "        1, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_paired_nodes(edge_index, edge_features, node1, node2,\n",
    "                                   bpps_value):\n",
    "    edge_feature1 = [\n",
    "        1, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        bpps_value, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        1, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        bpps_value, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_codon_nodes(edge_index, edge_features, node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        1, # is edge between coden nodes\n",
    "        1, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        1, # is edge between coden nodes\n",
    "        -1, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_codon_and_base_node(edge_index, edge_features,\n",
    "                                          node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        1, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        1, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_node(node_features, feature):\n",
    "    node_features.append(feature)\n",
    "\n",
    "\n",
    "def add_base_node(node_features, sequence, predicted_loop_type,\n",
    "                  bpps_sum, bpps_nb):\n",
    "    feature = [\n",
    "        0, # is codon node\n",
    "        sequence == 'A',\n",
    "        sequence == 'C',\n",
    "        sequence == 'G',\n",
    "        sequence == 'U',\n",
    "        predicted_loop_type == 'S',\n",
    "        predicted_loop_type == 'M',\n",
    "        predicted_loop_type == 'I',\n",
    "        predicted_loop_type == 'B',\n",
    "        predicted_loop_type == 'H',\n",
    "        predicted_loop_type == 'E',\n",
    "        predicted_loop_type == 'X',\n",
    "        bpps_sum,\n",
    "        bpps_nb,\n",
    "    ]\n",
    "    add_node(node_features, feature)\n",
    "\n",
    "def add_codon_node(node_features):\n",
    "    feature = [\n",
    "        1, # is codon node\n",
    "        0, # sequence == 'A',\n",
    "        0, # sequence == 'C',\n",
    "        0, # sequence == 'G',\n",
    "        0, # sequence == 'U',\n",
    "        0, # predicted_loop_type == 'S',\n",
    "        0, # predicted_loop_type == 'M',\n",
    "        0, # predicted_loop_type == 'I',\n",
    "        0, # predicted_loop_type == 'B',\n",
    "        0, # predicted_loop_type == 'H',\n",
    "        0, # predicted_loop_type == 'E',\n",
    "        0, # predicted_loop_type == 'X',\n",
    "        0, # bpps_sum\n",
    "        0, # bpps_nb\n",
    "    ]\n",
    "    add_node(node_features, feature)\n",
    "\n",
    "def build_data(df, is_train):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        targets = []\n",
    "        node_features = []\n",
    "        edge_features = []\n",
    "        edge_index = []\n",
    "        train_mask = []\n",
    "        test_mask = []\n",
    "        weights = []\n",
    "\n",
    "        id = df.loc[i, 'id']\n",
    "        path = os.path.join(bpps_top, id + '.npy')\n",
    "        bpps = np.load(path)\n",
    "        bpps_sum = bpps.sum(axis=0)\n",
    "        sequence = df.loc[i, 'sequence']\n",
    "        structure = df.loc[i, 'structure']\n",
    "        pair_info = match_pair(structure)\n",
    "        predicted_loop_type = df.loc[i, 'predicted_loop_type']\n",
    "        seq_length = df.loc[i, 'seq_length']\n",
    "        seq_scored = df.loc[i, 'seq_scored']\n",
    "        bpps_nb = (bpps > 0).sum(axis=0) / seq_length\n",
    "        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n",
    "        if is_train:\n",
    "            sample_weight = calc_sample_weight(df.loc[i])\n",
    "\n",
    "            reactivity = df.loc[i, 'reactivity']\n",
    "            deg_Mg_pH10 = df.loc[i, 'deg_Mg_pH10']\n",
    "            deg_Mg_50C = df.loc[i, 'deg_Mg_50C']\n",
    "\n",
    "            for j in range(seq_length):\n",
    "                if j < seq_scored:\n",
    "                    targets.append([\n",
    "                        reactivity[j],\n",
    "                        deg_Mg_pH10[j],\n",
    "                        deg_Mg_50C[j],\n",
    "                        ])\n",
    "                else:\n",
    "                    targets.append([0, 0, 0])\n",
    "\n",
    "        paired_nodes = {}\n",
    "        for j in range(seq_length):\n",
    "            add_base_node(node_features, sequence[j], predicted_loop_type[j],\n",
    "                          bpps_sum[j], bpps_nb[j])\n",
    "\n",
    "            if j + 1 < seq_length: # edge between current node and next node\n",
    "                add_edges_between_base_nodes(edge_index, edge_features,\n",
    "                                             j, j + 1)\n",
    "\n",
    "            if pair_info[j] != -1:\n",
    "                if pair_info[j] not in paired_nodes:\n",
    "                    paired_nodes[pair_info[j]] = [j]\n",
    "                else:\n",
    "                    paired_nodes[pair_info[j]].append(j)\n",
    "\n",
    "            train_mask.append(j < seq_scored)\n",
    "            test_mask.append(True)\n",
    "            if is_train:\n",
    "                weights.append(sample_weight)\n",
    "\n",
    "\n",
    "        if add_edge_for_paired_nodes:\n",
    "            for pair in paired_nodes.values():\n",
    "                bpps_value = bpps[pair[0], pair[1]]\n",
    "                add_edges_between_paired_nodes(edge_index, edge_features,\n",
    "                                               pair[0], pair[1], bpps_value)\n",
    "\n",
    "\n",
    "\n",
    "        if add_codon_nodes:\n",
    "            codon_node_idx = seq_length - 1\n",
    "            for j in range(seq_length):\n",
    "                if j % 3 == 0:\n",
    "                    # add codon node\n",
    "                    add_codon_node(node_features)\n",
    "                    codon_node_idx += 1\n",
    "                    train_mask.append(False)\n",
    "                    test_mask.append(False)\n",
    "                    if is_train:\n",
    "                        weights.append(0)\n",
    "                        targets.append([0, 0, 0])\n",
    "\n",
    "                    if codon_node_idx > seq_length:\n",
    "                        # add edges between adjacent codon nodes\n",
    "                        add_edges_between_codon_nodes(edge_index, edge_features,\n",
    "                                                      codon_node_idx - 1,\n",
    "                                                      codon_node_idx)\n",
    "\n",
    "                # add edges between codon node and base node\n",
    "                add_edges_between_codon_and_base_node(edge_index, edge_features,\n",
    "                                                      j, codon_node_idx)\n",
    "\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        edge_index    = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "\n",
    "        if is_train:\n",
    "            data.append(MyData(x=node_features, edge_index=edge_index,\n",
    "                               edge_attr=edge_features,\n",
    "                               train_mask=torch.tensor(train_mask),\n",
    "                               weight=torch.tensor(weights, dtype=torch.float),\n",
    "                               y=torch.tensor(targets, dtype=torch.float)))\n",
    "        else:\n",
    "            data.append(MyData(x=node_features, edge_index=edge_index,\n",
    "                               edge_attr=edge_features,\n",
    "                               test_mask=torch.tensor(test_mask)))\n",
    "\n",
    "    return data\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "def build_id_seqpos(df):\n",
    "    id_seqpos = []\n",
    "    for i in range(len(df)):\n",
    "        id = df.loc[i, 'id']\n",
    "        seq_length = df.loc[i, 'seq_length']\n",
    "        for seqpos in range(seq_length):\n",
    "            id_seqpos.append(id + '_' + str(seqpos))\n",
    "    return id_seqpos\n",
    "\n",
    "def sample_is_clean(row):\n",
    "    return row['SN_filter'] == 1\n",
    "    #return row['signal_to_noise'] > 1 and \\\n",
    "    #       min((min(row['reactivity']),\n",
    "    #            min(row['deg_Mg_pH10']),\n",
    "    #            min(row['deg_pH10']),\n",
    "    #            min(row['deg_Mg_50C']),\n",
    "    #            min(row['deg_50C']))) > -0.5\n",
    "\n",
    "# categorical value for target (used for stratified kfold)\n",
    "def add_y_cat(df):\n",
    "    target_mean = df['reactivity'].apply(np.mean) +                   df['deg_Mg_pH10'].apply(np.mean) +                   df['deg_Mg_50C'].apply(np.mean)\n",
    "    df['y_cat'] = pd.qcut(np.array(target_mean), q=20).codes\n",
    "\n",
    "##################### all model preparation ####################################\n",
    "\n",
    "# originally copied from\n",
    "# https://github.com/rusty1s/pytorch_geometric/blob/master/examples/ogbn_proteins_deepgcn.py\n",
    "#\n",
    "class MapE2NxN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels):\n",
    "        super(MapE2NxN, self).__init__()\n",
    "        self.linear1 = Linear(in_channels, hidden_channels)\n",
    "        self.linear2 = Linear(hidden_channels, out_channels)\n",
    "        self.dropout = Dropout(dropout3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionEncode(nn.Module):\n",
    "    def __init__(self, dim, length=174):\n",
    "        super(PositionEncode, self).__init__()\n",
    "        position = torch.zeros(length,dim)\n",
    "        p = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        position[:,0::2] = torch.sin(p * div)\n",
    "        position[:,1::2] = torch.cos(p * div)\n",
    "        #position = position.transpose(0, 1).reshape(1,dim,length) #.contiguous()\n",
    "        position = position.reshape(length, 1, dim) #.contiguous()\n",
    "        self.register_buffer('position', position)\n",
    "\n",
    "        #self.position = nn.Parameter( torch.randn(1, dim, length) ) #random\n",
    "\n",
    "    def forward(self, x):\n",
    "        length, batch_size, _ = x.shape\n",
    "        position = self.position.repeat(1, batch_size, 1)\n",
    "        position = position[:length, :, :, ].contiguous()\n",
    "        return position\n",
    "\n",
    "\n",
    "class Conv1dStack(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n",
    "        super(Conv1dStack, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.res = nn.Sequential(\n",
    "            nn.Conv1d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        h = self.res(x)\n",
    "        return x + h\n",
    "    \n",
    "class SELayer1D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer1D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _= x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.conv0 = Conv1dStack(in_dim, 128, 3, padding=1)\n",
    "        self.conv1 = Conv1dStack(128, 64, 6, padding=5, dilation=2)\n",
    "        self.conv2 = Conv1dStack(64, 32, 15, padding=7, dilation=1)\n",
    "        self.conv3 = Conv1dStack(32, 32, 30, padding=29, dilation=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.conv1(x1)\n",
    "        x3 = self.conv2(x2)\n",
    "        x4 = self.conv3(x3)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        # x = x.permute(0, 2, 1).contiguous()\n",
    "        # BATCH x 256 x seq_length\n",
    "        return x\n",
    "\n",
    "    \n",
    "class MyDeeperGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features,\n",
    "                 node_hidden_channels,\n",
    "                 edge_hidden_channels,\n",
    "                 num_layers, num_classes, seq_length = 143):\n",
    "        super(MyDeeperGCN, self).__init__()\n",
    "\n",
    "        self.node_encoder = GENConv(num_node_features, node_hidden_channels)\n",
    "        self.edge_encoder = Linear(num_edge_features, edge_hidden_channels)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(1, num_layers + 1):\n",
    "            conv = NNConv(node_hidden_channels, node_hidden_channels,\n",
    "                          MapE2NxN(edge_hidden_channels,\n",
    "                                   node_hidden_channels * node_hidden_channels,\n",
    "                                   hidden_channels3))\n",
    "            norm = LayerNorm(node_hidden_channels, elementwise_affine=True)\n",
    "            act = ReLU(inplace=True)\n",
    "\n",
    "            layer = DeepGCNLayer(conv, norm, act, block='res+',\n",
    "                                 dropout=dropout1, ckpt_grad=i % 3)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.postition = PositionEncode(node_hidden_channels)\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(node_hidden_channels, 8, 256, dropout=0.2, activation='relu'),\n",
    "            2\n",
    "        )\n",
    "\n",
    "        self.lin1 = Linear(node_hidden_channels, node_hidden_channels)\n",
    "        self.lin_final = Linear(256, num_classes)\n",
    "        self.dropout = Dropout(dropout2)\n",
    "        #self.conv0 = Conv1dStack(seq_length, seq_length, 3, padding=1)\n",
    "        self.seq_length = seq_length\n",
    "        #self.rnn = nn.GRU(node_hidden_channels*2, node_hidden_channels, batch_first=True, num_layers=2, bidirectional=True)\n",
    "        self.convEncoder0 = ConvEncoder(64)\n",
    "        self.convEncoder1 = ConvEncoder(256)\n",
    "        self.convEncoder2 = ConvEncoder(256)\n",
    "        self.convEncoder3 = ConvEncoder(256)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.x.shape[0]//self.seq_length ##hard code\n",
    "\n",
    "\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        # edge for paired nodes are excluded for encoding node\n",
    "        seq_edge_index = edge_index[:, edge_attr[:,0] == 0]\n",
    "        x = self.node_encoder(x, seq_edge_index)\n",
    "\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "\n",
    "        x = self.layers[0].conv(x, edge_index, edge_attr)\n",
    "\n",
    "        for layer in self.layers[1:]:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "\n",
    "        x = self.layers[0].act(self.layers[0].norm(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #----\n",
    "        #hard code\n",
    "        x = x.reshape(batch_size, -1, node_hidden_channels)\n",
    "        x = x.permute(1,0,2).contiguous() #length, batch_size, 128\n",
    "        \n",
    "        if 1:\n",
    "            pos = self.postition(x)\n",
    "            x = self.transformer(x+pos)\n",
    "\n",
    "        #----\n",
    "        predict = self.lin1(x)\n",
    "        predict = predict.permute(1,0,2).contiguous()\n",
    "        predict = self.convEncoder0(predict.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        predict = self.convEncoder1(predict.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        predict = self.convEncoder2(predict.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        predict = self.convEncoder3(predict.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        #print(predict.shape)\n",
    "        #predict, _ = self.rnn(predict)\n",
    "        #print(predict.shape)\n",
    "        predict = self.lin_final(predict)\n",
    "        predict = predict.reshape(-1, 3)  #to keep compatible with rest of code\n",
    "        return predict\n",
    "\n",
    "def weighted_mse_loss(prds, tgts, weight):\n",
    "    return torch.mean(weight * (prds - tgts)**2)\n",
    "\n",
    "def criterion(prds, tgts, weight=None):\n",
    "    if weight is None:\n",
    "        return (torch.sqrt(torch.nn.MSELoss()(prds[:,0], tgts[:,0])) +\n",
    "                torch.sqrt(torch.nn.MSELoss()(prds[:,1], tgts[:,1])) +\n",
    "                torch.sqrt(torch.nn.MSELoss()(prds[:,2], tgts[:,2]))) / 3\n",
    "    else:\n",
    "        return (torch.sqrt(weighted_mse_loss(prds[:,0], tgts[:,0], weight)) +\n",
    "                torch.sqrt(weighted_mse_loss(prds[:,1], tgts[:,1], weight)) +\n",
    "                torch.sqrt(weighted_mse_loss(prds[:,2], tgts[:,2], weight))) / 3\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "print('Reading', train_file)\n",
    "df_tr = pd.read_json(train_file, lines=True)\n",
    "add_y_cat(df_tr)\n",
    "\n",
    "is_clean = df_tr.apply(sample_is_clean, axis=1)\n",
    "df_clean = df_tr[is_clean].reset_index(drop=True)\n",
    "df_noisy = df_tr[is_clean==False].reset_index(drop=True)\n",
    "del df_tr\n",
    "\n",
    "#------------------------------------------------------------\n",
    "print('Training')\n",
    "all_ys = torch.zeros((0, 3)).to(device).detach()\n",
    "all_outs = torch.zeros((0, 3)).to(device).detach()\n",
    "best_model_states = []\n",
    "kf = StratifiedKFold(nb_fold, shuffle=True, random_state=seed)\n",
    "for fold, ((clean_train_idx, clean_valid_idx),\n",
    "           (noisy_train_idx, noisy_valid_idx)) \\\n",
    "               in enumerate(zip(kf.split(df_clean, df_clean['y_cat']),\n",
    "                                kf.split(df_noisy, df_noisy['y_cat']))):\n",
    "    print('Fold', fold)\n",
    "    #epochs = 5\n",
    "    #start_timer = timer()\n",
    "\n",
    "    out_dir = f'./{NAME}/sn1-fold-{fold}'#%fold\n",
    "    initial_checkpoint =         None #out_dir + '/checkpoint/00007200_model.pth' #\n",
    "\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint',] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "#     log = Logger()\n",
    "#     log.open(out_dir+'/log.train.txt',mode='a')\n",
    "#     log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n",
    "#     log.write('\\t%s\\n' % COMMON_STRING)\n",
    "#     log.write('\\t__file__ = %s\\n' % __file__)\n",
    "#     log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "#     log.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------\n",
    "    # build train data\n",
    "    df_train = df_clean.loc[clean_train_idx]\n",
    "    if train_with_noisy_data:\n",
    "        df_train_noisy = df_noisy.loc[noisy_train_idx]\n",
    "        df_train_noisy =            df_train_noisy[df_train_noisy.apply(calc_error_mean, axis=1) <=                           error_mean_limit]\n",
    "        df_train = df_train.append(df_train_noisy)\n",
    "    data_train = build_data(df_train.reset_index(drop=True), True)\n",
    "    del df_train\n",
    "    loader_train = DataLoader(data_train, batch_size=batch_size, num_workers=2,\n",
    "                              shuffle=True)\n",
    "\n",
    "    # build validation data\n",
    "    df_valid_clean = df_clean.loc[clean_valid_idx].reset_index(drop=True)\n",
    "    data_valid_clean = build_data(df_valid_clean, True)\n",
    "    del df_valid_clean\n",
    "    loader_valid_clean = DataLoader(data_valid_clean, batch_size=batch_size, num_workers=2,\n",
    "                                    shuffle=False)\n",
    "\n",
    "    model = MyDeeperGCN(data_train[0].num_node_features,\n",
    "                        data_train[0].num_edge_features,\n",
    "                        node_hidden_channels=node_hidden_channels,\n",
    "                        edge_hidden_channels=edge_hidden_channels,\n",
    "                        num_layers=num_layers,\n",
    "                        num_classes=3, seq_length=143).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "    best_mcrmse = np.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        #print('Epoch', epoch)\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        nb = 0\n",
    "        for data in (loader_train):\n",
    "            data = data.to(device)\n",
    "            mask = data.train_mask\n",
    "            weight = data.weight[mask]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)[mask]\n",
    "            y = data.y[mask]\n",
    "            loss = criterion(out, y, weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "            nb += y.size(0)\n",
    "\n",
    "            del data\n",
    "            del out\n",
    "            del y\n",
    "            del loss\n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "        train_loss /= nb\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        nb = 0\n",
    "        ys = torch.zeros((0, 3)).to(device).detach()\n",
    "        outs = torch.zeros((0, 3)).to(device).detach()\n",
    "        for data in (loader_valid_clean):\n",
    "            data = data.to(device)\n",
    "            mask = data.train_mask\n",
    "\n",
    "            out = model(data)[mask].detach()\n",
    "            y = data.y[mask].detach()\n",
    "            loss = criterion(out, y).detach()\n",
    "            valid_loss += loss.item() * y.size(0)\n",
    "            nb += y.size(0)\n",
    "\n",
    "            outs = torch.cat((outs, out), dim=0)\n",
    "            ys = torch.cat((ys, y), dim=0)\n",
    "\n",
    "            del data\n",
    "            del out\n",
    "            del y\n",
    "            del loss\n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "        valid_loss /= nb\n",
    "\n",
    "        mcrmse = criterion(outs, ys).item()\n",
    "        scheduler.step(mcrmse)\n",
    "        print('Epoch %3d |  T Loss: %0.5f  |  V Loss: %0.5f  |  V MCRMSE: %0.5f | %s \\n'%(\n",
    "            epoch, train_loss, valid_loss, mcrmse, 'min'))\n",
    "        \n",
    "        \n",
    "        if mcrmse < best_mcrmse:\n",
    "            #print('Best valid MCRMSE updated to', mcrmse)\n",
    "            best_mcrmse = mcrmse\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        if int(epoch)%5 ==0:\n",
    "            torch.save({'state_dict': model.state_dict(),}, out_dir + '/checkpoint/%08d_model.pth' % (int(epoch)))\n",
    "    print('#'*20, best_mcrmse)\n",
    "    del data_train\n",
    "    del data_valid_clean\n",
    "    #gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    best_model_states.append(best_model_state)\n",
    "\n",
    "    # predict for CV\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    for data in loader_valid_clean:\n",
    "        data = data.to(device)\n",
    "        mask = data.train_mask\n",
    "\n",
    "        out = model(data)[mask].detach()\n",
    "        y = data.y[mask].detach()\n",
    "\n",
    "        all_ys = torch.cat((all_ys, y), dim=0)\n",
    "        all_outs = torch.cat((all_outs, out), dim=0)\n",
    "\n",
    "        del data\n",
    "        del out\n",
    "        del y\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "# calculate MCRMSE by all training data\n",
    "print('CV MCRMSE ', criterion(all_outs, all_ys).item())\n",
    "del all_outs\n",
    "del all_ys\n",
    "#gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# predict for test data\n",
    "print('Predicting test data')\n",
    "print('Reading', test_file)\n",
    "df_te = pd.read_json(test_file, lines=True)\n",
    "\n",
    "df_te = df_te.query('seq_length==107').reset_index(drop=True)\n",
    "\n",
    "data_test = build_data(df_te, False)\n",
    "loader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "id_seqpos = build_id_seqpos(df_te)\n",
    "\n",
    "model = MyDeeperGCN(data_test[0].num_node_features,\n",
    "                    data_test[0].num_edge_features,\n",
    "                    node_hidden_channels=node_hidden_channels,\n",
    "                    edge_hidden_channels=edge_hidden_channels,\n",
    "                    num_layers=num_layers,\n",
    "                    num_classes=3, seq_length=143).to(device)\n",
    "\n",
    "preds = torch.zeros((len(id_seqpos), 3)).to(device).detach()\n",
    "for best_model_state in best_model_states:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "\n",
    "    outs = torch.zeros((0, 3)).to(device).detach()\n",
    "    for data in loader_test:\n",
    "        data = data.to(device)\n",
    "        mask = data.test_mask\n",
    "\n",
    "        out = model(data)[mask].detach()\n",
    "        outs = torch.cat((outs, out), dim=0)\n",
    "\n",
    "        del data\n",
    "        del out\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "    preds += outs\n",
    "preds /= len(best_model_states)\n",
    "preds = preds.cpu().numpy()\n",
    "\n",
    "df_sub_pub = pd.DataFrame({'id_seqpos': id_seqpos,\n",
    "                       'reactivity': preds[:,0],\n",
    "                       'deg_Mg_pH10': preds[:,1],\n",
    "                       'deg_pH10': 0,\n",
    "                       'deg_Mg_50C': preds[:,2],\n",
    "                       'deg_50C': 0})\n",
    "print('Writing submission.csv')\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# predict for test data\n",
    "print('Predicting test data')\n",
    "print('Reading', test_file)\n",
    "df_te = pd.read_json(test_file, lines=True)\n",
    "\n",
    "df_te = df_te.query('seq_length==130').reset_index(drop=True)\n",
    "\n",
    "data_test = build_data(df_te, False)\n",
    "loader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "id_seqpos = build_id_seqpos(df_te)\n",
    "\n",
    "model = MyDeeperGCN(data_test[0].num_node_features,\n",
    "                    data_test[0].num_edge_features,\n",
    "                    node_hidden_channels=node_hidden_channels,\n",
    "                    edge_hidden_channels=edge_hidden_channels,\n",
    "                    num_layers=num_layers,\n",
    "                    num_classes=3, seq_length=174).to(device)\n",
    "\n",
    "preds = torch.zeros((len(id_seqpos), 3)).to(device).detach()\n",
    "for best_model_state in best_model_states:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "\n",
    "    outs = torch.zeros((0, 3)).to(device).detach()\n",
    "    for data in loader_test:\n",
    "        data = data.to(device)\n",
    "        mask = data.test_mask\n",
    "\n",
    "        out = model(data)[mask].detach()\n",
    "        outs = torch.cat((outs, out), dim=0)\n",
    "\n",
    "        del data\n",
    "        del out\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "    preds += outs\n",
    "preds /= len(best_model_states)\n",
    "preds = preds.cpu().numpy()\n",
    "\n",
    "df_sub_pri = pd.DataFrame({'id_seqpos': id_seqpos,\n",
    "                       'reactivity': preds[:,0],\n",
    "                       'deg_Mg_pH10': preds[:,1],\n",
    "                       'deg_pH10': 0,\n",
    "                       'deg_Mg_50C': preds[:,2],\n",
    "                       'deg_50C': 0})\n",
    "print('Writing submission.csv')\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "df_sub = pd.concat([df_sub_pub, df_sub_pri]).reset_index(drop=True).sort_values('id_seqpos').reset_index(drop=True) \n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "best = pd.read_csv('ensemble_v37.csv').reset_index(drop=True).sort_values('id_seqpos').reset_index(drop=True) \n",
    "\n",
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
    "\n",
    "print(np.corrcoef(df_sub[pred_cols[0]], best[pred_cols[0]]))\n",
    "print(np.corrcoef(df_sub[pred_cols[1]], best[pred_cols[1]]))\n",
    "print(np.corrcoef(df_sub[pred_cols[3]], best[pred_cols[3]]))\n",
    "\n",
    "df_sub.to_csv(f'{NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyh",
   "language": "python",
   "name": "lyh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

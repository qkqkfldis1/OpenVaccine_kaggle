{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!! seed=815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [07:36:50] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../input//train.json\n",
      "Training\n",
      "Fold 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592173bdd871450f9fde8691ec08be7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.39059  |  V Loss: 0.33918  |  V MCRMSE: 0.34005 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.33812  |  V Loss: 0.31913  |  V MCRMSE: 0.32013 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30872  |  V Loss: 0.29105  |  V MCRMSE: 0.29218 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.28856  |  V Loss: 0.28454  |  V MCRMSE: 0.28566 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.27228  |  V Loss: 0.26911  |  V MCRMSE: 0.27037 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.26402  |  V Loss: 0.25527  |  V MCRMSE: 0.25683 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.25443  |  V Loss: 0.24303  |  V MCRMSE: 0.24433 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.24817  |  V Loss: 0.24238  |  V MCRMSE: 0.24391 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.24167  |  V Loss: 0.23958  |  V MCRMSE: 0.24089 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.23704  |  V Loss: 0.23448  |  V MCRMSE: 0.23599 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.23235  |  V Loss: 0.23179  |  V MCRMSE: 0.23328 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.22795  |  V Loss: 0.23611  |  V MCRMSE: 0.23768 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22461  |  V Loss: 0.23015  |  V MCRMSE: 0.23162 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.21886  |  V Loss: 0.22690  |  V MCRMSE: 0.22831 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.21509  |  V Loss: 0.22294  |  V MCRMSE: 0.22427 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.21393  |  V Loss: 0.22527  |  V MCRMSE: 0.22674 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.20814  |  V Loss: 0.22285  |  V MCRMSE: 0.22426 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.20514  |  V Loss: 0.22499  |  V MCRMSE: 0.22632 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.20332  |  V Loss: 0.21990  |  V MCRMSE: 0.22125 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.19971  |  V Loss: 0.22036  |  V MCRMSE: 0.22171 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.19760  |  V Loss: 0.22446  |  V MCRMSE: 0.22570 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.19494  |  V Loss: 0.21970  |  V MCRMSE: 0.22076 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.19185  |  V Loss: 0.21628  |  V MCRMSE: 0.21760 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.19027  |  V Loss: 0.21607  |  V MCRMSE: 0.21742 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.18712  |  V Loss: 0.21746  |  V MCRMSE: 0.21874 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.18448  |  V Loss: 0.21512  |  V MCRMSE: 0.21653 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.18276  |  V Loss: 0.21882  |  V MCRMSE: 0.22028 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.18198  |  V Loss: 0.22028  |  V MCRMSE: 0.22163 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.18028  |  V Loss: 0.21372  |  V MCRMSE: 0.21496 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.17817  |  V Loss: 0.22036  |  V MCRMSE: 0.22173 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.17570  |  V Loss: 0.21766  |  V MCRMSE: 0.21910 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.17443  |  V Loss: 0.21262  |  V MCRMSE: 0.21394 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17288  |  V Loss: 0.21541  |  V MCRMSE: 0.21659 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17236  |  V Loss: 0.21100  |  V MCRMSE: 0.21231 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.17018  |  V Loss: 0.21318  |  V MCRMSE: 0.21450 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.16837  |  V Loss: 0.21370  |  V MCRMSE: 0.21514 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.16772  |  V Loss: 0.21293  |  V MCRMSE: 0.21439 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.16594  |  V Loss: 0.21375  |  V MCRMSE: 0.21503 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16674  |  V Loss: 0.21002  |  V MCRMSE: 0.21141 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16469  |  V Loss: 0.21058  |  V MCRMSE: 0.21194 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16338  |  V Loss: 0.21337  |  V MCRMSE: 0.21482 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16230  |  V Loss: 0.21074  |  V MCRMSE: 0.21213 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.16034  |  V Loss: 0.21183  |  V MCRMSE: 0.21334 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.16042  |  V Loss: 0.21029  |  V MCRMSE: 0.21176 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.15978  |  V Loss: 0.21042  |  V MCRMSE: 0.21176 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.15913  |  V Loss: 0.21094  |  V MCRMSE: 0.21225 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.15741  |  V Loss: 0.20955  |  V MCRMSE: 0.21099 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.15620  |  V Loss: 0.21034  |  V MCRMSE: 0.21172 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.15571  |  V Loss: 0.21143  |  V MCRMSE: 0.21286 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.15601  |  V Loss: 0.21036  |  V MCRMSE: 0.21183 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15407  |  V Loss: 0.21044  |  V MCRMSE: 0.21191 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15339  |  V Loss: 0.20880  |  V MCRMSE: 0.21025 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15185  |  V Loss: 0.20795  |  V MCRMSE: 0.20923 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15219  |  V Loss: 0.20902  |  V MCRMSE: 0.21031 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15116  |  V Loss: 0.20963  |  V MCRMSE: 0.21101 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15016  |  V Loss: 0.20807  |  V MCRMSE: 0.20931 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.15051  |  V Loss: 0.20995  |  V MCRMSE: 0.21131 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.14919  |  V Loss: 0.20873  |  V MCRMSE: 0.21002 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.14902  |  V Loss: 0.20857  |  V MCRMSE: 0.20997 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.14849  |  V Loss: 0.20645  |  V MCRMSE: 0.20789 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.14784  |  V Loss: 0.21068  |  V MCRMSE: 0.21204 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.14739  |  V Loss: 0.20742  |  V MCRMSE: 0.20880 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.14600  |  V Loss: 0.20802  |  V MCRMSE: 0.20938 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.14631  |  V Loss: 0.20779  |  V MCRMSE: 0.20920 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.14525  |  V Loss: 0.20768  |  V MCRMSE: 0.20927 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.14435  |  V Loss: 0.20818  |  V MCRMSE: 0.20964 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.14388  |  V Loss: 0.20834  |  V MCRMSE: 0.20977 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.14376  |  V Loss: 0.20806  |  V MCRMSE: 0.20942 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.14256  |  V Loss: 0.20523  |  V MCRMSE: 0.20673 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.14129  |  V Loss: 0.20685  |  V MCRMSE: 0.20840 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.14208  |  V Loss: 0.20706  |  V MCRMSE: 0.20840 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.14187  |  V Loss: 0.20883  |  V MCRMSE: 0.21026 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.14082  |  V Loss: 0.20866  |  V MCRMSE: 0.21013 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.14097  |  V Loss: 0.20794  |  V MCRMSE: 0.20938 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.13986  |  V Loss: 0.20818  |  V MCRMSE: 0.20960 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.13944  |  V Loss: 0.20888  |  V MCRMSE: 0.21030 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.13920  |  V Loss: 0.20704  |  V MCRMSE: 0.20834 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.13908  |  V Loss: 0.20811  |  V MCRMSE: 0.20943 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.13906  |  V Loss: 0.20701  |  V MCRMSE: 0.20836 | min \n",
      "\n",
      "Epoch    80: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  79 |  T Loss: 0.13826  |  V Loss: 0.20759  |  V MCRMSE: 0.20893 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.13351  |  V Loss: 0.20492  |  V MCRMSE: 0.20635 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.13130  |  V Loss: 0.20487  |  V MCRMSE: 0.20626 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.13030  |  V Loss: 0.20458  |  V MCRMSE: 0.20594 | min \n",
      "\n",
      "Epoch  83 |  T Loss: 0.12929  |  V Loss: 0.20513  |  V MCRMSE: 0.20655 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.12936  |  V Loss: 0.20486  |  V MCRMSE: 0.20627 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.12867  |  V Loss: 0.20523  |  V MCRMSE: 0.20667 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.12843  |  V Loss: 0.20472  |  V MCRMSE: 0.20610 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.12759  |  V Loss: 0.20453  |  V MCRMSE: 0.20588 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.12797  |  V Loss: 0.20529  |  V MCRMSE: 0.20676 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.12728  |  V Loss: 0.20487  |  V MCRMSE: 0.20631 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.12763  |  V Loss: 0.20452  |  V MCRMSE: 0.20587 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.12719  |  V Loss: 0.20543  |  V MCRMSE: 0.20682 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.12726  |  V Loss: 0.20371  |  V MCRMSE: 0.20507 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.12696  |  V Loss: 0.20436  |  V MCRMSE: 0.20575 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.12640  |  V Loss: 0.20455  |  V MCRMSE: 0.20588 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.12575  |  V Loss: 0.20502  |  V MCRMSE: 0.20645 | min \n",
      "\n",
      "Epoch  96 |  T Loss: 0.12610  |  V Loss: 0.20445  |  V MCRMSE: 0.20587 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12593  |  V Loss: 0.20419  |  V MCRMSE: 0.20561 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12556  |  V Loss: 0.20458  |  V MCRMSE: 0.20601 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.12502  |  V Loss: 0.20415  |  V MCRMSE: 0.20555 | min \n",
      "\n",
      "\n",
      "#################### 0.20506513118743896\n",
      "Fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2cc235b0b648b989947e46a47b1f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.39335  |  V Loss: 0.46888  |  V MCRMSE: 0.50180 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.33545  |  V Loss: 0.31911  |  V MCRMSE: 0.31991 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30487  |  V Loss: 0.28705  |  V MCRMSE: 0.28786 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.28414  |  V Loss: 0.26522  |  V MCRMSE: 0.26618 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.26920  |  V Loss: 0.25837  |  V MCRMSE: 0.25934 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.26036  |  V Loss: 0.25405  |  V MCRMSE: 0.25509 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.25172  |  V Loss: 0.25047  |  V MCRMSE: 0.25146 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.24584  |  V Loss: 0.24735  |  V MCRMSE: 0.24830 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.24153  |  V Loss: 0.24296  |  V MCRMSE: 0.24403 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.23692  |  V Loss: 0.23763  |  V MCRMSE: 0.23868 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.23278  |  V Loss: 0.24346  |  V MCRMSE: 0.24444 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.22822  |  V Loss: 0.23490  |  V MCRMSE: 0.23595 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22366  |  V Loss: 0.23279  |  V MCRMSE: 0.23378 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.22121  |  V Loss: 0.23149  |  V MCRMSE: 0.23253 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.21801  |  V Loss: 0.22964  |  V MCRMSE: 0.23062 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.21286  |  V Loss: 0.23430  |  V MCRMSE: 0.23520 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.21094  |  V Loss: 0.23087  |  V MCRMSE: 0.23186 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.20822  |  V Loss: 0.22966  |  V MCRMSE: 0.23068 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.20420  |  V Loss: 0.22837  |  V MCRMSE: 0.22942 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.20089  |  V Loss: 0.22801  |  V MCRMSE: 0.22900 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.19894  |  V Loss: 0.22576  |  V MCRMSE: 0.22684 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.19643  |  V Loss: 0.24140  |  V MCRMSE: 0.24301 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.19697  |  V Loss: 0.22643  |  V MCRMSE: 0.22750 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.19347  |  V Loss: 0.22328  |  V MCRMSE: 0.22421 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.19054  |  V Loss: 0.22325  |  V MCRMSE: 0.22446 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.18806  |  V Loss: 0.21955  |  V MCRMSE: 0.22053 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.18549  |  V Loss: 0.21978  |  V MCRMSE: 0.22086 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.18285  |  V Loss: 0.22292  |  V MCRMSE: 0.22390 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.18241  |  V Loss: 0.21770  |  V MCRMSE: 0.21885 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.18047  |  V Loss: 0.21831  |  V MCRMSE: 0.21937 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.18191  |  V Loss: 0.21988  |  V MCRMSE: 0.22103 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.17711  |  V Loss: 0.21770  |  V MCRMSE: 0.21875 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17455  |  V Loss: 0.21771  |  V MCRMSE: 0.21886 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17314  |  V Loss: 0.21896  |  V MCRMSE: 0.22012 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.17131  |  V Loss: 0.21666  |  V MCRMSE: 0.21776 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.17036  |  V Loss: 0.21511  |  V MCRMSE: 0.21611 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.16957  |  V Loss: 0.21831  |  V MCRMSE: 0.21948 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.16838  |  V Loss: 0.21574  |  V MCRMSE: 0.21681 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16613  |  V Loss: 0.21464  |  V MCRMSE: 0.21582 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16572  |  V Loss: 0.21877  |  V MCRMSE: 0.21993 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16544  |  V Loss: 0.21441  |  V MCRMSE: 0.21543 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16449  |  V Loss: 0.21630  |  V MCRMSE: 0.21738 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.16216  |  V Loss: 0.21599  |  V MCRMSE: 0.21716 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.16221  |  V Loss: 0.22165  |  V MCRMSE: 0.22281 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.16047  |  V Loss: 0.21475  |  V MCRMSE: 0.21582 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.15965  |  V Loss: 0.21438  |  V MCRMSE: 0.21553 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.15867  |  V Loss: 0.21616  |  V MCRMSE: 0.21736 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.15779  |  V Loss: 0.21101  |  V MCRMSE: 0.21216 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.15598  |  V Loss: 0.21448  |  V MCRMSE: 0.21570 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.15485  |  V Loss: 0.21362  |  V MCRMSE: 0.21477 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15481  |  V Loss: 0.21734  |  V MCRMSE: 0.21850 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15379  |  V Loss: 0.21156  |  V MCRMSE: 0.21268 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15297  |  V Loss: 0.21249  |  V MCRMSE: 0.21369 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15308  |  V Loss: 0.21183  |  V MCRMSE: 0.21292 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15255  |  V Loss: 0.21180  |  V MCRMSE: 0.21303 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15109  |  V Loss: 0.21209  |  V MCRMSE: 0.21325 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.14990  |  V Loss: 0.21150  |  V MCRMSE: 0.21266 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.14891  |  V Loss: 0.21445  |  V MCRMSE: 0.21562 | min \n",
      "\n",
      "Epoch    59: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  58 |  T Loss: 0.15003  |  V Loss: 0.21401  |  V MCRMSE: 0.21521 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.14330  |  V Loss: 0.21138  |  V MCRMSE: 0.21259 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.14092  |  V Loss: 0.20949  |  V MCRMSE: 0.21073 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.13905  |  V Loss: 0.20947  |  V MCRMSE: 0.21079 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.13851  |  V Loss: 0.20965  |  V MCRMSE: 0.21083 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.13859  |  V Loss: 0.20955  |  V MCRMSE: 0.21074 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.13750  |  V Loss: 0.20944  |  V MCRMSE: 0.21067 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.13705  |  V Loss: 0.20961  |  V MCRMSE: 0.21089 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.13745  |  V Loss: 0.21018  |  V MCRMSE: 0.21148 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.13689  |  V Loss: 0.20986  |  V MCRMSE: 0.21111 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.13633  |  V Loss: 0.20946  |  V MCRMSE: 0.21064 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.13574  |  V Loss: 0.21035  |  V MCRMSE: 0.21155 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.13564  |  V Loss: 0.20989  |  V MCRMSE: 0.21114 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.13487  |  V Loss: 0.20896  |  V MCRMSE: 0.21018 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.13516  |  V Loss: 0.20898  |  V MCRMSE: 0.21015 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.13467  |  V Loss: 0.20915  |  V MCRMSE: 0.21042 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.13398  |  V Loss: 0.21039  |  V MCRMSE: 0.21173 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.13428  |  V Loss: 0.21006  |  V MCRMSE: 0.21132 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.13369  |  V Loss: 0.20960  |  V MCRMSE: 0.21091 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.13338  |  V Loss: 0.21006  |  V MCRMSE: 0.21130 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.13259  |  V Loss: 0.21060  |  V MCRMSE: 0.21178 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.13301  |  V Loss: 0.20964  |  V MCRMSE: 0.21098 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.13304  |  V Loss: 0.20948  |  V MCRMSE: 0.21075 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.13202  |  V Loss: 0.20988  |  V MCRMSE: 0.21121 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.13245  |  V Loss: 0.21029  |  V MCRMSE: 0.21157 | min \n",
      "\n",
      "Epoch    84: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch  83 |  T Loss: 0.13187  |  V Loss: 0.20926  |  V MCRMSE: 0.21053 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.12882  |  V Loss: 0.20852  |  V MCRMSE: 0.20983 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.12725  |  V Loss: 0.20934  |  V MCRMSE: 0.21069 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.12723  |  V Loss: 0.20865  |  V MCRMSE: 0.20997 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.12664  |  V Loss: 0.20848  |  V MCRMSE: 0.20979 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.12641  |  V Loss: 0.20874  |  V MCRMSE: 0.21005 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.12621  |  V Loss: 0.20972  |  V MCRMSE: 0.21106 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.12635  |  V Loss: 0.20954  |  V MCRMSE: 0.21096 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.12559  |  V Loss: 0.20900  |  V MCRMSE: 0.21033 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.12571  |  V Loss: 0.20887  |  V MCRMSE: 0.21018 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.12512  |  V Loss: 0.20830  |  V MCRMSE: 0.20963 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.12502  |  V Loss: 0.20847  |  V MCRMSE: 0.20982 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.12506  |  V Loss: 0.20918  |  V MCRMSE: 0.21047 | min \n",
      "\n",
      "Epoch  96 |  T Loss: 0.12469  |  V Loss: 0.20815  |  V MCRMSE: 0.20946 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12471  |  V Loss: 0.20841  |  V MCRMSE: 0.20975 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12460  |  V Loss: 0.20863  |  V MCRMSE: 0.20997 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.12377  |  V Loss: 0.20883  |  V MCRMSE: 0.21008 | min \n",
      "\n",
      "\n",
      "#################### 0.2094610333442688\n",
      "Fold 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb24dc715424d8eb77f6fe2591bdab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.39917  |  V Loss: 0.35024  |  V MCRMSE: 0.35133 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.33277  |  V Loss: 0.32779  |  V MCRMSE: 0.32903 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30623  |  V Loss: 0.29629  |  V MCRMSE: 0.29754 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.28209  |  V Loss: 0.28943  |  V MCRMSE: 0.29087 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.26706  |  V Loss: 0.26717  |  V MCRMSE: 0.26894 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.25759  |  V Loss: 0.26138  |  V MCRMSE: 0.26334 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.24954  |  V Loss: 0.26231  |  V MCRMSE: 0.26409 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.24535  |  V Loss: 0.28545  |  V MCRMSE: 0.28753 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.24005  |  V Loss: 0.25125  |  V MCRMSE: 0.25316 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.23537  |  V Loss: 0.24721  |  V MCRMSE: 0.24913 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.23067  |  V Loss: 0.24580  |  V MCRMSE: 0.24782 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.22714  |  V Loss: 0.24221  |  V MCRMSE: 0.24427 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22269  |  V Loss: 0.24178  |  V MCRMSE: 0.24373 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.21826  |  V Loss: 0.23675  |  V MCRMSE: 0.23873 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.21451  |  V Loss: 0.23759  |  V MCRMSE: 0.23947 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.21320  |  V Loss: 0.23694  |  V MCRMSE: 0.23889 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.20994  |  V Loss: 0.23010  |  V MCRMSE: 0.23189 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.20641  |  V Loss: 0.23049  |  V MCRMSE: 0.23236 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.20390  |  V Loss: 0.23405  |  V MCRMSE: 0.23605 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.20132  |  V Loss: 0.23098  |  V MCRMSE: 0.23293 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.19710  |  V Loss: 0.22805  |  V MCRMSE: 0.22990 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.19502  |  V Loss: 0.22762  |  V MCRMSE: 0.22962 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.19406  |  V Loss: 0.22576  |  V MCRMSE: 0.22776 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.19121  |  V Loss: 0.22695  |  V MCRMSE: 0.22853 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.18743  |  V Loss: 0.22718  |  V MCRMSE: 0.22906 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.18691  |  V Loss: 0.23095  |  V MCRMSE: 0.23251 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.18574  |  V Loss: 0.21924  |  V MCRMSE: 0.22132 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.18195  |  V Loss: 0.22258  |  V MCRMSE: 0.22460 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.18050  |  V Loss: 0.22194  |  V MCRMSE: 0.22401 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.17916  |  V Loss: 0.22028  |  V MCRMSE: 0.22218 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.17759  |  V Loss: 0.21820  |  V MCRMSE: 0.22030 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.17454  |  V Loss: 0.22013  |  V MCRMSE: 0.22208 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17339  |  V Loss: 0.22169  |  V MCRMSE: 0.22371 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17227  |  V Loss: 0.22167  |  V MCRMSE: 0.22341 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.17123  |  V Loss: 0.22041  |  V MCRMSE: 0.22246 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.16918  |  V Loss: 0.21994  |  V MCRMSE: 0.22194 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.16866  |  V Loss: 0.21862  |  V MCRMSE: 0.22078 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.16637  |  V Loss: 0.21625  |  V MCRMSE: 0.21815 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16584  |  V Loss: 0.21600  |  V MCRMSE: 0.21803 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16528  |  V Loss: 0.21824  |  V MCRMSE: 0.22013 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16424  |  V Loss: 0.21619  |  V MCRMSE: 0.21819 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16436  |  V Loss: 0.21728  |  V MCRMSE: 0.21940 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.16226  |  V Loss: 0.21575  |  V MCRMSE: 0.21783 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.15966  |  V Loss: 0.21523  |  V MCRMSE: 0.21721 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.15994  |  V Loss: 0.21710  |  V MCRMSE: 0.21906 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.15880  |  V Loss: 0.21665  |  V MCRMSE: 0.21847 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.15931  |  V Loss: 0.21643  |  V MCRMSE: 0.21846 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.15692  |  V Loss: 0.21793  |  V MCRMSE: 0.21987 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.15613  |  V Loss: 0.21522  |  V MCRMSE: 0.21717 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.15465  |  V Loss: 0.21528  |  V MCRMSE: 0.21723 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15461  |  V Loss: 0.21574  |  V MCRMSE: 0.21774 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15420  |  V Loss: 0.21586  |  V MCRMSE: 0.21788 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15270  |  V Loss: 0.21569  |  V MCRMSE: 0.21773 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15246  |  V Loss: 0.21513  |  V MCRMSE: 0.21734 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15049  |  V Loss: 0.21607  |  V MCRMSE: 0.21808 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15030  |  V Loss: 0.21412  |  V MCRMSE: 0.21609 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.15138  |  V Loss: 0.21486  |  V MCRMSE: 0.21688 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.14965  |  V Loss: 0.21558  |  V MCRMSE: 0.21752 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.14864  |  V Loss: 0.21547  |  V MCRMSE: 0.21724 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.14920  |  V Loss: 0.21554  |  V MCRMSE: 0.21746 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.14755  |  V Loss: 0.21463  |  V MCRMSE: 0.21673 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.14826  |  V Loss: 0.21386  |  V MCRMSE: 0.21589 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.14662  |  V Loss: 0.21475  |  V MCRMSE: 0.21683 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.14530  |  V Loss: 0.21740  |  V MCRMSE: 0.21936 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.14630  |  V Loss: 0.21520  |  V MCRMSE: 0.21711 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.14539  |  V Loss: 0.21584  |  V MCRMSE: 0.21780 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.14434  |  V Loss: 0.21803  |  V MCRMSE: 0.22000 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.14388  |  V Loss: 0.21428  |  V MCRMSE: 0.21638 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.14336  |  V Loss: 0.21605  |  V MCRMSE: 0.21807 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.14301  |  V Loss: 0.21586  |  V MCRMSE: 0.21795 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.14259  |  V Loss: 0.21738  |  V MCRMSE: 0.21951 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.14165  |  V Loss: 0.21434  |  V MCRMSE: 0.21654 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.14149  |  V Loss: 0.21278  |  V MCRMSE: 0.21485 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.14044  |  V Loss: 0.21339  |  V MCRMSE: 0.21544 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.14065  |  V Loss: 0.21355  |  V MCRMSE: 0.21560 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.13980  |  V Loss: 0.21516  |  V MCRMSE: 0.21720 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.13894  |  V Loss: 0.21531  |  V MCRMSE: 0.21727 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.14135  |  V Loss: 0.21543  |  V MCRMSE: 0.21740 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.13919  |  V Loss: 0.21464  |  V MCRMSE: 0.21669 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.13838  |  V Loss: 0.21355  |  V MCRMSE: 0.21566 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.13764  |  V Loss: 0.21339  |  V MCRMSE: 0.21538 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.13748  |  V Loss: 0.21427  |  V MCRMSE: 0.21643 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.13690  |  V Loss: 0.21328  |  V MCRMSE: 0.21538 | min \n",
      "\n",
      "Epoch    84: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  83 |  T Loss: 0.13683  |  V Loss: 0.21457  |  V MCRMSE: 0.21661 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.13237  |  V Loss: 0.21093  |  V MCRMSE: 0.21305 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.13026  |  V Loss: 0.21079  |  V MCRMSE: 0.21298 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.12924  |  V Loss: 0.21102  |  V MCRMSE: 0.21320 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.12826  |  V Loss: 0.21111  |  V MCRMSE: 0.21332 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.12798  |  V Loss: 0.21139  |  V MCRMSE: 0.21359 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.12763  |  V Loss: 0.21237  |  V MCRMSE: 0.21451 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.12735  |  V Loss: 0.21136  |  V MCRMSE: 0.21356 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.12654  |  V Loss: 0.21149  |  V MCRMSE: 0.21370 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.12633  |  V Loss: 0.21288  |  V MCRMSE: 0.21506 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.12682  |  V Loss: 0.21194  |  V MCRMSE: 0.21412 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.12623  |  V Loss: 0.21255  |  V MCRMSE: 0.21478 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.12586  |  V Loss: 0.21275  |  V MCRMSE: 0.21495 | min \n",
      "\n",
      "Epoch    97: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch  96 |  T Loss: 0.12530  |  V Loss: 0.21156  |  V MCRMSE: 0.21366 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12371  |  V Loss: 0.21143  |  V MCRMSE: 0.21366 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12267  |  V Loss: 0.21122  |  V MCRMSE: 0.21339 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.12223  |  V Loss: 0.21164  |  V MCRMSE: 0.21380 | min \n",
      "\n",
      "\n",
      "#################### 0.21298335492610931\n",
      "Fold 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c168765ee204f1a867de7ecb37c43ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.39317  |  V Loss: 0.34850  |  V MCRMSE: 0.34913 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.33035  |  V Loss: 0.32627  |  V MCRMSE: 0.32707 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30283  |  V Loss: 0.29376  |  V MCRMSE: 0.29444 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.27971  |  V Loss: 0.28335  |  V MCRMSE: 0.28406 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.26579  |  V Loss: 0.27465  |  V MCRMSE: 0.27556 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.25745  |  V Loss: 0.26418  |  V MCRMSE: 0.26513 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.25011  |  V Loss: 0.26273  |  V MCRMSE: 0.26386 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.24310  |  V Loss: 0.25892  |  V MCRMSE: 0.25982 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.23848  |  V Loss: 0.24859  |  V MCRMSE: 0.24965 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.23187  |  V Loss: 0.24848  |  V MCRMSE: 0.24952 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.22838  |  V Loss: 0.24445  |  V MCRMSE: 0.24554 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.22509  |  V Loss: 0.25146  |  V MCRMSE: 0.25261 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22799  |  V Loss: 0.25796  |  V MCRMSE: 0.25905 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.22434  |  V Loss: 0.24977  |  V MCRMSE: 0.25090 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.21545  |  V Loss: 0.24071  |  V MCRMSE: 0.24185 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.21140  |  V Loss: 0.23739  |  V MCRMSE: 0.23855 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.20821  |  V Loss: 0.23987  |  V MCRMSE: 0.24101 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.20460  |  V Loss: 0.24853  |  V MCRMSE: 0.24965 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.20406  |  V Loss: 0.23558  |  V MCRMSE: 0.23689 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.19989  |  V Loss: 0.23365  |  V MCRMSE: 0.23496 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.19750  |  V Loss: 0.23532  |  V MCRMSE: 0.23648 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.19571  |  V Loss: 0.23581  |  V MCRMSE: 0.23695 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.19207  |  V Loss: 0.23539  |  V MCRMSE: 0.23662 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.18964  |  V Loss: 0.23132  |  V MCRMSE: 0.23260 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.18803  |  V Loss: 0.23126  |  V MCRMSE: 0.23255 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.18628  |  V Loss: 0.22783  |  V MCRMSE: 0.22929 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.18258  |  V Loss: 0.23028  |  V MCRMSE: 0.23163 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.18121  |  V Loss: 0.23007  |  V MCRMSE: 0.23141 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.17938  |  V Loss: 0.22884  |  V MCRMSE: 0.23015 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.17844  |  V Loss: 0.23102  |  V MCRMSE: 0.23257 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.17660  |  V Loss: 0.23144  |  V MCRMSE: 0.23269 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.17518  |  V Loss: 0.22969  |  V MCRMSE: 0.23114 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17340  |  V Loss: 0.22800  |  V MCRMSE: 0.22941 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17028  |  V Loss: 0.22714  |  V MCRMSE: 0.22853 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.16988  |  V Loss: 0.22785  |  V MCRMSE: 0.22910 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.17002  |  V Loss: 0.22956  |  V MCRMSE: 0.23083 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.16855  |  V Loss: 0.22729  |  V MCRMSE: 0.22863 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.16826  |  V Loss: 0.22703  |  V MCRMSE: 0.22834 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16780  |  V Loss: 0.22756  |  V MCRMSE: 0.22887 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16602  |  V Loss: 0.22824  |  V MCRMSE: 0.22956 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16378  |  V Loss: 0.22597  |  V MCRMSE: 0.22728 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16291  |  V Loss: 0.22675  |  V MCRMSE: 0.22814 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.16168  |  V Loss: 0.22676  |  V MCRMSE: 0.22809 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.16088  |  V Loss: 0.22378  |  V MCRMSE: 0.22505 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.15990  |  V Loss: 0.22466  |  V MCRMSE: 0.22600 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.15914  |  V Loss: 0.22386  |  V MCRMSE: 0.22521 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.15797  |  V Loss: 0.22559  |  V MCRMSE: 0.22715 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.15709  |  V Loss: 0.22432  |  V MCRMSE: 0.22556 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.15750  |  V Loss: 0.22223  |  V MCRMSE: 0.22367 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.15571  |  V Loss: 0.22208  |  V MCRMSE: 0.22350 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15417  |  V Loss: 0.22326  |  V MCRMSE: 0.22476 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15366  |  V Loss: 0.22274  |  V MCRMSE: 0.22407 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15309  |  V Loss: 0.22421  |  V MCRMSE: 0.22552 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15135  |  V Loss: 0.22547  |  V MCRMSE: 0.22681 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15097  |  V Loss: 0.22480  |  V MCRMSE: 0.22612 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15084  |  V Loss: 0.22430  |  V MCRMSE: 0.22565 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.14970  |  V Loss: 0.22499  |  V MCRMSE: 0.22631 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.14984  |  V Loss: 0.22371  |  V MCRMSE: 0.22510 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.14922  |  V Loss: 0.22384  |  V MCRMSE: 0.22524 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.14901  |  V Loss: 0.22123  |  V MCRMSE: 0.22265 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.14719  |  V Loss: 0.22379  |  V MCRMSE: 0.22504 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.14740  |  V Loss: 0.22249  |  V MCRMSE: 0.22376 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.14638  |  V Loss: 0.22255  |  V MCRMSE: 0.22392 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.14580  |  V Loss: 0.22192  |  V MCRMSE: 0.22325 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.14568  |  V Loss: 0.22350  |  V MCRMSE: 0.22488 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.14408  |  V Loss: 0.22357  |  V MCRMSE: 0.22492 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.14448  |  V Loss: 0.22393  |  V MCRMSE: 0.22539 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.14322  |  V Loss: 0.22578  |  V MCRMSE: 0.22726 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.14285  |  V Loss: 0.22529  |  V MCRMSE: 0.22673 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.14197  |  V Loss: 0.22360  |  V MCRMSE: 0.22496 | min \n",
      "\n",
      "Epoch    71: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  70 |  T Loss: 0.14107  |  V Loss: 0.22163  |  V MCRMSE: 0.22289 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.13680  |  V Loss: 0.22137  |  V MCRMSE: 0.22274 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.13400  |  V Loss: 0.22071  |  V MCRMSE: 0.22207 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.13310  |  V Loss: 0.21906  |  V MCRMSE: 0.22048 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.13266  |  V Loss: 0.22023  |  V MCRMSE: 0.22163 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.13198  |  V Loss: 0.21893  |  V MCRMSE: 0.22026 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.13196  |  V Loss: 0.22021  |  V MCRMSE: 0.22159 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.13117  |  V Loss: 0.21955  |  V MCRMSE: 0.22088 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.13103  |  V Loss: 0.21965  |  V MCRMSE: 0.22108 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.13059  |  V Loss: 0.21982  |  V MCRMSE: 0.22124 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.13058  |  V Loss: 0.21990  |  V MCRMSE: 0.22123 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.12989  |  V Loss: 0.21957  |  V MCRMSE: 0.22093 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.12994  |  V Loss: 0.22068  |  V MCRMSE: 0.22204 | min \n",
      "\n",
      "Epoch  83 |  T Loss: 0.13024  |  V Loss: 0.21972  |  V MCRMSE: 0.22109 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.12936  |  V Loss: 0.22067  |  V MCRMSE: 0.22201 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.12889  |  V Loss: 0.21986  |  V MCRMSE: 0.22125 | min \n",
      "\n",
      "Epoch    87: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch  86 |  T Loss: 0.12847  |  V Loss: 0.21947  |  V MCRMSE: 0.22081 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.12690  |  V Loss: 0.21890  |  V MCRMSE: 0.22020 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.12554  |  V Loss: 0.21840  |  V MCRMSE: 0.21973 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.12448  |  V Loss: 0.21868  |  V MCRMSE: 0.22007 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.12414  |  V Loss: 0.21904  |  V MCRMSE: 0.22037 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.12438  |  V Loss: 0.21863  |  V MCRMSE: 0.21995 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.12406  |  V Loss: 0.21855  |  V MCRMSE: 0.21992 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.12375  |  V Loss: 0.21904  |  V MCRMSE: 0.22039 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.12366  |  V Loss: 0.22008  |  V MCRMSE: 0.22139 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.12299  |  V Loss: 0.21852  |  V MCRMSE: 0.21985 | min \n",
      "\n",
      "Epoch  96 |  T Loss: 0.12320  |  V Loss: 0.21889  |  V MCRMSE: 0.22019 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.12257  |  V Loss: 0.21917  |  V MCRMSE: 0.22046 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.12267  |  V Loss: 0.21919  |  V MCRMSE: 0.22052 | min \n",
      "\n",
      "Epoch   100: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch  99 |  T Loss: 0.12234  |  V Loss: 0.21911  |  V MCRMSE: 0.22041 | min \n",
      "\n",
      "\n",
      "#################### 0.21973460912704468\n",
      "Fold 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b023bd483a64d94bbabcf57ec485b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 |  T Loss: 0.38621  |  V Loss: 0.39929  |  V MCRMSE: 0.41055 | min \n",
      "\n",
      "Epoch   1 |  T Loss: 0.33181  |  V Loss: 0.32850  |  V MCRMSE: 0.32946 | min \n",
      "\n",
      "Epoch   2 |  T Loss: 0.30346  |  V Loss: 0.29551  |  V MCRMSE: 0.29652 | min \n",
      "\n",
      "Epoch   3 |  T Loss: 0.28155  |  V Loss: 0.27105  |  V MCRMSE: 0.27213 | min \n",
      "\n",
      "Epoch   4 |  T Loss: 0.26760  |  V Loss: 0.27308  |  V MCRMSE: 0.27405 | min \n",
      "\n",
      "Epoch   5 |  T Loss: 0.25941  |  V Loss: 0.27433  |  V MCRMSE: 0.27537 | min \n",
      "\n",
      "Epoch   6 |  T Loss: 0.25228  |  V Loss: 0.25786  |  V MCRMSE: 0.25895 | min \n",
      "\n",
      "Epoch   7 |  T Loss: 0.24575  |  V Loss: 0.25948  |  V MCRMSE: 0.26054 | min \n",
      "\n",
      "Epoch   8 |  T Loss: 0.24222  |  V Loss: 0.24698  |  V MCRMSE: 0.24820 | min \n",
      "\n",
      "Epoch   9 |  T Loss: 0.23850  |  V Loss: 0.95417  |  V MCRMSE: 1.71087 | min \n",
      "\n",
      "Epoch  10 |  T Loss: 0.24791  |  V Loss: 0.25467  |  V MCRMSE: 0.25575 | min \n",
      "\n",
      "Epoch  11 |  T Loss: 0.23458  |  V Loss: 0.24416  |  V MCRMSE: 0.24534 | min \n",
      "\n",
      "Epoch  12 |  T Loss: 0.22841  |  V Loss: 0.24126  |  V MCRMSE: 0.24247 | min \n",
      "\n",
      "Epoch  13 |  T Loss: 0.22435  |  V Loss: 0.24202  |  V MCRMSE: 0.24327 | min \n",
      "\n",
      "Epoch  14 |  T Loss: 0.22097  |  V Loss: 0.23688  |  V MCRMSE: 0.23808 | min \n",
      "\n",
      "Epoch  15 |  T Loss: 0.21746  |  V Loss: 0.23597  |  V MCRMSE: 0.23727 | min \n",
      "\n",
      "Epoch  16 |  T Loss: 0.21310  |  V Loss: 0.23800  |  V MCRMSE: 0.23919 | min \n",
      "\n",
      "Epoch  17 |  T Loss: 0.21253  |  V Loss: 0.23893  |  V MCRMSE: 0.24020 | min \n",
      "\n",
      "Epoch  18 |  T Loss: 0.20802  |  V Loss: 0.23579  |  V MCRMSE: 0.23703 | min \n",
      "\n",
      "Epoch  19 |  T Loss: 0.20948  |  V Loss: 0.23308  |  V MCRMSE: 0.23421 | min \n",
      "\n",
      "Epoch  20 |  T Loss: 0.20710  |  V Loss: 0.23904  |  V MCRMSE: 0.24026 | min \n",
      "\n",
      "Epoch  21 |  T Loss: 0.20759  |  V Loss: 0.23056  |  V MCRMSE: 0.23186 | min \n",
      "\n",
      "Epoch  22 |  T Loss: 0.20117  |  V Loss: 0.23299  |  V MCRMSE: 0.23434 | min \n",
      "\n",
      "Epoch  23 |  T Loss: 0.19827  |  V Loss: 0.22803  |  V MCRMSE: 0.22930 | min \n",
      "\n",
      "Epoch  24 |  T Loss: 0.19446  |  V Loss: 0.22876  |  V MCRMSE: 0.23016 | min \n",
      "\n",
      "Epoch  25 |  T Loss: 0.19211  |  V Loss: 0.22721  |  V MCRMSE: 0.22852 | min \n",
      "\n",
      "Epoch  26 |  T Loss: 0.19068  |  V Loss: 0.22882  |  V MCRMSE: 0.23007 | min \n",
      "\n",
      "Epoch  27 |  T Loss: 0.18899  |  V Loss: 0.22899  |  V MCRMSE: 0.23049 | min \n",
      "\n",
      "Epoch  28 |  T Loss: 0.18567  |  V Loss: 0.23692  |  V MCRMSE: 0.23826 | min \n",
      "\n",
      "Epoch  29 |  T Loss: 0.18467  |  V Loss: 0.23062  |  V MCRMSE: 0.23209 | min \n",
      "\n",
      "Epoch  30 |  T Loss: 0.18278  |  V Loss: 0.22762  |  V MCRMSE: 0.22907 | min \n",
      "\n",
      "Epoch  31 |  T Loss: 0.18199  |  V Loss: 0.22496  |  V MCRMSE: 0.22629 | min \n",
      "\n",
      "Epoch  32 |  T Loss: 0.17828  |  V Loss: 0.22349  |  V MCRMSE: 0.22498 | min \n",
      "\n",
      "Epoch  33 |  T Loss: 0.17839  |  V Loss: 0.22330  |  V MCRMSE: 0.22479 | min \n",
      "\n",
      "Epoch  34 |  T Loss: 0.17516  |  V Loss: 0.22544  |  V MCRMSE: 0.22684 | min \n",
      "\n",
      "Epoch  35 |  T Loss: 0.17503  |  V Loss: 0.22724  |  V MCRMSE: 0.22869 | min \n",
      "\n",
      "Epoch  36 |  T Loss: 0.17333  |  V Loss: 0.22098  |  V MCRMSE: 0.22251 | min \n",
      "\n",
      "Epoch  37 |  T Loss: 0.17071  |  V Loss: 0.22653  |  V MCRMSE: 0.22801 | min \n",
      "\n",
      "Epoch  38 |  T Loss: 0.16983  |  V Loss: 0.22308  |  V MCRMSE: 0.22452 | min \n",
      "\n",
      "Epoch  39 |  T Loss: 0.16945  |  V Loss: 0.22433  |  V MCRMSE: 0.22584 | min \n",
      "\n",
      "Epoch  40 |  T Loss: 0.16770  |  V Loss: 0.22387  |  V MCRMSE: 0.22529 | min \n",
      "\n",
      "Epoch  41 |  T Loss: 0.16770  |  V Loss: 0.22348  |  V MCRMSE: 0.22506 | min \n",
      "\n",
      "Epoch  42 |  T Loss: 0.16733  |  V Loss: 0.22006  |  V MCRMSE: 0.22157 | min \n",
      "\n",
      "Epoch  43 |  T Loss: 0.16576  |  V Loss: 0.22288  |  V MCRMSE: 0.22436 | min \n",
      "\n",
      "Epoch  44 |  T Loss: 0.16378  |  V Loss: 0.21872  |  V MCRMSE: 0.22019 | min \n",
      "\n",
      "Epoch  45 |  T Loss: 0.16277  |  V Loss: 0.22013  |  V MCRMSE: 0.22170 | min \n",
      "\n",
      "Epoch  46 |  T Loss: 0.16138  |  V Loss: 0.22243  |  V MCRMSE: 0.22386 | min \n",
      "\n",
      "Epoch  47 |  T Loss: 0.16179  |  V Loss: 0.22256  |  V MCRMSE: 0.22413 | min \n",
      "\n",
      "Epoch  48 |  T Loss: 0.16013  |  V Loss: 0.22158  |  V MCRMSE: 0.22312 | min \n",
      "\n",
      "Epoch  49 |  T Loss: 0.16028  |  V Loss: 0.22084  |  V MCRMSE: 0.22236 | min \n",
      "\n",
      "Epoch  50 |  T Loss: 0.15965  |  V Loss: 0.21991  |  V MCRMSE: 0.22141 | min \n",
      "\n",
      "Epoch  51 |  T Loss: 0.15735  |  V Loss: 0.22034  |  V MCRMSE: 0.22188 | min \n",
      "\n",
      "Epoch  52 |  T Loss: 0.15629  |  V Loss: 0.21991  |  V MCRMSE: 0.22155 | min \n",
      "\n",
      "Epoch  53 |  T Loss: 0.15620  |  V Loss: 0.22005  |  V MCRMSE: 0.22150 | min \n",
      "\n",
      "Epoch  54 |  T Loss: 0.15643  |  V Loss: 0.21846  |  V MCRMSE: 0.21983 | min \n",
      "\n",
      "Epoch  55 |  T Loss: 0.15466  |  V Loss: 0.22219  |  V MCRMSE: 0.22369 | min \n",
      "\n",
      "Epoch  56 |  T Loss: 0.15467  |  V Loss: 0.21826  |  V MCRMSE: 0.21977 | min \n",
      "\n",
      "Epoch  57 |  T Loss: 0.15244  |  V Loss: 0.21779  |  V MCRMSE: 0.21941 | min \n",
      "\n",
      "Epoch  58 |  T Loss: 0.15287  |  V Loss: 0.22039  |  V MCRMSE: 0.22196 | min \n",
      "\n",
      "Epoch  59 |  T Loss: 0.15095  |  V Loss: 0.21927  |  V MCRMSE: 0.22085 | min \n",
      "\n",
      "Epoch  60 |  T Loss: 0.15144  |  V Loss: 0.21699  |  V MCRMSE: 0.21862 | min \n",
      "\n",
      "Epoch  61 |  T Loss: 0.15056  |  V Loss: 0.22036  |  V MCRMSE: 0.22188 | min \n",
      "\n",
      "Epoch  62 |  T Loss: 0.14901  |  V Loss: 0.21805  |  V MCRMSE: 0.21963 | min \n",
      "\n",
      "Epoch  63 |  T Loss: 0.14951  |  V Loss: 0.21835  |  V MCRMSE: 0.22005 | min \n",
      "\n",
      "Epoch  64 |  T Loss: 0.14733  |  V Loss: 0.21787  |  V MCRMSE: 0.21939 | min \n",
      "\n",
      "Epoch  65 |  T Loss: 0.14656  |  V Loss: 0.21725  |  V MCRMSE: 0.21877 | min \n",
      "\n",
      "Epoch  66 |  T Loss: 0.14723  |  V Loss: 0.21850  |  V MCRMSE: 0.22007 | min \n",
      "\n",
      "Epoch  67 |  T Loss: 0.14680  |  V Loss: 0.21829  |  V MCRMSE: 0.21979 | min \n",
      "\n",
      "Epoch  68 |  T Loss: 0.14644  |  V Loss: 0.21768  |  V MCRMSE: 0.21928 | min \n",
      "\n",
      "Epoch  69 |  T Loss: 0.14546  |  V Loss: 0.21805  |  V MCRMSE: 0.21974 | min \n",
      "\n",
      "Epoch  70 |  T Loss: 0.14525  |  V Loss: 0.21711  |  V MCRMSE: 0.21857 | min \n",
      "\n",
      "Epoch  71 |  T Loss: 0.14433  |  V Loss: 0.21707  |  V MCRMSE: 0.21859 | min \n",
      "\n",
      "Epoch  72 |  T Loss: 0.14528  |  V Loss: 0.21741  |  V MCRMSE: 0.21899 | min \n",
      "\n",
      "Epoch  73 |  T Loss: 0.14380  |  V Loss: 0.22104  |  V MCRMSE: 0.22265 | min \n",
      "\n",
      "Epoch  74 |  T Loss: 0.14453  |  V Loss: 0.21654  |  V MCRMSE: 0.21805 | min \n",
      "\n",
      "Epoch  75 |  T Loss: 0.14401  |  V Loss: 0.21683  |  V MCRMSE: 0.21840 | min \n",
      "\n",
      "Epoch  76 |  T Loss: 0.14257  |  V Loss: 0.21757  |  V MCRMSE: 0.21904 | min \n",
      "\n",
      "Epoch  77 |  T Loss: 0.14139  |  V Loss: 0.21671  |  V MCRMSE: 0.21828 | min \n",
      "\n",
      "Epoch  78 |  T Loss: 0.14175  |  V Loss: 0.21755  |  V MCRMSE: 0.21915 | min \n",
      "\n",
      "Epoch  79 |  T Loss: 0.14029  |  V Loss: 0.21624  |  V MCRMSE: 0.21783 | min \n",
      "\n",
      "Epoch  80 |  T Loss: 0.14001  |  V Loss: 0.21577  |  V MCRMSE: 0.21739 | min \n",
      "\n",
      "Epoch  81 |  T Loss: 0.14084  |  V Loss: 0.21745  |  V MCRMSE: 0.21899 | min \n",
      "\n",
      "Epoch  82 |  T Loss: 0.13992  |  V Loss: 0.21588  |  V MCRMSE: 0.21747 | min \n",
      "\n",
      "Epoch  83 |  T Loss: 0.14016  |  V Loss: 0.21733  |  V MCRMSE: 0.21889 | min \n",
      "\n",
      "Epoch  84 |  T Loss: 0.14055  |  V Loss: 0.21762  |  V MCRMSE: 0.21922 | min \n",
      "\n",
      "Epoch  85 |  T Loss: 0.13892  |  V Loss: 0.21560  |  V MCRMSE: 0.21724 | min \n",
      "\n",
      "Epoch  86 |  T Loss: 0.13823  |  V Loss: 0.21695  |  V MCRMSE: 0.21858 | min \n",
      "\n",
      "Epoch  87 |  T Loss: 0.13729  |  V Loss: 0.21570  |  V MCRMSE: 0.21730 | min \n",
      "\n",
      "Epoch  88 |  T Loss: 0.13722  |  V Loss: 0.21605  |  V MCRMSE: 0.21765 | min \n",
      "\n",
      "Epoch  89 |  T Loss: 0.13700  |  V Loss: 0.21545  |  V MCRMSE: 0.21705 | min \n",
      "\n",
      "Epoch  90 |  T Loss: 0.13632  |  V Loss: 0.21705  |  V MCRMSE: 0.21873 | min \n",
      "\n",
      "Epoch  91 |  T Loss: 0.13738  |  V Loss: 0.21611  |  V MCRMSE: 0.21783 | min \n",
      "\n",
      "Epoch  92 |  T Loss: 0.13610  |  V Loss: 0.21630  |  V MCRMSE: 0.21796 | min \n",
      "\n",
      "Epoch  93 |  T Loss: 0.13504  |  V Loss: 0.21508  |  V MCRMSE: 0.21668 | min \n",
      "\n",
      "Epoch  94 |  T Loss: 0.13586  |  V Loss: 0.21683  |  V MCRMSE: 0.21843 | min \n",
      "\n",
      "Epoch  95 |  T Loss: 0.13429  |  V Loss: 0.21453  |  V MCRMSE: 0.21613 | min \n",
      "\n",
      "Epoch  96 |  T Loss: 0.13368  |  V Loss: 0.21767  |  V MCRMSE: 0.21924 | min \n",
      "\n",
      "Epoch  97 |  T Loss: 0.13308  |  V Loss: 0.21613  |  V MCRMSE: 0.21764 | min \n",
      "\n",
      "Epoch  98 |  T Loss: 0.13359  |  V Loss: 0.21657  |  V MCRMSE: 0.21816 | min \n",
      "\n",
      "Epoch  99 |  T Loss: 0.13354  |  V Loss: 0.21510  |  V MCRMSE: 0.21670 | min \n",
      "\n",
      "\n",
      "#################### 0.2161296308040619\n",
      "CV MCRMSE  0.21273939311504364\n",
      "Predicting test data\n",
      "Reading ../input//test.json\n",
      "Writing submission.csv\n",
      "Predicting test data\n",
      "Reading ../input//test.json\n",
      "Writing submission.csv\n",
      "[[1.         0.92586472]\n",
      " [0.92586472 1.        ]]\n",
      "[[1.         0.89321673]\n",
      " [0.89321673 1.        ]]\n",
      "[[1.        0.8600036]\n",
      " [0.8600036 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import math\n",
    "import torch.nn as nn\n",
    "# fix seed ----------------------------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed = 815\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "def seed_py(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    return seed\n",
    "\n",
    "seed_torch(seed)\n",
    "seed_py(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "NAME = 'final_gnn_arma_deep_815_conv1d'\n",
    "\n",
    "print('!!!!! seed=%d'%seed)\n",
    "#---------------------------------------------------------------------------------\n",
    "## https://www.kaggle.com/symyksr/openvaccine-deepergcn #########################\n",
    "\n",
    "\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
    "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, ARMAConv, ClusterGCNConv, FeaStConv, GENConv, ARMAConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "data_dir = '../input/'\n",
    "train_file = data_dir+'/train.json'\n",
    "test_file  = data_dir+'/test.json'\n",
    "bpps_top   = data_dir+'/bpps'\n",
    "\n",
    "\n",
    "# settings\n",
    "\n",
    "train_with_noisy_data     = True\n",
    "add_edge_for_paired_nodes = True\n",
    "add_codon_nodes           = True\n",
    "\n",
    "bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n",
    "bpps_nb_std  = 0.08914   # std of bpps_nb across all training data\n",
    "error_mean_limit = 0.5\n",
    "\n",
    "nb_fold    = 5\n",
    "device     = 'cuda'\n",
    "batch_size = 16\n",
    "epochs     = 100\n",
    "lr         = 0.001\n",
    "T = 5\n",
    "node_hidden_channels = 64\n",
    "edge_hidden_channels = 64\n",
    "hidden_channels3 = 64\n",
    "num_layers = 4\n",
    "dropout1 = 0.1\n",
    "dropout2 = 0.1\n",
    "dropout3 = 0.1\n",
    "\n",
    "##################### all data preparation ####################################\n",
    "\n",
    "def match_pair(structure):\n",
    "    pair = [-1] * len(structure)\n",
    "    pair_no = -1\n",
    "\n",
    "    pair_no_stack = []\n",
    "    for i, c in enumerate(structure):\n",
    "        if c == '(':\n",
    "            pair_no += 1\n",
    "            pair[i] = pair_no\n",
    "            pair_no_stack.append(pair_no)\n",
    "        elif c == ')':\n",
    "            pair[i] = pair_no_stack.pop()\n",
    "    return pair\n",
    "\n",
    "class MyData(Data):\n",
    "    def __init__(self, x=None, edge_index=None, edge_attr=None, y=None,\n",
    "                 pos=None, norm=None, face=None, weight=None, **kwargs):\n",
    "        super(MyData, self).__init__(x=x, edge_index=edge_index,\n",
    "                                     edge_attr=edge_attr, y=y, pos=pos,\n",
    "                                     norm=norm, face=face, **kwargs)\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "def calc_error_mean(row):\n",
    "    reactivity_error = row['reactivity_error']\n",
    "    deg_error_Mg_pH10 = row['deg_error_Mg_pH10']\n",
    "    deg_error_Mg_50C = row['deg_error_Mg_50C']\n",
    "\n",
    "    return np.mean(np.abs(reactivity_error) +\n",
    "                   np.abs(deg_error_Mg_pH10) + \\\n",
    "                   np.abs(deg_error_Mg_50C)) / 3\n",
    "\n",
    "\n",
    "def calc_sample_weight(row):\n",
    "    if sample_is_clean(row):\n",
    "        return 1.\n",
    "    else:\n",
    "        error_mean = calc_error_mean(row)\n",
    "        if error_mean >= error_mean_limit:\n",
    "            return 0.\n",
    "\n",
    "        return 1. - error_mean / error_mean_limit\n",
    "\n",
    "\n",
    "# add directed edge for node1 -> node2 and for node2 -> node1\n",
    "def add_edges(edge_index, edge_features, node1, node2, feature1, feature2):\n",
    "    edge_index.append([node1, node2])\n",
    "    edge_features.append(feature1)\n",
    "    edge_index.append([node2, node1])\n",
    "    edge_features.append(feature2)\n",
    "\n",
    "\n",
    "def add_edges_between_base_nodes(edge_index, edge_features, node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        1, # forward edge: 1, backward edge: -1\n",
    "        1, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        -1, # forward edge: 1, backward edge: -1\n",
    "        1, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_paired_nodes(edge_index, edge_features, node1, node2,\n",
    "                                   bpps_value):\n",
    "    edge_feature1 = [\n",
    "        1, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        bpps_value, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        1, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        bpps_value, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_codon_nodes(edge_index, edge_features, node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        1, # is edge between coden nodes\n",
    "        1, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        0, # is edge between codon node and base node\n",
    "        1, # is edge between coden nodes\n",
    "        -1, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_edges_between_codon_and_base_node(edge_index, edge_features,\n",
    "                                          node1, node2):\n",
    "    edge_feature1 = [\n",
    "        0, # is edge for paired nodes\n",
    "        1, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    edge_feature2 = [\n",
    "        0, # is edge for paired nodes\n",
    "        1, # is edge between codon node and base node\n",
    "        0, # is edge between coden nodes\n",
    "        0, # forward edge: 1, backward edge: -1\n",
    "        0, # bpps if edge is for paired nodes\n",
    "    ]\n",
    "    add_edges(edge_index, edge_features, node1, node2,\n",
    "              edge_feature1, edge_feature2)\n",
    "\n",
    "def add_node(node_features, feature):\n",
    "    node_features.append(feature)\n",
    "\n",
    "\n",
    "def add_base_node(node_features, sequence, predicted_loop_type,\n",
    "                  bpps_sum, bpps_nb):\n",
    "    feature = [\n",
    "        0, # is codon node\n",
    "        sequence == 'A',\n",
    "        sequence == 'C',\n",
    "        sequence == 'G',\n",
    "        sequence == 'U',\n",
    "        predicted_loop_type == 'S',\n",
    "        predicted_loop_type == 'M',\n",
    "        predicted_loop_type == 'I',\n",
    "        predicted_loop_type == 'B',\n",
    "        predicted_loop_type == 'H',\n",
    "        predicted_loop_type == 'E',\n",
    "        predicted_loop_type == 'X',\n",
    "        bpps_sum,\n",
    "        bpps_nb,\n",
    "    ]\n",
    "    add_node(node_features, feature)\n",
    "\n",
    "def add_codon_node(node_features):\n",
    "    feature = [\n",
    "        1, # is codon node\n",
    "        0, # sequence == 'A',\n",
    "        0, # sequence == 'C',\n",
    "        0, # sequence == 'G',\n",
    "        0, # sequence == 'U',\n",
    "        0, # predicted_loop_type == 'S',\n",
    "        0, # predicted_loop_type == 'M',\n",
    "        0, # predicted_loop_type == 'I',\n",
    "        0, # predicted_loop_type == 'B',\n",
    "        0, # predicted_loop_type == 'H',\n",
    "        0, # predicted_loop_type == 'E',\n",
    "        0, # predicted_loop_type == 'X',\n",
    "        0, # bpps_sum\n",
    "        0, # bpps_nb\n",
    "    ]\n",
    "    add_node(node_features, feature)\n",
    "\n",
    "def build_data(df, is_train):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        targets = []\n",
    "        node_features = []\n",
    "        edge_features = []\n",
    "        edge_index = []\n",
    "        train_mask = []\n",
    "        test_mask = []\n",
    "        weights = []\n",
    "\n",
    "        id = df.loc[i, 'id']\n",
    "        path = os.path.join(bpps_top, id + '.npy')\n",
    "        bpps = np.load(path)\n",
    "        bpps_sum = bpps.sum(axis=0)\n",
    "        sequence = df.loc[i, 'sequence']\n",
    "        structure = df.loc[i, 'structure']\n",
    "        pair_info = match_pair(structure)\n",
    "        predicted_loop_type = df.loc[i, 'predicted_loop_type']\n",
    "        seq_length = df.loc[i, 'seq_length']\n",
    "        seq_scored = df.loc[i, 'seq_scored']\n",
    "        bpps_nb = (bpps > 0).sum(axis=0) / seq_length\n",
    "        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n",
    "        if is_train:\n",
    "            sample_weight = calc_sample_weight(df.loc[i])\n",
    "\n",
    "            reactivity = df.loc[i, 'reactivity']\n",
    "            deg_Mg_pH10 = df.loc[i, 'deg_Mg_pH10']\n",
    "            deg_Mg_50C = df.loc[i, 'deg_Mg_50C']\n",
    "\n",
    "            for j in range(seq_length):\n",
    "                if j < seq_scored:\n",
    "                    targets.append([\n",
    "                        reactivity[j],\n",
    "                        deg_Mg_pH10[j],\n",
    "                        deg_Mg_50C[j],\n",
    "                        ])\n",
    "                else:\n",
    "                    targets.append([0, 0, 0])\n",
    "\n",
    "        paired_nodes = {}\n",
    "        for j in range(seq_length):\n",
    "            add_base_node(node_features, sequence[j], predicted_loop_type[j],\n",
    "                          bpps_sum[j], bpps_nb[j])\n",
    "\n",
    "            if j + 1 < seq_length: # edge between current node and next node\n",
    "                add_edges_between_base_nodes(edge_index, edge_features,\n",
    "                                             j, j + 1)\n",
    "\n",
    "            if pair_info[j] != -1:\n",
    "                if pair_info[j] not in paired_nodes:\n",
    "                    paired_nodes[pair_info[j]] = [j]\n",
    "                else:\n",
    "                    paired_nodes[pair_info[j]].append(j)\n",
    "\n",
    "            train_mask.append(j < seq_scored)\n",
    "            test_mask.append(True)\n",
    "            if is_train:\n",
    "                weights.append(sample_weight)\n",
    "\n",
    "\n",
    "        if add_edge_for_paired_nodes:\n",
    "            for pair in paired_nodes.values():\n",
    "                bpps_value = bpps[pair[0], pair[1]]\n",
    "                add_edges_between_paired_nodes(edge_index, edge_features,\n",
    "                                               pair[0], pair[1], bpps_value)\n",
    "\n",
    "\n",
    "\n",
    "        if add_codon_nodes:\n",
    "            codon_node_idx = seq_length - 1\n",
    "            for j in range(seq_length):\n",
    "                if j % 3 == 0:\n",
    "                    # add codon node\n",
    "                    add_codon_node(node_features)\n",
    "                    codon_node_idx += 1\n",
    "                    train_mask.append(False)\n",
    "                    test_mask.append(False)\n",
    "                    if is_train:\n",
    "                        weights.append(0)\n",
    "                        targets.append([0, 0, 0])\n",
    "\n",
    "                    if codon_node_idx > seq_length:\n",
    "                        # add edges between adjacent codon nodes\n",
    "                        add_edges_between_codon_nodes(edge_index, edge_features,\n",
    "                                                      codon_node_idx - 1,\n",
    "                                                      codon_node_idx)\n",
    "\n",
    "                # add edges between codon node and base node\n",
    "                add_edges_between_codon_and_base_node(edge_index, edge_features,\n",
    "                                                      j, codon_node_idx)\n",
    "\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "        edge_index    = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "\n",
    "        if is_train:\n",
    "            data.append(MyData(x=node_features, edge_index=edge_index,\n",
    "                               edge_attr=edge_features,\n",
    "                               train_mask=torch.tensor(train_mask),\n",
    "                               weight=torch.tensor(weights, dtype=torch.float),\n",
    "                               y=torch.tensor(targets, dtype=torch.float)))\n",
    "        else:\n",
    "            data.append(MyData(x=node_features, edge_index=edge_index,\n",
    "                               edge_attr=edge_features,\n",
    "                               test_mask=torch.tensor(test_mask)))\n",
    "\n",
    "    return data\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "def build_id_seqpos(df):\n",
    "    id_seqpos = []\n",
    "    for i in range(len(df)):\n",
    "        id = df.loc[i, 'id']\n",
    "        seq_length = df.loc[i, 'seq_length']\n",
    "        for seqpos in range(seq_length):\n",
    "            id_seqpos.append(id + '_' + str(seqpos))\n",
    "    return id_seqpos\n",
    "\n",
    "def sample_is_clean(row):\n",
    "    return row['SN_filter'] == 1\n",
    "    #return row['signal_to_noise'] > 1 and \\\n",
    "    #       min((min(row['reactivity']),\n",
    "    #            min(row['deg_Mg_pH10']),\n",
    "    #            min(row['deg_pH10']),\n",
    "    #            min(row['deg_Mg_50C']),\n",
    "    #            min(row['deg_50C']))) > -0.5\n",
    "\n",
    "# categorical value for target (used for stratified kfold)\n",
    "def add_y_cat(df):\n",
    "    target_mean = df['reactivity'].apply(np.mean) +                   df['deg_Mg_pH10'].apply(np.mean) +                   df['deg_Mg_50C'].apply(np.mean)\n",
    "    df['y_cat'] = pd.qcut(np.array(target_mean), q=20).codes\n",
    "\n",
    "##################### all model preparation ####################################\n",
    "\n",
    "# originally copied from\n",
    "# https://github.com/rusty1s/pytorch_geometric/blob/master/examples/ogbn_proteins_deepgcn.py\n",
    "#\n",
    "class MapE2NxN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels):\n",
    "        super(MapE2NxN, self).__init__()\n",
    "        self.linear1 = Linear(in_channels, hidden_channels)\n",
    "        self.linear2 = Linear(hidden_channels, out_channels)\n",
    "        self.dropout = Dropout(dropout3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionEncode(nn.Module):\n",
    "    def __init__(self, dim, length=174):\n",
    "        super(PositionEncode, self).__init__()\n",
    "        position = torch.zeros(length,dim)\n",
    "        p = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        position[:,0::2] = torch.sin(p * div)\n",
    "        position[:,1::2] = torch.cos(p * div)\n",
    "        #position = position.transpose(0, 1).reshape(1,dim,length) #.contiguous()\n",
    "        position = position.reshape(length, 1, dim) #.contiguous()\n",
    "        self.register_buffer('position', position)\n",
    "\n",
    "        #self.position = nn.Parameter( torch.randn(1, dim, length) ) #random\n",
    "\n",
    "    def forward(self, x):\n",
    "        length, batch_size, _ = x.shape\n",
    "        position = self.position.repeat(1, batch_size, 1)\n",
    "        position = position[:length, :, :, ].contiguous()\n",
    "        return position\n",
    "\n",
    "\n",
    "class Conv1dStack(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n",
    "        super(Conv1dStack, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.res = nn.Sequential(\n",
    "            nn.Conv1d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        h = self.res(x)\n",
    "        return x + h\n",
    "    \n",
    "class SELayer1D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer1D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _= x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.conv0 = Conv1dStack(in_dim, 128, 3, padding=1)\n",
    "        self.conv1 = Conv1dStack(128, 64, 6, padding=5, dilation=2)\n",
    "        self.conv2 = Conv1dStack(64, 32, 15, padding=7, dilation=1)\n",
    "        self.conv3 = Conv1dStack(32, 32, 30, padding=29, dilation=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.conv1(x1)\n",
    "        x3 = self.conv2(x2)\n",
    "        x4 = self.conv3(x3)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        # x = x.permute(0, 2, 1).contiguous()\n",
    "        # BATCH x 256 x seq_length\n",
    "        return x\n",
    "\n",
    "    \n",
    "class MyDeeperGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features,\n",
    "                 node_hidden_channels,\n",
    "                 edge_hidden_channels,\n",
    "                 num_layers, num_classes, seq_length = 143):\n",
    "        super(MyDeeperGCN, self).__init__()\n",
    "\n",
    "        self.node_encoder = ARMAConv(num_node_features, node_hidden_channels)\n",
    "        self.edge_encoder = Linear(num_edge_features, edge_hidden_channels)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(1, num_layers + 1):\n",
    "            conv = NNConv(node_hidden_channels, node_hidden_channels,\n",
    "                          MapE2NxN(edge_hidden_channels,\n",
    "                                   node_hidden_channels * node_hidden_channels,\n",
    "                                   hidden_channels3))\n",
    "            norm = LayerNorm(node_hidden_channels, elementwise_affine=True)\n",
    "            act = ReLU(inplace=True)\n",
    "\n",
    "            layer = DeepGCNLayer(conv, norm, act, block='res+',\n",
    "                                 dropout=dropout1, ckpt_grad=i % 3)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.postition = PositionEncode(node_hidden_channels)\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(node_hidden_channels, 8, 256, dropout=0.2, activation='relu'),\n",
    "            2\n",
    "        )\n",
    "\n",
    "        self.lin1 = Linear(node_hidden_channels, node_hidden_channels)\n",
    "        self.lin_final = Linear(256, num_classes)\n",
    "        self.dropout = Dropout(dropout2)\n",
    "        #self.conv0 = Conv1dStack(seq_length, seq_length, 3, padding=1)\n",
    "        self.seq_length = seq_length\n",
    "        #self.rnn = nn.GRU(node_hidden_channels*2, node_hidden_channels, batch_first=True, num_layers=2, bidirectional=True)\n",
    "        self.convEncoder0 = ConvEncoder(64)\n",
    "        self.convEncoder1 = ConvEncoder(256)\n",
    "        self.convEncoder2 = ConvEncoder(256)\n",
    "        self.convEncoder3 = ConvEncoder(256)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size = data.x.shape[0]//self.seq_length ##hard code\n",
    "\n",
    "\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        # edge for paired nodes are excluded for encoding node\n",
    "        seq_edge_index = edge_index[:, edge_attr[:,0] == 0]\n",
    "        x = self.node_encoder(x, seq_edge_index)\n",
    "\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "\n",
    "        x = self.layers[0].conv(x, edge_index, edge_attr)\n",
    "\n",
    "        for layer in self.layers[1:]:\n",
    "            x = layer(x, edge_index, edge_attr)\n",
    "\n",
    "        x = self.layers[0].act(self.layers[0].norm(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #----\n",
    "        #hard code\n",
    "        x = x.reshape(batch_size, -1, node_hidden_channels)\n",
    "        x = x.permute(1,0,2).contiguous() #length, batch_size, 128\n",
    "        \n",
    "        if 1:\n",
    "            pos = self.postition(x)\n",
    "            x = self.transformer(x+pos)\n",
    "\n",
    "        #----\n",
    "        predict = self.lin1(x)\n",
    "        predict = predict.permute(1,0,2).contiguous()\n",
    "        predict = self.convEncoder0(predict.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        predict = self.convEncoder1(predict.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        predict = self.convEncoder2(predict.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        predict = self.convEncoder3(predict.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        #print(predict.shape)\n",
    "        #predict, _ = self.rnn(predict)\n",
    "        #print(predict.shape)\n",
    "        predict = self.lin_final(predict)\n",
    "        predict = predict.reshape(-1, 3)  #to keep compatible with rest of code\n",
    "        return predict\n",
    "\n",
    "def weighted_mse_loss(prds, tgts, weight):\n",
    "    return torch.mean(weight * (prds - tgts)**2)\n",
    "\n",
    "def criterion(prds, tgts, weight=None):\n",
    "    if weight is None:\n",
    "        return (torch.sqrt(torch.nn.MSELoss()(prds[:,0], tgts[:,0])) +\n",
    "                torch.sqrt(torch.nn.MSELoss()(prds[:,1], tgts[:,1])) +\n",
    "                torch.sqrt(torch.nn.MSELoss()(prds[:,2], tgts[:,2]))) / 3\n",
    "    else:\n",
    "        return (torch.sqrt(weighted_mse_loss(prds[:,0], tgts[:,0], weight)) +\n",
    "                torch.sqrt(weighted_mse_loss(prds[:,1], tgts[:,1], weight)) +\n",
    "                torch.sqrt(weighted_mse_loss(prds[:,2], tgts[:,2], weight))) / 3\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "print('Reading', train_file)\n",
    "df_tr = pd.read_json(train_file, lines=True)\n",
    "add_y_cat(df_tr)\n",
    "\n",
    "is_clean = df_tr.apply(sample_is_clean, axis=1)\n",
    "df_clean = df_tr[is_clean].reset_index(drop=True)\n",
    "df_noisy = df_tr[is_clean==False].reset_index(drop=True)\n",
    "del df_tr\n",
    "\n",
    "#------------------------------------------------------------\n",
    "print('Training')\n",
    "all_ys = torch.zeros((0, 3)).to(device).detach()\n",
    "all_outs = torch.zeros((0, 3)).to(device).detach()\n",
    "best_model_states = []\n",
    "kf = StratifiedKFold(nb_fold, shuffle=True, random_state=seed)\n",
    "for fold, ((clean_train_idx, clean_valid_idx),\n",
    "           (noisy_train_idx, noisy_valid_idx)) \\\n",
    "               in enumerate(zip(kf.split(df_clean, df_clean['y_cat']),\n",
    "                                kf.split(df_noisy, df_noisy['y_cat']))):\n",
    "    print('Fold', fold)\n",
    "    #epochs = 5\n",
    "    #start_timer = timer()\n",
    "\n",
    "    out_dir = f'./{NAME}/sn1-fold-{fold}'#%fold\n",
    "    initial_checkpoint =         None #out_dir + '/checkpoint/00007200_model.pth' #\n",
    "\n",
    "\n",
    "    ## setup  ----------------------------------------\n",
    "    for f in ['checkpoint',] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "#     log = Logger()\n",
    "#     log.open(out_dir+'/log.train.txt',mode='a')\n",
    "#     log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n",
    "#     log.write('\\t%s\\n' % COMMON_STRING)\n",
    "#     log.write('\\t__file__ = %s\\n' % __file__)\n",
    "#     log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "#     log.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------\n",
    "    # build train data\n",
    "    df_train = df_clean.loc[clean_train_idx]\n",
    "    if train_with_noisy_data:\n",
    "        df_train_noisy = df_noisy.loc[noisy_train_idx]\n",
    "        df_train_noisy =            df_train_noisy[df_train_noisy.apply(calc_error_mean, axis=1) <=                           error_mean_limit]\n",
    "        df_train = df_train.append(df_train_noisy)\n",
    "    data_train = build_data(df_train.reset_index(drop=True), True)\n",
    "    del df_train\n",
    "    loader_train = DataLoader(data_train, batch_size=batch_size, num_workers=2,\n",
    "                              shuffle=True)\n",
    "\n",
    "    # build validation data\n",
    "    df_valid_clean = df_clean.loc[clean_valid_idx].reset_index(drop=True)\n",
    "    data_valid_clean = build_data(df_valid_clean, True)\n",
    "    del df_valid_clean\n",
    "    loader_valid_clean = DataLoader(data_valid_clean, batch_size=batch_size, num_workers=2,\n",
    "                                    shuffle=False)\n",
    "\n",
    "    model = MyDeeperGCN(data_train[0].num_node_features,\n",
    "                        data_train[0].num_edge_features,\n",
    "                        node_hidden_channels=node_hidden_channels,\n",
    "                        edge_hidden_channels=edge_hidden_channels,\n",
    "                        num_layers=num_layers,\n",
    "                        num_classes=3, seq_length=143).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "    best_mcrmse = np.inf\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        #print('Epoch', epoch)\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        nb = 0\n",
    "        for data in (loader_train):\n",
    "            data = data.to(device)\n",
    "            mask = data.train_mask\n",
    "            weight = data.weight[mask]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)[mask]\n",
    "            y = data.y[mask]\n",
    "            loss = criterion(out, y, weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "            nb += y.size(0)\n",
    "\n",
    "            del data\n",
    "            del out\n",
    "            del y\n",
    "            del loss\n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "        train_loss /= nb\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        nb = 0\n",
    "        ys = torch.zeros((0, 3)).to(device).detach()\n",
    "        outs = torch.zeros((0, 3)).to(device).detach()\n",
    "        for data in (loader_valid_clean):\n",
    "            data = data.to(device)\n",
    "            mask = data.train_mask\n",
    "\n",
    "            out = model(data)[mask].detach()\n",
    "            y = data.y[mask].detach()\n",
    "            loss = criterion(out, y).detach()\n",
    "            valid_loss += loss.item() * y.size(0)\n",
    "            nb += y.size(0)\n",
    "\n",
    "            outs = torch.cat((outs, out), dim=0)\n",
    "            ys = torch.cat((ys, y), dim=0)\n",
    "\n",
    "            del data\n",
    "            del out\n",
    "            del y\n",
    "            del loss\n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "        valid_loss /= nb\n",
    "\n",
    "        mcrmse = criterion(outs, ys).item()\n",
    "        scheduler.step(mcrmse)\n",
    "        print('Epoch %3d |  T Loss: %0.5f  |  V Loss: %0.5f  |  V MCRMSE: %0.5f | %s \\n'%(\n",
    "            epoch, train_loss, valid_loss, mcrmse, 'min'))\n",
    "        \n",
    "        \n",
    "        if mcrmse < best_mcrmse:\n",
    "            #print('Best valid MCRMSE updated to', mcrmse)\n",
    "            best_mcrmse = mcrmse\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        if int(epoch)%5 ==0:\n",
    "            torch.save({'state_dict': model.state_dict(),}, out_dir + '/checkpoint/%08d_model.pth' % (int(epoch)))\n",
    "    print('#'*20, best_mcrmse)\n",
    "    del data_train\n",
    "    del data_valid_clean\n",
    "    #gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "    #--------------------------------------------------\n",
    "    best_model_states.append(best_model_state)\n",
    "\n",
    "    # predict for CV\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    for data in loader_valid_clean:\n",
    "        data = data.to(device)\n",
    "        mask = data.train_mask\n",
    "\n",
    "        out = model(data)[mask].detach()\n",
    "        y = data.y[mask].detach()\n",
    "\n",
    "        all_ys = torch.cat((all_ys, y), dim=0)\n",
    "        all_outs = torch.cat((all_outs, out), dim=0)\n",
    "\n",
    "        del data\n",
    "        del out\n",
    "        del y\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "# calculate MCRMSE by all training data\n",
    "print('CV MCRMSE ', criterion(all_outs, all_ys).item())\n",
    "del all_outs\n",
    "del all_ys\n",
    "#gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# predict for test data\n",
    "print('Predicting test data')\n",
    "print('Reading', test_file)\n",
    "df_te = pd.read_json(test_file, lines=True)\n",
    "\n",
    "df_te = df_te.query('seq_length==107').reset_index(drop=True)\n",
    "\n",
    "data_test = build_data(df_te, False)\n",
    "loader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "id_seqpos = build_id_seqpos(df_te)\n",
    "\n",
    "model = MyDeeperGCN(data_test[0].num_node_features,\n",
    "                    data_test[0].num_edge_features,\n",
    "                    node_hidden_channels=node_hidden_channels,\n",
    "                    edge_hidden_channels=edge_hidden_channels,\n",
    "                    num_layers=num_layers,\n",
    "                    num_classes=3, seq_length=143).to(device)\n",
    "\n",
    "preds = torch.zeros((len(id_seqpos), 3)).to(device).detach()\n",
    "for best_model_state in best_model_states:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "\n",
    "    outs = torch.zeros((0, 3)).to(device).detach()\n",
    "    for data in loader_test:\n",
    "        data = data.to(device)\n",
    "        mask = data.test_mask\n",
    "\n",
    "        out = model(data)[mask].detach()\n",
    "        outs = torch.cat((outs, out), dim=0)\n",
    "\n",
    "        del data\n",
    "        del out\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "    preds += outs\n",
    "preds /= len(best_model_states)\n",
    "preds = preds.cpu().numpy()\n",
    "\n",
    "df_sub_pub = pd.DataFrame({'id_seqpos': id_seqpos,\n",
    "                       'reactivity': preds[:,0],\n",
    "                       'deg_Mg_pH10': preds[:,1],\n",
    "                       'deg_pH10': 0,\n",
    "                       'deg_Mg_50C': preds[:,2],\n",
    "                       'deg_50C': 0})\n",
    "print('Writing submission.csv')\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# predict for test data\n",
    "print('Predicting test data')\n",
    "print('Reading', test_file)\n",
    "df_te = pd.read_json(test_file, lines=True)\n",
    "\n",
    "df_te = df_te.query('seq_length==130').reset_index(drop=True)\n",
    "\n",
    "data_test = build_data(df_te, False)\n",
    "loader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
    "id_seqpos = build_id_seqpos(df_te)\n",
    "\n",
    "model = MyDeeperGCN(data_test[0].num_node_features,\n",
    "                    data_test[0].num_edge_features,\n",
    "                    node_hidden_channels=node_hidden_channels,\n",
    "                    edge_hidden_channels=edge_hidden_channels,\n",
    "                    num_layers=num_layers,\n",
    "                    num_classes=3, seq_length=174).to(device)\n",
    "\n",
    "preds = torch.zeros((len(id_seqpos), 3)).to(device).detach()\n",
    "for best_model_state in best_model_states:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "\n",
    "    outs = torch.zeros((0, 3)).to(device).detach()\n",
    "    for data in loader_test:\n",
    "        data = data.to(device)\n",
    "        mask = data.test_mask\n",
    "\n",
    "        out = model(data)[mask].detach()\n",
    "        outs = torch.cat((outs, out), dim=0)\n",
    "\n",
    "        del data\n",
    "        del out\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "    preds += outs\n",
    "preds /= len(best_model_states)\n",
    "preds = preds.cpu().numpy()\n",
    "\n",
    "df_sub_pri = pd.DataFrame({'id_seqpos': id_seqpos,\n",
    "                       'reactivity': preds[:,0],\n",
    "                       'deg_Mg_pH10': preds[:,1],\n",
    "                       'deg_pH10': 0,\n",
    "                       'deg_Mg_50C': preds[:,2],\n",
    "                       'deg_50C': 0})\n",
    "print('Writing submission.csv')\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "df_sub = pd.concat([df_sub_pub, df_sub_pri]).reset_index(drop=True).sort_values('id_seqpos').reset_index(drop=True) \n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "best = pd.read_csv('ensemble_v37.csv').reset_index(drop=True).sort_values('id_seqpos').reset_index(drop=True) \n",
    "\n",
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
    "\n",
    "print(np.corrcoef(df_sub[pred_cols[0]], best[pred_cols[0]]))\n",
    "print(np.corrcoef(df_sub[pred_cols[1]], best[pred_cols[1]]))\n",
    "print(np.corrcoef(df_sub[pred_cols[3]], best[pred_cols[3]]))\n",
    "\n",
    "df_sub.to_csv(f'{NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyh",
   "language": "python",
   "name": "lyh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

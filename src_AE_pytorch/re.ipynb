{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from fastprogress import progress_bar\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
    "input_cols = ['sequence', 'structure', 'predicted_loop_type']\n",
    "error_cols = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_Mg_50C', 'deg_error_pH10', 'deg_error_50C']\n",
    "\n",
    "token_dicts = {\n",
    "    \"sequence\": {x: i for i, x in enumerate(\"ACGU\")},\n",
    "    \"structure\": {x: i for i, x in enumerate('().')},\n",
    "    \"predicted_loop_type\": {x: i for i, x in enumerate(\"BEHIMSX\")}\n",
    "}\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "BASE_PATH = \"../../input/\"\n",
    "MODEL_SAVE_PATH = \"./model\"\n",
    "\n",
    "\n",
    "def preprocess_inputs(df, cols):\n",
    "    return np.concatenate([preprocess_feature_col(df, col) for col in cols], axis=2)\n",
    "\n",
    "\n",
    "def preprocess_feature_col(df, col):\n",
    "    dic = token_dicts[col]\n",
    "    dic_len = len(dic)\n",
    "    seq_length = len(df[col][0])\n",
    "    ident = np.identity(dic_len)\n",
    "    # convert to one hot\n",
    "    arr = np.array(\n",
    "        df[[col]].applymap(lambda seq: [ident[dic[x]] for x in seq]).values.tolist()\n",
    "    ).squeeze(1)\n",
    "    # shape: data_size x seq_length x dic_length\n",
    "    assert arr.shape == (len(df), seq_length, dic_len)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def preprocess(base_data, is_test=False):\n",
    "    inputs = preprocess_inputs(base_data, input_cols)\n",
    "    if is_test:\n",
    "        labels = None\n",
    "    else:\n",
    "        labels = np.array(base_data[target_cols].values.tolist()).transpose((0, 2, 1))\n",
    "        assert labels.shape[2] == len(target_cols)\n",
    "    assert inputs.shape[2] == 14\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def get_bpp_feature(bpp):\n",
    "    bpp_nb_mean = 0.077522  # mean of bpps_nb across all training data\n",
    "    bpp_nb_std = 0.08914  # std of bpps_nb across all training data\n",
    "    bpp_max = bpp.max(-1)[0]\n",
    "    bpp_sum = bpp.sum(-1)\n",
    "    bpp_nb = torch.true_divide((bpp > 0).sum(dim=1), bpp.shape[1])\n",
    "    bpp_nb = torch.true_divide(bpp_nb - bpp_nb_mean, bpp_nb_std)\n",
    "    return [bpp_max.unsqueeze(2), bpp_sum.unsqueeze(2), bpp_nb.unsqueeze(2)]\n",
    "\n",
    "\n",
    "@functools.lru_cache(5000)\n",
    "def load_from_id(id_):\n",
    "    path = Path(BASE_PATH) / f\"bpps/{id_}.npy\"\n",
    "    data = np.load(str(path))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_distance_matrix(leng):\n",
    "    idx = np.arange(leng)\n",
    "    Ds = []\n",
    "    for i in range(len(idx)):\n",
    "        d = np.abs(idx[i] - idx)\n",
    "        Ds.append(d)\n",
    "\n",
    "    Ds = np.array(Ds) + 1\n",
    "    Ds = 1 / Ds\n",
    "    Ds = Ds[None, :, :]\n",
    "    Ds = np.repeat(Ds, 1, axis=0)\n",
    "\n",
    "    Dss = []\n",
    "    for i in [1, 2, 4]:\n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis=3)\n",
    "    print(Ds.shape)\n",
    "    return Ds\n",
    "\n",
    "\n",
    "def get_structure_adj(df):\n",
    "    Ss = []\n",
    "    for i in range(len(df)):\n",
    "        seq_length = df[\"seq_length\"].iloc[i]\n",
    "        structure = df[\"structure\"].iloc[i]\n",
    "        sequence = df[\"sequence\"].iloc[i]\n",
    "\n",
    "        cue = []\n",
    "        a_structures = OrderedDict([\n",
    "            ((\"A\", \"U\"), np.zeros([seq_length, seq_length])),\n",
    "            ((\"C\", \"G\"), np.zeros([seq_length, seq_length])),\n",
    "            ((\"U\", \"G\"), np.zeros([seq_length, seq_length])),\n",
    "            ((\"U\", \"A\"), np.zeros([seq_length, seq_length])),\n",
    "            ((\"G\", \"C\"), np.zeros([seq_length, seq_length])),\n",
    "            ((\"G\", \"U\"), np.zeros([seq_length, seq_length])),\n",
    "        ])\n",
    "        for j in range(seq_length):\n",
    "            if structure[j] == \"(\":\n",
    "                cue.append(j)\n",
    "            elif structure[j] == \")\":\n",
    "                start = cue.pop()\n",
    "                a_structures[(sequence[start], sequence[j])][start, j] = 1\n",
    "                a_structures[(sequence[j], sequence[start])][j, start] = 1\n",
    "\n",
    "        a_strc = np.stack([a for a in a_structures.values()], axis=2)\n",
    "        a_strc = np.sum(a_strc, axis=2, keepdims=True)\n",
    "        Ss.append(a_strc)\n",
    "\n",
    "    Ss = np.array(Ss)\n",
    "    return Ss\n",
    "\n",
    "\n",
    "def create_loader(df, batch_size=1, is_test=False):\n",
    "    features, labels = preprocess(df, is_test)\n",
    "    features_tensor = torch.from_numpy(features)\n",
    "    if labels is not None:\n",
    "        labels_tensor = torch.from_numpy(labels)\n",
    "        dataset = VacDataset(features_tensor, df, labels_tensor)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, drop_last=False)\n",
    "    else:\n",
    "        dataset = VacDataset(features_tensor, df, None)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=False, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "\n",
    "class VacDataset(Dataset):\n",
    "    def __init__(self, features, df, labels=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.test = labels is None\n",
    "        self.ids = df[\"id\"]\n",
    "        self.score = None\n",
    "        self.structure_adj = get_structure_adj(df)\n",
    "        self.distance_matrix = get_distance_matrix(self.structure_adj.shape[1])\n",
    "        if \"score\" in df.columns:\n",
    "            self.score = df[\"score\"]\n",
    "        else:\n",
    "            df[\"score\"] = 1.0\n",
    "            self.score = df[\"score\"]\n",
    "        self.signal_to_noise = None\n",
    "        if not self.test:\n",
    "            self.signal_to_noise = df[\"signal_to_noise\"]\n",
    "            assert self.features.shape[0] == self.labels.shape[0]\n",
    "        else:\n",
    "            assert self.ids is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bpp = torch.from_numpy(load_from_id(self.ids[index]).copy()).float()\n",
    "        adj = self.structure_adj[index]\n",
    "        distance = self.distance_matrix[0]\n",
    "        bpp = np.concatenate([bpp[:, :, None], adj, distance], axis=2)\n",
    "        if self.test:\n",
    "            return dict(sequence=self.features[index].float(), bpp=bpp, ids=self.ids[index])\n",
    "        else:\n",
    "            return dict(sequence=self.features[index].float(), bpp=bpp,\n",
    "                        label=self.labels[index], ids=self.ids[index],\n",
    "                        signal_to_noise=self.signal_to_noise[index],\n",
    "                        score=self.score[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "class Conv1dStack(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n",
    "        super(Conv1dStack, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.res = nn.Sequential(\n",
    "            nn.Conv1d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        h = self.res(x)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class Conv2dStack(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n",
    "        super(Conv2dStack, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "        self.res = nn.Sequential(\n",
    "            nn.Conv2d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.se = SELayer2D(out_dim, 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.se(x)\n",
    "        x = self.relu(x)\n",
    "        h = self.res(x)\n",
    "        return x + h\n",
    "\n",
    "class SELayer1D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer1D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "class SELayer2D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer2D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "    \n",
    "class SeqEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super(SeqEncoder, self).__init__()\n",
    "        self.conv0 = Conv1dStack(in_dim, 128, 3, padding=1)\n",
    "        self.conv1 = Conv1dStack(128, 64, 6, padding=5, dilation=2)\n",
    "        self.conv2 = Conv1dStack(64, 32, 15, padding=7, dilation=1)\n",
    "        self.conv3 = Conv1dStack(32, 32, 30, padding=29, dilation=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.conv1(x1)\n",
    "        x3 = self.conv2(x2)\n",
    "        x4 = self.conv3(x3)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        # x = x.permute(0, 2, 1).contiguous()\n",
    "        # BATCH x 256 x seq_length\n",
    "        return x\n",
    "\n",
    "\n",
    "class BppAttn(nn.Module):\n",
    "    def __init__(self, in_channel: int, out_channel: int):\n",
    "        super(BppAttn, self).__init__()\n",
    "        self.conv0 = Conv1dStack(in_channel, out_channel, 3, padding=1)\n",
    "        self.bpp_conv = Conv2dStack(5, out_channel)\n",
    "\n",
    "    def forward(self, x, bpp):\n",
    "        x = self.conv0(x)\n",
    "        bpp = self.bpp_conv(bpp)\n",
    "        # BATCH x C x SEQ x SEQ\n",
    "        # BATCH x C x SEQ\n",
    "        x = torch.matmul(bpp, x.unsqueeze(-1))\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerWrapper(nn.Module):\n",
    "    def __init__(self, dmodel=256, nhead=8, num_layers=2):\n",
    "        super(TransformerWrapper, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(256)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=dmodel, nhead=nhead)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.pos_emb = PositionalEncoding(dmodel)\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute((1, 0, 2)).contiguous()\n",
    "        x = self.pos_emb(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute((1, 0, 2)).contiguous()\n",
    "        return x, None\n",
    "\n",
    "\n",
    "class RnnLayers(nn.Module):\n",
    "    def __init__(self, dmodel, dropout=0.3, transformer_layers: int = 2):\n",
    "        super(RnnLayers, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn0 = TransformerWrapper(dmodel, nhead=8, num_layers=transformer_layers)\n",
    "        self.rnn1 = nn.LSTM(dmodel, dmodel // 2, batch_first=True, num_layers=1, bidirectional=True)\n",
    "        self.rnn2 = nn.GRU(dmodel, dmodel // 2, batch_first=True, num_layers=1, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.rnn0.flatten_parameters()\n",
    "        x, _ = self.rnn0(x)\n",
    "        if self.rnn1 is not None:\n",
    "            self.rnn1.flatten_parameters()\n",
    "            x = self.dropout(x)\n",
    "            x, _ = self.rnn1(x)\n",
    "        if self.rnn2 is not None:\n",
    "            self.rnn2.flatten_parameters()\n",
    "            x = self.dropout(x)\n",
    "            x, _ = self.rnn2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class BaseAttnModel(nn.Module):\n",
    "    def __init__(self, transformer_layers: int = 2):\n",
    "        super(BaseAttnModel, self).__init__()\n",
    "        self.linear0 = nn.Linear(14 + 3, 1)\n",
    "        self.seq_encoder_x = SeqEncoder(18)\n",
    "        self.attn = BppAttn(256, 128)\n",
    "        self.seq_encoder_bpp = SeqEncoder(128)\n",
    "        self.seq = RnnLayers(256 * 2, dropout=0.3,\n",
    "                             transformer_layers=transformer_layers)\n",
    "\n",
    "    def forward(self, x, bpp):\n",
    "        bpp_features = get_bpp_feature(bpp[:, :, :, 0].float())\n",
    "        x = torch.cat([x] + bpp_features, dim=-1)\n",
    "        learned = self.linear0(x)\n",
    "        x = torch.cat([x, learned], dim=-1)\n",
    "        x = x.permute(0, 2, 1).contiguous().float()\n",
    "        # BATCH x 18 x seq_len\n",
    "        bpp = bpp.permute([0, 3, 1, 2]).contiguous().float()\n",
    "        # BATCH x 5 x seq_len x seq_len\n",
    "        x = self.seq_encoder_x(x)\n",
    "        # BATCH x 256 x seq_len\n",
    "        bpp = self.attn(x, bpp)\n",
    "        bpp = self.seq_encoder_bpp(bpp)\n",
    "        # BATCH x 256 x seq_len\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        # BATCH x seq_len x 256\n",
    "        bpp = bpp.permute(0, 2, 1).contiguous()\n",
    "        # BATCH x seq_len x 256\n",
    "        x = torch.cat([x, bpp], dim=2)\n",
    "        # BATCH x seq_len x 512\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AEModel(nn.Module):\n",
    "    def __init__(self, transformer_layers: int = 2):\n",
    "        super(AEModel, self).__init__()\n",
    "        self.seq = BaseAttnModel(transformer_layers=transformer_layers)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(256 * 2, 14),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, bpp):\n",
    "        x = self.seq(x, bpp)\n",
    "        x = F.dropout(x, p=0.3)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FromAeModel(nn.Module):\n",
    "    def __init__(self, seq, pred_len=68, dmodel: int = 256):\n",
    "        super(FromAeModel, self).__init__()\n",
    "        self.seq = seq\n",
    "        self.pred_len = pred_len\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dmodel * 2, len(target_cols)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, bpp):\n",
    "        x = self.seq(x, bpp)\n",
    "        x = self.linear(x)\n",
    "        x = x[:, :self.pred_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public_df: (629, 7)\n",
      "private_df: (3005, 7)\n",
      "(1, 107, 107, 3)\n",
      "(1, 107, 107, 3)\n",
      "(1, 130, 130, 3)\n"
     ]
    }
   ],
   "source": [
    "base_train_data = pd.read_json(str(Path(BASE_PATH) / 'train.json'), lines=True)\n",
    "base_train_data.head()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "BATCH_SIZE = 32\n",
    "base_train_data = pd.read_json(str(Path(BASE_PATH) / 'train.json'), lines=True)\n",
    "base_test_data = pd.read_json(str(Path(BASE_PATH) / 'test.json'), lines=True)\n",
    "public_df = base_test_data.query(\"seq_length == 107\").copy()\n",
    "private_df = base_test_data.query(\"seq_length == 130\").copy()\n",
    "print(f\"public_df: {public_df.shape}\")\n",
    "print(f\"private_df: {private_df.shape}\")\n",
    "public_df = public_df.reset_index()\n",
    "private_df = private_df.reset_index()\n",
    "\n",
    "features, _ = preprocess(base_train_data, True)\n",
    "features_tensor = torch.from_numpy(features)\n",
    "dataset0 = VacDataset(features_tensor, base_train_data, None)\n",
    "features, _ = preprocess(public_df, True)\n",
    "features_tensor = torch.from_numpy(features)\n",
    "dataset1 = VacDataset(features_tensor, public_df, None)\n",
    "features, _ = preprocess(private_df, True)\n",
    "features_tensor = torch.from_numpy(features)\n",
    "dataset2 = VacDataset(features_tensor, private_df, None)\n",
    "\n",
    "loader0 = torch.utils.data.DataLoader(dataset0, BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "loader1 = torch.utils.data.DataLoader(dataset1, BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "loader2 = torch.utils.data.DataLoader(dataset2, BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_batch_ae(model, data, device):\n",
    "    seq = data[\"sequence\"].clone()\n",
    "    seq[:, :, :14] = F.dropout2d(seq[:, :, :14], p=0.3)\n",
    "    target = data[\"sequence\"][:, :, :14]\n",
    "    out = model(seq.to(device), data[\"bpp\"].to(device))\n",
    "    loss = F.binary_cross_entropy(out, target.to(device))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_ae(model, train_data, optimizer, lr_scheduler, epochs=10, device=\"cpu\",\n",
    "             start_epoch: int = 0, start_it: int = 0, log_path: str = \"./logs\"):\n",
    "    print(f\"device: {device}\")\n",
    "    losses = []\n",
    "    it = start_it\n",
    "    model_save_path = Path(MODEL_SAVE_PATH)\n",
    "    start_epoch = start_epoch\n",
    "    end_epoch = start_epoch + epochs\n",
    "    min_loss = 10.0\n",
    "    min_loss_epoch = 0\n",
    "    if not model_save_path.exists():\n",
    "        model_save_path.mkdir(parents=True)\n",
    "    for epoch in progress_bar(range(start_epoch, end_epoch)):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            loss = learn_from_batch_ae(model, data, device)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step()\n",
    "            loss_v = loss.item()\n",
    "            losses.append(loss_v)\n",
    "            it += 1\n",
    "        loss_m = np.mean(losses)\n",
    "        if loss_m < min_loss:\n",
    "            min_loss_epoch = epoch\n",
    "            min_loss = loss_m\n",
    "        print(f'epoch: {epoch} loss: {loss_m}')\n",
    "        losses = []\n",
    "        torch.save(optimizer.state_dict(), str(model_save_path / \"optimizer.pt\"))\n",
    "        torch.save(model.state_dict(), str(model_save_path / f\"model-{epoch}.pt\"))\n",
    "    return dict(end_epoch=end_epoch, it=it, min_loss_epoch=min_loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:55<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 0 loss: 0.25412579397360485\n",
      "epoch: 1\n",
      "epoch: 1 loss: 0.13216191202402114\n",
      "epoch: 2\n",
      "epoch: 2 loss: 0.06935024549563726\n",
      "epoch: 3\n",
      "epoch: 3 loss: 0.028938308432698248\n",
      "epoch: 4\n",
      "epoch: 4 loss: 0.020188981965184212\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:15<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "epoch: 5 loss: 0.015516863716766239\n",
      "epoch: 6\n",
      "epoch: 6 loss: 0.16228479091078044\n",
      "epoch: 7\n",
      "epoch: 7 loss: 0.06675708461552858\n",
      "epoch: 8\n",
      "epoch: 8 loss: 0.026076144073158503\n",
      "epoch: 9\n",
      "epoch: 9 loss: 0.017415161617100237\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 01:28<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "epoch: 10 loss: 0.020721865639566107\n",
      "epoch: 11\n",
      "epoch: 11 loss: 0.016409699260158107\n",
      "epoch: 12\n",
      "epoch: 12 loss: 0.01466008892996197\n",
      "epoch: 13\n",
      "epoch: 13 loss: 0.01360777104669746\n",
      "epoch: 14\n",
      "epoch: 14 loss: 0.0130978908508699\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:58<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15\n",
      "epoch: 15 loss: 0.024169061488161486\n",
      "epoch: 16\n",
      "epoch: 16 loss: 0.02279738210141659\n",
      "epoch: 17\n",
      "epoch: 17 loss: 0.010225677366058032\n",
      "epoch: 18\n",
      "epoch: 18 loss: 0.008707359122733275\n",
      "epoch: 19\n",
      "epoch: 19 loss: 0.008218180400629839\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:15<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20\n",
      "epoch: 20 loss: 0.006329368962906301\n",
      "epoch: 21\n",
      "epoch: 21 loss: 0.0059927744325250385\n",
      "epoch: 22\n",
      "epoch: 22 loss: 0.005773757956922055\n",
      "epoch: 23\n",
      "epoch: 23 loss: 0.005522173235658556\n",
      "epoch: 24\n",
      "epoch: 24 loss: 0.005406275019049644\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 01:27<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25\n",
      "epoch: 25 loss: 0.012985392404284249\n",
      "epoch: 26\n",
      "epoch: 26 loss: 0.012053749434887729\n",
      "epoch: 27\n",
      "epoch: 27 loss: 0.011598565912944205\n",
      "epoch: 28\n",
      "epoch: 28 loss: 0.011280736123072975\n",
      "epoch: 29\n",
      "epoch: 29 loss: 0.011070258746993668\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:56<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30\n",
      "epoch: 30 loss: 0.006801698009173076\n",
      "epoch: 31\n",
      "epoch: 31 loss: 0.006209328363959988\n",
      "epoch: 32\n",
      "epoch: 32 loss: 0.006049357218046983\n",
      "epoch: 33\n",
      "epoch: 33 loss: 0.0059371042251586915\n",
      "epoch: 34\n",
      "epoch: 34 loss: 0.005784233702967564\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:14<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35\n",
      "epoch: 35 loss: 0.004913143662270159\n",
      "epoch: 36\n",
      "epoch: 36 loss: 0.004844974970910698\n",
      "epoch: 37\n",
      "epoch: 37 loss: 0.004375648417044431\n",
      "epoch: 38\n",
      "epoch: 38 loss: 0.00417260117828846\n",
      "epoch: 39\n",
      "epoch: 39 loss: 0.0040570184239186345\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 01:28<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40\n",
      "epoch: 40 loss: 0.010656323531602925\n",
      "epoch: 41\n",
      "epoch: 41 loss: 0.009965676913394573\n",
      "epoch: 42\n",
      "epoch: 42 loss: 0.00953505505768067\n",
      "epoch: 43\n",
      "epoch: 43 loss: 0.00984791328417177\n",
      "epoch: 44\n",
      "epoch: 44 loss: 0.009858391153566699\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:54<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45\n",
      "epoch: 45 loss: 0.0058253320182363195\n",
      "epoch: 46\n",
      "epoch: 46 loss: 0.005865139256541928\n",
      "epoch: 47\n",
      "epoch: 47 loss: 0.005555450444420179\n",
      "epoch: 48\n",
      "epoch: 48 loss: 0.0054214531493683655\n",
      "epoch: 49\n",
      "epoch: 49 loss: 0.005005289750794569\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:15<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50\n",
      "epoch: 50 loss: 0.004363679292146117\n",
      "epoch: 51\n",
      "epoch: 51 loss: 0.5766543057747185\n",
      "epoch: 52\n",
      "epoch: 52 loss: 0.4566668406128883\n",
      "epoch: 53\n",
      "epoch: 53 loss: 0.40210413485765456\n",
      "epoch: 54\n",
      "epoch: 54 loss: 0.3841605201363564\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 01:26<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55\n",
      "epoch: 55 loss: 0.38130888279448166\n",
      "epoch: 56\n",
      "epoch: 56 loss: 0.36800752865507247\n",
      "epoch: 57\n",
      "epoch: 57 loss: 0.353798822836673\n",
      "epoch: 58\n",
      "epoch: 58 loss: 0.3440731884317195\n",
      "epoch: 59\n",
      "epoch: 59 loss: 0.3405891951728374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ae-model.pt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "set_seed(123)\n",
    "shutil.rmtree(\"./model\", True)\n",
    "shutil.rmtree(\"./logs\", True)\n",
    "save_path = Path(\"./model_prediction\")\n",
    "if not save_path.exists():\n",
    "    save_path.mkdir(parents=True)\n",
    "\n",
    "lr_scheduler = None\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AEModel()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "res = dict(end_epoch=0, it=0, min_loss_epoch=0)\n",
    "epochs = [5, 5, 5, 5]\n",
    "for e in epochs:\n",
    "    res = train_ae(model, loader0, optimizer, lr_scheduler, e, device=device,\n",
    "                   start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n",
    "    res = train_ae(model, loader1, optimizer, lr_scheduler, e, device=device,\n",
    "                   start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n",
    "    res = train_ae(model, loader2, optimizer, lr_scheduler, e, device=device,\n",
    "                   start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n",
    "\n",
    "epoch = res[\"min_loss_epoch\"]\n",
    "shutil.copyfile(str(Path(MODEL_SAVE_PATH) / f\"model-{epoch}.pt\"), \"ae-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCRMSE(y_true, y_pred):\n",
    "    colwise_mse = torch.mean(torch.square(y_true - y_pred), dim=1)\n",
    "    return torch.mean(torch.sqrt(colwise_mse), dim=1)\n",
    "\n",
    "\n",
    "def sn_mcrmse_loss(predict, target, signal_to_noise):\n",
    "    loss = MCRMSE(target, predict)\n",
    "    weight = 0.5 * torch.log(signal_to_noise + 1.01)\n",
    "    loss = (loss * weight).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def learn_from_batch(model, data, optimizer, lr_scheduler, device):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data[\"sequence\"].to(device), data[\"bpp\"].to(device))\n",
    "    signal_to_noise = data[\"signal_to_noise\"] * data[\"score\"]\n",
    "    loss = sn_mcrmse_loss(out, data[\"label\"].to(device), signal_to_noise.to(device))\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    if lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "    return out, loss\n",
    "\n",
    "\n",
    "def evaluate(model, valid_data, device):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    mcrmse = []\n",
    "    for i, data in enumerate(valid_data):\n",
    "        with torch.no_grad():\n",
    "            y = model(data[\"sequence\"].to(device), data[\"bpp\"].to(device))\n",
    "            mcrmse_ = MCRMSE(data[\"label\"].to(device), y)[data[\"signal_to_noise\"] > 1]\n",
    "            mcrmse.append(mcrmse_.mean().item())\n",
    "            loss = sn_mcrmse_loss(y, data[\"label\"].to(device), data[\"signal_to_noise\"].to(device))\n",
    "            loss_list.append(loss.item())\n",
    "    model.train()\n",
    "    return dict(loss=np.mean(loss_list), mcmse=np.mean(mcrmse))\n",
    "\n",
    "\n",
    "def train(model, train_data, valid_data, optimizer, lr_scheduler, epochs=10, device=\"cpu\",\n",
    "          start_epoch: int = 0, log_path: str = \"./logs\"):\n",
    "    print(f\"device: {device}\")\n",
    "    losses = []\n",
    "    writer = SummaryWriter(log_path)\n",
    "    it = 0\n",
    "    model_save_path = Path(MODEL_SAVE_PATH)\n",
    "    start_epoch = start_epoch\n",
    "    end_epoch = start_epoch + epochs\n",
    "    if not model_save_path.exists():\n",
    "        model_save_path.mkdir(parents=True)\n",
    "    min_eval_loss = 10.0\n",
    "    min_eval_epoch = None\n",
    "    for epoch in progress_bar(range(start_epoch, end_epoch)):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_data):\n",
    "            _, loss = learn_from_batch(model, data, optimizer, lr_scheduler, device)\n",
    "            loss_v = loss.item()\n",
    "            writer.add_scalar('loss', loss_v, it)\n",
    "            losses.append(loss_v)\n",
    "            it += 1\n",
    "        print(f'epoch: {epoch} loss: {np.mean(losses)}')\n",
    "        losses = []\n",
    "\n",
    "        eval_result = evaluate(model, valid_data, device)\n",
    "        eval_loss = eval_result[\"loss\"]\n",
    "        if eval_loss <= min_eval_loss:\n",
    "            min_eval_epoch = epoch\n",
    "            min_eval_loss = eval_loss\n",
    "\n",
    "        print(f\"eval loss: {eval_loss} {eval_result['mcmse']}\")\n",
    "        writer.add_scalar(f\"evaluate/loss\", eval_loss, epoch)\n",
    "        writer.add_scalar(f\"evaluate/mcmse\", eval_result[\"mcmse\"], epoch)\n",
    "        model.train()\n",
    "        torch.save(optimizer.state_dict(), str(model_save_path / \"optimizer.pt\"))\n",
    "        torch.save(model.state_dict(), str(model_save_path / f\"model-{epoch}.pt\"))\n",
    "    print(f'min eval loss: {min_eval_loss} epoch {min_eval_epoch}')\n",
    "    return min_eval_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "(1, 107, 107, 3)\n",
      "(1, 107, 107, 3)\n",
      "(2160, 21) (240, 21)\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='200' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [200/200 31:42<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 0 loss: 0.35893201413014175\n",
      "eval loss: 0.32571133703687594 0.42200799856151605\n",
      "epoch: 1\n",
      "epoch: 1 loss: 0.32377848282525445\n",
      "eval loss: 0.3159935092790472 0.4099649967466611\n",
      "epoch: 2\n",
      "epoch: 2 loss: 0.320223978817654\n",
      "eval loss: 0.3152107171678853 0.4110719909266878\n",
      "epoch: 3\n",
      "epoch: 3 loss: 0.3212728114396724\n",
      "eval loss: 0.3154512321095718 0.41004085730569784\n",
      "epoch: 4\n",
      "epoch: 4 loss: 0.32117550254504007\n",
      "eval loss: 0.313507628402655 0.4091822635711965\n",
      "epoch: 5\n",
      "epoch: 5 loss: 0.3204418301763755\n",
      "eval loss: 0.3168630116036648 0.4107762732618996\n",
      "epoch: 6\n",
      "epoch: 6 loss: 0.3207079624261503\n",
      "eval loss: 0.31523044712609694 0.41029571536629333\n",
      "epoch: 7\n",
      "epoch: 7 loss: 0.32086160736872926\n",
      "eval loss: 0.31556930137734307 0.4107864735242974\n",
      "epoch: 8\n",
      "epoch: 8 loss: 0.320299039225034\n",
      "eval loss: 0.3150132145471407 0.41051437274194746\n",
      "epoch: 9\n",
      "epoch: 9 loss: 0.32002861818797634\n",
      "eval loss: 0.31657246028396957 0.4096875953026655\n",
      "epoch: 10\n",
      "epoch: 10 loss: 0.3199227827465274\n",
      "eval loss: 0.3165500721549604 0.41062765097300835\n",
      "epoch: 11\n",
      "epoch: 11 loss: 0.32160456332890464\n",
      "eval loss: 0.31570956225995556 0.4111558781000969\n",
      "epoch: 12\n",
      "epoch: 12 loss: 0.3201708544724227\n",
      "eval loss: 0.315937557589106 0.4099440047129006\n",
      "epoch: 13\n",
      "epoch: 13 loss: 0.31975649335643497\n",
      "eval loss: 0.31398659075197133 0.409919961493936\n",
      "epoch: 14\n",
      "epoch: 14 loss: 0.31974457799197953\n",
      "eval loss: 0.31662273626111254 0.41165060828883876\n",
      "epoch: 15\n",
      "epoch: 15 loss: 0.3201570833557301\n",
      "eval loss: 0.31559286945889786 0.41049862835566364\n",
      "epoch: 16\n",
      "epoch: 16 loss: 0.3190354664141169\n",
      "eval loss: 0.3170768986298354 0.41331489608289007\n",
      "epoch: 17\n",
      "epoch: 17 loss: 0.3199703716212547\n",
      "eval loss: 0.31539285321092825 0.41010890240508285\n",
      "epoch: 18\n",
      "epoch: 18 loss: 0.3208107075920462\n",
      "eval loss: 0.3180313107866869 0.4099918630959216\n",
      "epoch: 19\n",
      "epoch: 19 loss: 0.3197942775731425\n",
      "eval loss: 0.31606693919834544 0.40979953072933056\n",
      "epoch: 20\n",
      "epoch: 20 loss: 0.31918220693236937\n",
      "eval loss: 0.31240977579567736 0.4056698332620681\n",
      "epoch: 21\n",
      "epoch: 21 loss: 0.3178730032706273\n",
      "eval loss: 0.3148091673761679 0.41278434667804575\n",
      "epoch: 22\n",
      "epoch: 22 loss: 0.3170685030750433\n",
      "eval loss: 0.3156991159001483 0.4099552847881241\n",
      "epoch: 23\n",
      "epoch: 23 loss: 0.3153436371214709\n",
      "eval loss: 0.30733865832989743 0.40206539332097024\n",
      "epoch: 24\n",
      "epoch: 24 loss: 0.3160813610586376\n",
      "eval loss: 0.31799710103241047 0.41349969932204644\n",
      "epoch: 25\n",
      "epoch: 25 loss: 0.31618629995870384\n",
      "eval loss: 0.31773785884160755 0.41225585907864404\n",
      "epoch: 26\n",
      "epoch: 26 loss: 0.3148370695857703\n",
      "eval loss: 0.31655763125590597 0.41213049912871547\n",
      "epoch: 27\n",
      "epoch: 27 loss: 0.3142374499731271\n",
      "eval loss: 0.31230460004456795 0.4057562271018842\n",
      "epoch: 28\n",
      "epoch: 28 loss: 0.31494156329085005\n",
      "eval loss: 0.31323468084978867 0.40682213765849873\n",
      "epoch: 29\n",
      "epoch: 29 loss: 0.3142433122177385\n",
      "eval loss: 0.3090153571105979 0.4015439048023925\n",
      "epoch: 30\n",
      "epoch: 30 loss: 0.31483838091302235\n",
      "eval loss: 0.3110242542223063 0.4034771784015477\n",
      "epoch: 31\n",
      "epoch: 31 loss: 0.3138861724379389\n",
      "eval loss: 0.3123041428313347 0.40658970167884073\n",
      "epoch: 32\n",
      "epoch: 32 loss: 0.31343410903544094\n",
      "eval loss: 0.3111471733033382 0.4038586887977237\n",
      "epoch: 33\n",
      "epoch: 33 loss: 0.3140213427369419\n",
      "eval loss: 0.3083751428955078 0.40098360374559394\n",
      "epoch: 34\n",
      "epoch: 34 loss: 0.3131538209749672\n",
      "eval loss: 0.3087353213305245 0.4019919041198371\n",
      "epoch: 35\n",
      "epoch: 35 loss: 0.3131432261233032\n",
      "eval loss: 0.3067688468357467 0.40068625214086967\n",
      "epoch: 36\n",
      "epoch: 36 loss: 0.312826447506855\n",
      "eval loss: 0.30785887302347975 0.4013408152610735\n",
      "epoch: 37\n",
      "epoch: 37 loss: 0.3129843446756925\n",
      "eval loss: 0.30843827362649495 0.39993080937189146\n",
      "epoch: 38\n",
      "epoch: 38 loss: 0.31458324447792213\n",
      "eval loss: 0.3115199108962235 0.40416959691061116\n",
      "epoch: 39\n",
      "epoch: 39 loss: 0.31267801602454304\n",
      "eval loss: 0.3095863143563339 0.4034792222077953\n",
      "epoch: 40\n",
      "epoch: 40 loss: 0.3129264176046505\n",
      "eval loss: 0.3087932893067522 0.4018224680570843\n",
      "epoch: 41\n",
      "epoch: 41 loss: 0.3129182040389453\n",
      "eval loss: 0.3095999131807314 0.40131327076962925\n",
      "epoch: 42\n",
      "epoch: 42 loss: 0.3132543496556465\n",
      "eval loss: 0.30820269869237515 0.40173902958139174\n",
      "epoch: 43\n",
      "epoch: 43 loss: 0.3126994879674063\n",
      "eval loss: 0.30633817202489366 0.40003024084436495\n",
      "epoch: 44\n",
      "epoch: 44 loss: 0.3122981282828896\n",
      "eval loss: 0.31048649226691905 0.4030111732026455\n",
      "epoch: 45\n",
      "epoch: 45 loss: 0.3125213070496433\n",
      "eval loss: 0.3096260221609717 0.40093853310756566\n",
      "epoch: 46\n",
      "epoch: 46 loss: 0.3126870335918032\n",
      "eval loss: 0.3090202506619835 0.4022005883033913\n",
      "epoch: 47\n",
      "epoch: 47 loss: 0.31180369738573754\n",
      "eval loss: 0.3094298158387544 0.4030394923106339\n",
      "epoch: 48\n",
      "epoch: 48 loss: 0.31251159940590323\n",
      "eval loss: 0.3122906878432256 0.40423232465732634\n",
      "epoch: 49\n",
      "epoch: 49 loss: 0.3114611557237225\n",
      "eval loss: 0.3141749024212924 0.4083570418590487\n",
      "epoch: 50\n",
      "epoch: 50 loss: 0.31137847373832317\n",
      "eval loss: 0.3099136463082523 0.40333217926397164\n",
      "epoch: 51\n",
      "epoch: 51 loss: 0.31160889814884013\n",
      "eval loss: 0.3095041340360642 0.40167888304614763\n",
      "epoch: 52\n",
      "epoch: 52 loss: 0.31158684407275844\n",
      "eval loss: 0.31237782437250894 0.4051139292944018\n",
      "epoch: 53\n",
      "epoch: 53 loss: 0.31134109020933765\n",
      "eval loss: 0.3129303157688986 0.4039822720868298\n",
      "epoch: 54\n",
      "epoch: 54 loss: 0.31109867176120526\n",
      "eval loss: 0.3075884444999664 0.40179027848711046\n",
      "epoch: 55\n",
      "epoch: 55 loss: 0.31090550317312715\n",
      "eval loss: 0.3095625552853084 0.4002831811539721\n",
      "epoch: 56\n",
      "epoch: 56 loss: 0.31138770948953504\n",
      "eval loss: 0.3123842400137879 0.4070070784225305\n",
      "epoch: 57\n",
      "epoch: 57 loss: 0.31070524767590446\n",
      "eval loss: 0.3130574562717136 0.40638639106411495\n",
      "epoch: 58\n",
      "epoch: 58 loss: 0.3126767569120227\n",
      "eval loss: 0.30790186496989563 0.40311439749799416\n",
      "epoch: 59\n",
      "epoch: 59 loss: 0.3110245458801839\n",
      "eval loss: 0.3105246058041995 0.40336946056699496\n",
      "epoch: 60\n",
      "epoch: 60 loss: 0.3102816876390665\n",
      "eval loss: 0.3083864773662839 0.3975511166516815\n",
      "epoch: 61\n",
      "epoch: 61 loss: 0.3110181509896311\n",
      "eval loss: 0.3147241064259455 0.40714380009317996\n",
      "epoch: 62\n",
      "epoch: 62 loss: 0.3110142691103239\n",
      "eval loss: 0.30687616374653925 0.3988033581032443\n",
      "epoch: 63\n",
      "epoch: 63 loss: 0.3108549536069317\n",
      "eval loss: 0.31273977666393166 0.4048683624874139\n",
      "epoch: 64\n",
      "epoch: 64 loss: 0.3110033527878405\n",
      "eval loss: 0.31160018785152455 0.40396235279120407\n",
      "epoch: 65\n",
      "epoch: 65 loss: 0.3110354090170947\n",
      "eval loss: 0.310106698566627 0.40177097814019813\n",
      "epoch: 66\n",
      "epoch: 66 loss: 0.3100031438450361\n",
      "eval loss: 0.30984740701171354 0.40368886581422614\n",
      "epoch: 67\n",
      "epoch: 67 loss: 0.3102620765989525\n",
      "eval loss: 0.3069507715875838 0.3983543262659777\n",
      "epoch: 68\n",
      "epoch: 68 loss: 0.3102601249354346\n",
      "eval loss: 0.30533979024553937 0.396386220867831\n",
      "epoch: 69\n",
      "epoch: 69 loss: 0.30949925498342296\n",
      "eval loss: 0.30497179122568885 0.39677769343631564\n",
      "epoch: 70\n",
      "epoch: 70 loss: 0.30971841253745075\n",
      "eval loss: 0.3046915983838669 0.395762547968021\n",
      "epoch: 71\n",
      "epoch: 71 loss: 0.310242514751728\n",
      "eval loss: 0.30438827820153114 0.3978378656881992\n",
      "epoch: 72\n",
      "epoch: 72 loss: 0.3104240990642512\n",
      "eval loss: 0.3041574473921367 0.3942426506765982\n",
      "epoch: 73\n",
      "epoch: 73 loss: 0.31112249848177975\n",
      "eval loss: 0.30586056671413653 0.39755053128565454\n",
      "epoch: 74\n",
      "epoch: 74 loss: 0.3107386841623163\n",
      "eval loss: 0.3061690520050753 0.39725462035281195\n",
      "epoch: 75\n",
      "epoch: 75 loss: 0.3084695632598816\n",
      "eval loss: 0.309478830383474 0.40138118747854556\n",
      "epoch: 76\n",
      "epoch: 76 loss: 0.30957735976506723\n",
      "eval loss: 0.30741926309626955 0.3986412276191591\n",
      "epoch: 77\n",
      "epoch: 77 loss: 0.3105785896507397\n",
      "eval loss: 0.30364968538074716 0.39415372701499907\n",
      "epoch: 78\n",
      "epoch: 78 loss: 0.31017975004410137\n",
      "eval loss: 0.30792501665452426 0.3993057488800849\n",
      "epoch: 79\n",
      "epoch: 79 loss: 0.309330114394109\n",
      "eval loss: 0.3113330165248198 0.4021164014751904\n",
      "epoch: 80\n",
      "epoch: 80 loss: 0.3098654945967149\n",
      "eval loss: 0.3044489795470352 0.3959560964806105\n",
      "epoch: 81\n",
      "epoch: 81 loss: 0.308850960152028\n",
      "eval loss: 0.30273864498382774 0.39260329177106196\n",
      "epoch: 82\n",
      "epoch: 82 loss: 0.3087422842442123\n",
      "eval loss: 0.30172548790893516 0.3937702755126311\n",
      "epoch: 83\n",
      "epoch: 83 loss: 0.30881682440302216\n",
      "eval loss: 0.3040959193941963 0.3943991343461219\n",
      "epoch: 84\n",
      "epoch: 84 loss: 0.3087021467689809\n",
      "eval loss: 0.3066873030933211 0.39664996773710864\n",
      "epoch: 85\n",
      "epoch: 85 loss: 0.30898635321887197\n",
      "eval loss: 0.30354265472861075 0.39560430981089456\n",
      "epoch: 86\n",
      "epoch: 86 loss: 0.3090288776103347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.3035050611851159 0.3950646537515273\n",
      "epoch: 87\n",
      "epoch: 87 loss: 0.3085537409296057\n",
      "eval loss: 0.3019460057440915 0.3912063824715222\n",
      "epoch: 88\n",
      "epoch: 88 loss: 0.30741362409350165\n",
      "eval loss: 0.30075640594643716 0.3904603366525723\n",
      "epoch: 89\n",
      "epoch: 89 loss: 0.30854253078719246\n",
      "eval loss: 0.30445276459885584 0.3921741378418848\n",
      "epoch: 90\n",
      "epoch: 90 loss: 0.30771273984141545\n",
      "eval loss: 0.3016285347133114 0.3924424328265846\n",
      "epoch: 91\n",
      "epoch: 91 loss: 0.30709673042840613\n",
      "eval loss: 0.3030954298595632 0.3954841125009456\n",
      "epoch: 92\n",
      "epoch: 92 loss: 0.307574002095627\n",
      "eval loss: 0.3065661429816176 0.39690611244613905\n",
      "epoch: 93\n",
      "epoch: 93 loss: 0.30733173848617557\n",
      "eval loss: 0.30531940837547566 0.3946554225195\n",
      "epoch: 94\n",
      "epoch: 94 loss: 0.30630655975487997\n",
      "eval loss: 0.3021161384300707 0.39172967080301363\n",
      "epoch: 95\n",
      "epoch: 95 loss: 0.3073638188375676\n",
      "eval loss: 0.303235442198711 0.39292315708132874\n",
      "epoch: 96\n",
      "epoch: 96 loss: 0.30667767209289565\n",
      "eval loss: 0.3050269664122171 0.39382570583984466\n",
      "epoch: 97\n",
      "epoch: 97 loss: 0.30655216439457106\n",
      "eval loss: 0.3033962160536063 0.39229454452544893\n",
      "epoch: 98\n",
      "epoch: 98 loss: 0.3068133511344001\n",
      "eval loss: 0.3065782875076606 0.3980834788377917\n",
      "epoch: 99\n",
      "epoch: 99 loss: 0.30902823019806336\n",
      "eval loss: 0.29874653780347477 0.3886692935568245\n",
      "epoch: 100\n",
      "epoch: 100 loss: 0.3084734374331763\n",
      "eval loss: 0.304393545042001 0.3938297401983268\n",
      "epoch: 101\n",
      "epoch: 101 loss: 0.30835732229994667\n",
      "eval loss: 0.3053417187650553 0.39530046824345993\n",
      "epoch: 102\n",
      "epoch: 102 loss: 0.30782664003754223\n",
      "eval loss: 0.30163437285488437 0.3912490407984433\n",
      "epoch: 103\n",
      "epoch: 103 loss: 0.30633212173388064\n",
      "eval loss: 0.30241463552766107 0.39220985016604315\n",
      "epoch: 104\n",
      "epoch: 104 loss: 0.30623021970126474\n",
      "eval loss: 0.30429755961121097 0.3947116319153981\n",
      "epoch: 105\n",
      "epoch: 105 loss: 0.30757753113354225\n",
      "eval loss: 0.3058239027103914 0.39690361541918406\n",
      "epoch: 106\n",
      "epoch: 106 loss: 0.30637511536229467\n",
      "eval loss: 0.3040858026975846 0.39226714183950273\n",
      "epoch: 107\n",
      "epoch: 107 loss: 0.3065417993547822\n",
      "eval loss: 0.29913906931622514 0.38869157911944885\n",
      "epoch: 108\n",
      "epoch: 108 loss: 0.3082181096857349\n",
      "eval loss: 0.305351080688264 0.3951839446635339\n",
      "epoch: 109\n",
      "epoch: 109 loss: 0.3078811208397959\n",
      "eval loss: 0.3039469569643102 0.3965528004730301\n",
      "epoch: 110\n",
      "epoch: 110 loss: 0.3080951769633864\n",
      "eval loss: 0.3082732726286611 0.39972012639830057\n",
      "epoch: 111\n",
      "epoch: 111 loss: 0.30745145176217575\n",
      "eval loss: 0.30965700717433464 0.4035079179315255\n",
      "epoch: 112\n",
      "epoch: 112 loss: 0.3089432520361415\n",
      "eval loss: 0.30650287283676286 0.39870630214308184\n",
      "epoch: 113\n",
      "epoch: 113 loss: 0.3094331998660005\n",
      "eval loss: 0.30391637902670976 0.3949056759963645\n",
      "epoch: 114\n",
      "epoch: 114 loss: 0.30911138249876025\n",
      "eval loss: 0.30377602721762575 0.3954319998103716\n",
      "epoch: 115\n",
      "epoch: 115 loss: 0.3081199777507283\n",
      "eval loss: 0.30224025859447384 0.39302832972100854\n",
      "epoch: 116\n",
      "epoch: 116 loss: 0.3078621224787641\n",
      "eval loss: 0.3016441281316542 0.3913395309794716\n",
      "epoch: 117\n",
      "epoch: 117 loss: 0.30620426476723384\n",
      "eval loss: 0.3015096379666512 0.39150280751855854\n",
      "epoch: 118\n",
      "epoch: 118 loss: 0.3071268910838739\n",
      "eval loss: 0.31269094700482625 0.4087814857869531\n",
      "epoch: 119\n",
      "epoch: 119 loss: 0.31063007521877206\n",
      "eval loss: 0.300616658519355 0.39037610076738116\n",
      "epoch: 120\n",
      "epoch: 120 loss: 0.30680010006418307\n",
      "eval loss: 0.3026773548326986 0.39271888157665036\n",
      "epoch: 121\n",
      "epoch: 121 loss: 0.3072377493219896\n",
      "eval loss: 0.3020963267098553 0.3912716828593551\n",
      "epoch: 122\n",
      "epoch: 122 loss: 0.30625658129181166\n",
      "eval loss: 0.2974358263031046 0.3877012383100459\n",
      "epoch: 123\n",
      "epoch: 123 loss: 0.3052691308855683\n",
      "eval loss: 0.30879240827453436 0.40039295655985346\n",
      "epoch: 124\n",
      "epoch: 124 loss: 0.30587333905104774\n",
      "eval loss: 0.29910141537062873 0.38768718329727947\n",
      "epoch: 125\n",
      "epoch: 125 loss: 0.3068391478338346\n",
      "eval loss: 0.3014300728755277 0.3917706082123359\n",
      "epoch: 126\n",
      "epoch: 126 loss: 0.3072483913321738\n",
      "eval loss: 0.29960301891099317 0.3882724580195252\n",
      "epoch: 127\n",
      "epoch: 127 loss: 0.30512701428643624\n",
      "eval loss: 0.2975352585345834 0.3878644607262962\n",
      "epoch: 128\n",
      "epoch: 128 loss: 0.3066587718239736\n",
      "eval loss: 0.29872236307613453 0.3874335480208771\n",
      "epoch: 129\n",
      "epoch: 129 loss: 0.3054140512394653\n",
      "eval loss: 0.29834159358250945 0.3852537436347819\n",
      "epoch: 130\n",
      "epoch: 130 loss: 0.30491756590757035\n",
      "eval loss: 0.2987872917924007 0.3871862732866264\n",
      "epoch: 131\n",
      "epoch: 131 loss: 0.3055080083049788\n",
      "eval loss: 0.29710546512925773 0.38795800421540105\n",
      "epoch: 132\n",
      "epoch: 132 loss: 0.304295401315755\n",
      "eval loss: 0.2977575407370929 0.39002945840420583\n",
      "epoch: 133\n",
      "epoch: 133 loss: 0.3048539592930646\n",
      "eval loss: 0.2985201892916005 0.38765969330397665\n",
      "epoch: 134\n",
      "epoch: 134 loss: 0.30474838493920975\n",
      "eval loss: 0.29919857725050536 0.3861889815249104\n",
      "epoch: 135\n",
      "epoch: 135 loss: 0.3038578004288675\n",
      "eval loss: 0.29983811083525125 0.38861902620702615\n",
      "epoch: 136\n",
      "epoch: 136 loss: 0.3036225220438561\n",
      "eval loss: 0.2985395507607881 0.3885534480763216\n",
      "epoch: 137\n",
      "epoch: 137 loss: 0.30515869020375647\n",
      "eval loss: 0.29744340429908833 0.38591262740160487\n",
      "epoch: 138\n",
      "epoch: 138 loss: 0.3033288556683282\n",
      "eval loss: 0.2963727719961794 0.38376651055796657\n",
      "epoch: 139\n",
      "epoch: 139 loss: 0.3041031031099197\n",
      "eval loss: 0.2976558294575617 0.38712610751518006\n",
      "epoch: 140\n",
      "epoch: 140 loss: 0.3056108597767277\n",
      "eval loss: 0.2985597703081277 0.38777835024033047\n",
      "epoch: 141\n",
      "epoch: 141 loss: 0.30345601878293815\n",
      "eval loss: 0.2964871856241412 0.3849902211502767\n",
      "epoch: 142\n",
      "epoch: 142 loss: 0.3036929765017992\n",
      "eval loss: 0.29816644213847265 0.38735021423358584\n",
      "epoch: 143\n",
      "epoch: 143 loss: 0.30324071064958996\n",
      "eval loss: 0.2972096748227111 0.38545418845952717\n",
      "epoch: 144\n",
      "epoch: 144 loss: 0.30287025821914726\n",
      "eval loss: 0.2966378625810475 0.38500312512571855\n",
      "epoch: 145\n",
      "epoch: 145 loss: 0.3041112391322786\n",
      "eval loss: 0.29712516986322557 0.3859677217102487\n",
      "epoch: 146\n",
      "epoch: 146 loss: 0.3048814940528812\n",
      "eval loss: 0.2991994082256785 0.38909583351095567\n",
      "epoch: 147\n",
      "epoch: 147 loss: 0.3053476906086949\n",
      "eval loss: 0.29573960665500465 0.38424481567641344\n",
      "epoch: 148\n",
      "epoch: 148 loss: 0.303454487466768\n",
      "eval loss: 0.30032487370707456 0.3892704190663542\n",
      "epoch: 149\n",
      "epoch: 149 loss: 0.3034238863531812\n",
      "eval loss: 0.29899828001619355 0.3856388472228333\n",
      "epoch: 150\n",
      "epoch: 150 loss: 0.3048404593336488\n",
      "eval loss: 0.29995352404281916 0.3892726258590459\n",
      "epoch: 151\n",
      "epoch: 151 loss: 0.30350952089645367\n",
      "eval loss: 0.29671478802731344 0.3831022986017617\n",
      "epoch: 152\n",
      "epoch: 152 loss: 0.3026504256726762\n",
      "eval loss: 0.29735630442687816 0.3858703417308831\n",
      "epoch: 153\n",
      "epoch: 153 loss: 0.3026323299954307\n",
      "eval loss: 0.2955336351529492 0.3851959797009715\n",
      "epoch: 154\n",
      "epoch: 154 loss: 0.30389008927399275\n",
      "eval loss: 0.2962928905330062 0.3827224022406024\n",
      "epoch: 155\n",
      "epoch: 155 loss: 0.3021653086611107\n",
      "eval loss: 0.29936729612358637 0.38805832680145735\n",
      "epoch: 156\n",
      "epoch: 156 loss: 0.30203386373425123\n",
      "eval loss: 0.2960834484884922 0.3853468931344253\n",
      "epoch: 157\n",
      "epoch: 157 loss: 0.3026922919078763\n",
      "eval loss: 0.2963128360882833 0.38502587614657957\n",
      "epoch: 158\n",
      "epoch: 158 loss: 0.302940984260463\n",
      "eval loss: 0.29420642403552916 0.3816129346070211\n",
      "epoch: 159\n",
      "epoch: 159 loss: 0.30192577179760044\n",
      "eval loss: 0.2963252003197722 0.3823436918868563\n",
      "epoch: 160\n",
      "epoch: 160 loss: 0.30161746504384823\n",
      "eval loss: 0.296195143956257 0.3844996386481492\n",
      "epoch: 161\n",
      "epoch: 161 loss: 0.303302049068172\n",
      "eval loss: 0.29913950948887974 0.38452011393498364\n",
      "epoch: 162\n",
      "epoch: 162 loss: 0.3013670124871837\n",
      "eval loss: 0.2946627151548354 0.38297371497896665\n",
      "epoch: 163\n",
      "epoch: 163 loss: 0.30207971043816867\n",
      "eval loss: 0.30162113278443775 0.38976994443532753\n",
      "epoch: 164\n",
      "epoch: 164 loss: 0.3026042796058266\n",
      "eval loss: 0.2984879879236346 0.3850463909372512\n",
      "epoch: 165\n",
      "epoch: 165 loss: 0.30255019918060994\n",
      "eval loss: 0.2950110726594235 0.38498857700268946\n",
      "epoch: 166\n",
      "epoch: 166 loss: 0.30352142971226465\n",
      "eval loss: 0.29488733028985764 0.3837174787047477\n",
      "epoch: 167\n",
      "epoch: 167 loss: 0.3020955977623291\n",
      "eval loss: 0.2964137217945794 0.3852112710461755\n",
      "epoch: 168\n",
      "epoch: 168 loss: 0.3021856001706105\n",
      "eval loss: 0.3001710297621847 0.3884970997623023\n",
      "epoch: 169\n",
      "epoch: 169 loss: 0.30347752608567713\n",
      "eval loss: 0.29910530027952903 0.38621794222902855\n",
      "epoch: 170\n",
      "epoch: 170 loss: 0.30183368268147814\n",
      "eval loss: 0.2947852388615072 0.38333684361709863\n",
      "epoch: 171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 171 loss: 0.30256787970437304\n",
      "eval loss: 0.2966598930118262 0.3848768562590066\n",
      "epoch: 172\n",
      "epoch: 172 loss: 0.30212269302459527\n",
      "eval loss: 0.2980501724341208 0.3870595562651022\n",
      "epoch: 173\n",
      "epoch: 173 loss: 0.30191469798016624\n",
      "eval loss: 0.2971405796156257 0.3841263456956243\n",
      "epoch: 174\n",
      "epoch: 174 loss: 0.30692438196435795\n",
      "eval loss: 0.3121683068645721 0.4062905121379845\n",
      "epoch: 175\n",
      "epoch: 175 loss: 0.31048234464641183\n",
      "eval loss: 0.30314706690400467 0.395852315893677\n",
      "epoch: 176\n",
      "epoch: 176 loss: 0.30630863578829487\n",
      "eval loss: 0.3000290887428738 0.3902634356936934\n",
      "epoch: 177\n",
      "epoch: 177 loss: 0.30598390564195493\n",
      "eval loss: 0.30052479020378176 0.3888731242730509\n",
      "epoch: 178\n",
      "epoch: 178 loss: 0.3056511542123049\n",
      "eval loss: 0.3031183748613747 0.3919834791409864\n",
      "epoch: 179\n",
      "epoch: 179 loss: 0.3059345692111122\n",
      "eval loss: 0.3018507667348325 0.39222546381042056\n",
      "epoch: 180\n",
      "epoch: 180 loss: 0.30549650746414475\n",
      "eval loss: 0.30380335892779575 0.3926245323496738\n",
      "epoch: 181\n",
      "epoch: 181 loss: 0.305445335282577\n",
      "eval loss: 0.300380811623143 0.38881961263761433\n",
      "epoch: 182\n",
      "epoch: 182 loss: 0.3047738419587292\n",
      "eval loss: 0.29898402966962423 0.38839034384249727\n",
      "epoch: 183\n",
      "epoch: 183 loss: 0.3050958430560618\n",
      "eval loss: 0.3005021457096549 0.3895856096855027\n",
      "epoch: 184\n",
      "epoch: 184 loss: 0.30456072588258726\n",
      "eval loss: 0.29852109729802934 0.3870506195532297\n",
      "epoch: 185\n",
      "epoch: 185 loss: 0.3043383873413647\n",
      "eval loss: 0.29987924174190733 0.3868980719439061\n",
      "epoch: 186\n",
      "epoch: 186 loss: 0.3040192633206893\n",
      "eval loss: 0.29695173084769905 0.3850528376571631\n",
      "epoch: 187\n",
      "epoch: 187 loss: 0.302924736882251\n",
      "eval loss: 0.2982135793092494 0.38668362179235904\n",
      "epoch: 188\n",
      "epoch: 188 loss: 0.3029450100617992\n",
      "eval loss: 0.29953565803042964 0.38736058289815556\n",
      "epoch: 189\n",
      "epoch: 189 loss: 0.3031972425181082\n",
      "eval loss: 0.29903684937676656 0.3875822563952221\n",
      "epoch: 190\n",
      "epoch: 190 loss: 0.30338487809695014\n",
      "eval loss: 0.29702527251342076 0.3862008131721787\n",
      "epoch: 191\n",
      "epoch: 191 loss: 0.302788913616451\n",
      "eval loss: 0.29484331451985213 0.38380661474893885\n",
      "epoch: 192\n",
      "epoch: 192 loss: 0.30200639809506774\n",
      "eval loss: 0.29653146472245717 0.3855012667884661\n",
      "epoch: 193\n",
      "epoch: 193 loss: 0.30234214591869\n",
      "eval loss: 0.2998539428421566 0.38839492003747733\n",
      "epoch: 194\n",
      "epoch: 194 loss: 0.30202441490946624\n",
      "eval loss: 0.29498477464432044 0.3847409814040699\n",
      "epoch: 195\n",
      "epoch: 195 loss: 0.30243872518223286\n",
      "eval loss: 0.29336334135192144 0.3821287449960725\n",
      "epoch: 196\n",
      "epoch: 196 loss: 0.3014822392394719\n",
      "eval loss: 0.2967788463946978 0.38400754401146064\n",
      "epoch: 197\n",
      "epoch: 197 loss: 0.3019694047225319\n",
      "eval loss: 0.3024384091371958 0.39270907281490525\n",
      "epoch: 198\n",
      "epoch: 198 loss: 0.30407451870876645\n",
      "eval loss: 0.2973809541367549 0.38552658427514774\n",
      "epoch: 199\n",
      "epoch: 199 loss: 0.30114361533299827\n",
      "eval loss: 0.29275509968356556 0.38292596805436224\n",
      "min eval loss: 0.29275509968356556 epoch 199\n",
      "fold: 1\n",
      "(1, 107, 107, 3)\n",
      "(1, 107, 107, 3)\n",
      "(2160, 21) (240, 21)\n",
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='41' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.50% [41/200 06:33<25:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 0 loss: 0.3844470835099143\n",
      "eval loss: 0.33202091487027713 0.4235491876871253\n",
      "epoch: 1\n",
      "epoch: 1 loss: 0.3229697166164548\n",
      "eval loss: 0.32318378910769385 0.413047705922972\n",
      "epoch: 2\n",
      "epoch: 2 loss: 0.3192985091344393\n",
      "eval loss: 0.3234063966229204 0.41305827032941367\n",
      "epoch: 3\n",
      "epoch: 3 loss: 0.31868978202620574\n",
      "eval loss: 0.3242491889730413 0.41379789724669813\n",
      "epoch: 4\n",
      "epoch: 4 loss: 0.320753958722659\n",
      "eval loss: 0.3229115317365371 0.4129795371532284\n",
      "epoch: 5\n",
      "epoch: 5 loss: 0.31864876599206227\n",
      "eval loss: 0.3224206873047216 0.41159034540368394\n",
      "epoch: 6\n",
      "epoch: 6 loss: 0.31867296624680497\n",
      "eval loss: 0.32499016218073074 0.4125301926460784\n",
      "epoch: 7\n",
      "epoch: 7 loss: 0.3185606647828574\n",
      "eval loss: 0.3219736470359364 0.41273511312710637\n",
      "epoch: 8\n",
      "epoch: 8 loss: 0.31907298428890835\n",
      "eval loss: 0.3227612897792978 0.4121580660147873\n",
      "epoch: 9\n",
      "epoch: 9 loss: 0.3189043742336824\n",
      "eval loss: 0.3260500138731212 0.41482326867677305\n",
      "epoch: 10\n",
      "epoch: 10 loss: 0.3196179614584133\n",
      "eval loss: 0.3256356683714379 0.4151293548732351\n",
      "epoch: 11\n",
      "epoch: 11 loss: 0.31910465104375924\n",
      "eval loss: 0.3243423412940209 0.4118593435021451\n",
      "epoch: 12\n",
      "epoch: 12 loss: 0.3198876316815747\n",
      "eval loss: 0.32479923591202087 0.4169913898713318\n",
      "epoch: 13\n",
      "epoch: 13 loss: 0.31961169249230403\n",
      "eval loss: 0.3243546492015022 0.4123279660244768\n",
      "epoch: 14\n",
      "epoch: 14 loss: 0.31850984893000955\n",
      "eval loss: 0.32143904508625065 0.4128014139887102\n",
      "epoch: 15\n",
      "epoch: 15 loss: 0.3192836855431456\n",
      "eval loss: 0.3246489678805416 0.4135120815002832\n",
      "epoch: 16\n",
      "epoch: 16 loss: 0.31865375911962934\n",
      "eval loss: 0.32488030238121735 0.41378989329587\n",
      "epoch: 17\n",
      "epoch: 17 loss: 0.3179966117869042\n",
      "eval loss: 0.32116988368865396 0.41230985276747334\n",
      "epoch: 18\n",
      "epoch: 18 loss: 0.3196190603456301\n",
      "eval loss: 0.326843588496873 0.41473837710425104\n",
      "epoch: 19\n",
      "epoch: 19 loss: 0.3184788368621733\n",
      "eval loss: 0.3222731020032889 0.41320241029464416\n",
      "epoch: 20\n",
      "epoch: 20 loss: 0.3186961574554954\n",
      "eval loss: 0.32609438355232107 0.41451280951985564\n",
      "epoch: 21\n",
      "epoch: 21 loss: 0.31840139151873054\n",
      "eval loss: 0.32591025630301984 0.41324203617399075\n",
      "epoch: 22\n",
      "epoch: 22 loss: 0.3186353187094012\n",
      "eval loss: 0.32481175649673555 0.41132822029729693\n",
      "epoch: 23\n",
      "epoch: 23 loss: 0.31872539653230475\n",
      "eval loss: 0.3255284336648885 0.41433967550314643\n",
      "epoch: 24\n",
      "epoch: 24 loss: 0.3193143236579179\n",
      "eval loss: 0.3266191163011227 0.4153351094816907\n",
      "epoch: 25\n",
      "epoch: 25 loss: 0.3190532228837122\n",
      "eval loss: 0.32482471940887125 0.41281992789043487\n",
      "epoch: 26\n",
      "epoch: 26 loss: 0.318630122579944\n",
      "eval loss: 0.3229813298484093 0.4133768732718262\n",
      "epoch: 27\n",
      "epoch: 27 loss: 0.31861221923630145\n",
      "eval loss: 0.32338571892971846 0.41090400365303864\n",
      "epoch: 28\n",
      "epoch: 28 loss: 0.31902752115193667\n",
      "eval loss: 0.32482001150059137 0.4128394688309682\n",
      "epoch: 29\n",
      "epoch: 29 loss: 0.31904057267549574\n",
      "eval loss: 0.32587011250054787 0.41618880197699865\n",
      "epoch: 30\n",
      "epoch: 30 loss: 0.31923434132240547\n",
      "eval loss: 0.3250750245900593 0.4139625373597399\n",
      "epoch: 31\n",
      "epoch: 31 loss: 0.3192199854506166\n",
      "eval loss: 0.32684536651943275 0.41724518954887335\n",
      "epoch: 32\n",
      "epoch: 32 loss: 0.3188599939257309\n",
      "eval loss: 0.3278950342464618 0.4158487646802871\n",
      "epoch: 33\n",
      "epoch: 33 loss: 0.3182835467125744\n",
      "eval loss: 0.3235217431249552 0.4160534406971459\n",
      "epoch: 34\n",
      "epoch: 34 loss: 0.31882771626027956\n",
      "eval loss: 0.3244863499101896 0.4136332355843307\n",
      "epoch: 35\n",
      "epoch: 35 loss: 0.3181423621710356\n",
      "eval loss: 0.3274786411125775 0.4181196478913554\n",
      "epoch: 36\n",
      "epoch: 36 loss: 0.319024007287291\n",
      "eval loss: 0.3226247291047145 0.4131583006564246\n",
      "epoch: 37\n",
      "epoch: 37 loss: 0.31920260608579865\n",
      "eval loss: 0.3245068418926349 0.41413247135289805\n",
      "epoch: 38\n",
      "epoch: 38 loss: 0.3182218453080479\n",
      "eval loss: 0.32311398048477424 0.414421904141482\n",
      "epoch: 39\n",
      "epoch: 39 loss: 0.31869912185296984\n",
      "eval loss: 0.3248247812602496 0.4136628558095439\n",
      "epoch: 40\n",
      "epoch: 40 loss: 0.31967542246831177\n",
      "eval loss: 0.32610543799846264 0.4163797935576731\n",
      "epoch: 41\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-21b00aa98391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     epoch = train(model, train_loader, valid_loader, optimizer, lr_scheduler, 200, device=device,\n\u001b[0;32m---> 29\u001b[0;31m                   log_path=f\"logs/{fold}\")\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"./model-{epoch}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"model_prediction/model-{fold}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-8fa1639579e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, valid_data, optimizer, lr_scheduler, epochs, device, start_epoch, log_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_from_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-8fa1639579e5>\u001b[0m in \u001b[0;36mlearn_from_batch\u001b[0;34m(model, data, optimizer, lr_scheduler, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msn_mcrmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal_to_noise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyh/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "BATCH_SIZE = 64\n",
    "base_train_data = pd.read_json(str(Path(BASE_PATH) / 'train.json'), lines=True)\n",
    "samples = base_train_data\n",
    "save_path = Path(\"./model_prediction\")\n",
    "if not save_path.exists():\n",
    "    save_path.mkdir(parents=True)\n",
    "shutil.rmtree(\"./model\", True)\n",
    "shutil.rmtree(\"./logs\", True)\n",
    "split = ShuffleSplit(n_splits=5, test_size=.1)\n",
    "ids = samples.reset_index()[\"id\"]\n",
    "set_seed(124)\n",
    "for fold, (train_index, test_index) in enumerate(split.split(samples)):\n",
    "    print(f\"fold: {fold}\")\n",
    "    train_df = samples.loc[train_index].reset_index()\n",
    "    val_df = samples.loc[test_index].reset_index()\n",
    "    train_loader = create_loader(train_df, BATCH_SIZE)\n",
    "    valid_loader = create_loader(val_df, BATCH_SIZE)\n",
    "    print(train_df.shape, val_df.shape)\n",
    "    ae_model = AEModel()\n",
    "    state_dict = torch.load(\"./ae-model.pt\")\n",
    "    ae_model.load_state_dict(state_dict)\n",
    "    del state_dict\n",
    "    model = FromAeModel(ae_model.seq)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    lr_scheduler = None\n",
    "    epoch = train(model, train_loader, valid_loader, optimizer, lr_scheduler, 200, device=device,\n",
    "                  log_path=f\"logs/{fold}\")\n",
    "    shutil.copyfile(str(Path(MODEL_SAVE_PATH) / f\"./model-{epoch}.pt\"), f\"model_prediction/model-{fold}.pt\")\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyh",
   "language": "python",
   "name": "lyh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
